{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#slisemap-combine-supervised-dimensionality-reduction-with-local-explanations","title":"SLISEMAP: Combine supervised dimensionality reduction with local explanations","text":"<p>SLISEMAP is a supervised dimensionality reduction method, that takes data, in the form of vectors, and predictions from a \"black box\" regression or classification model as input. SLISEMAP then simultaneously finds local explanations for all data items and builds a (typically) two-dimensional global visualisation of the black box model such that data items with similar local explanations are projected nearby. The explanations consists of interpretable models that locally approximate the \"black box\" model.</p> <p>SLISEMAP is implemented in Python using PyTorch for efficient optimisation, and optional GPU-acceleration. For more information see the papers, the examples, or the documentation.</p> <p>This library also includes the faster SLIPMAP variant, that uses \"prototypes\" to speed up the calculations (linear time and memory complexity instead of quadratic). SLIPMAP is largely compatible with SLISEMAP, just change the class name (<code>Slisemap</code> to <code>Slipmap</code>, see example below).</p>"},{"location":"#citations","title":"Citations","text":"<p>The new SLIPMAP paper (supplements and slides):</p> <p>Bj\u00f6rklund, A., Sepp\u00e4l\u00e4inen, L., &amp; Puolam\u00e4ki, K. (2024). SLIPMAP: Fast and Robust Manifold Visualisation for Explainable AI Advances in Intelligent Data Analysis XXII, IDA 2024, pp. 223-235. Lecture Notes in Computer Science, vol 14642. DOI: 10.1007/978-3-031-58553-1_18 </p> <p>The full SLISEMAP paper (arXiv, supplements, and slides):</p> <p>Bj\u00f6rklund, A., M\u00e4kel\u00e4, J., &amp; Puolam\u00e4ki, K. (2023). SLISEMAP: Supervised dimensionality reduction through local explanations. Machine Learning 112, 1-43. DOI: 10.1007/s10994-022-06261-1 </p> <p>The short demo paper (video and slides):</p> <p>Bj\u00f6rklund, A., M\u00e4kel\u00e4, J., &amp; Puolam\u00e4ki, K. (2023). SLISEMAP: Combining Supervised Dimensionality Reduction with Local Explanations. Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2022. Lecture Notes in Computer Science, vol 13718. DOI: 10.1007/978-3-031-26422-1_41.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the package just run:</p> <pre><code>pip install slisemap\n</code></pre> <p>Or install the latest version directly from GitHub:</p> <pre><code>pip install git+https://github.com/edahelsinki/slisemap\n</code></pre> <p>To use the built-in hyperparameter tuning you also need <code>scikit-optimize</code>, which is automatically installed if you do:</p> <pre><code>pip install slisemap[tuning]\n</code></pre>"},{"location":"#pytorch","title":"PyTorch","text":"<p>Since SLISEMAP utilises PyTorch for efficient calculations, you might want to install a version that is optimised for your hardware. See https://pytorch.org/get-started/locally for details.</p>"},{"location":"#example","title":"Example","text":"<pre><code>import numpy as np\nfrom slisemap import Slisemap\n\nX = np.array(...)\ny = np.array(...)\nsm = Slisemap(X, y, radius=3.5, lasso=0.01)\nsm.optimise()\nsm.plot(clusters=5, bars=5)\n</code></pre> <p>To use the faster SLIPMAP variant just replace the relevant lines:</p> <pre><code>from slisemap import Slipmap\nsm = Slipmap(X, y, radius=2.0, lasso=0.01)\n</code></pre> <p>See the examples for more detailed examples, and the documentation for more detailed instructions.</p>"},{"location":"slisemap.diagnostics/","title":"slisemap.diagnostics","text":""},{"location":"slisemap.diagnostics/#slisemap.diagnostics","title":"<code>slisemap.diagnostics</code>","text":"<p>These are diagnostics for identifying potential issues with SLISEMAP solutions.</p> <p>Typical usage:</p> <pre><code>sm = Slisemap(...)\nsm.optimise()\ndiagnostics = diagnose(sm)\nprint_diagnostics(diagnostics)\nplot_diagnostics(sm, diagnostics)\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.global_model_losses","title":"<code>global_model_losses(sm, indices=None, **kwargs)</code>","text":"<p>Train a global model.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Slisemap object.</p> required <code>indices</code> <code>Optional[ndarray]</code> <p>Optional subsampling indices. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to LBFGS.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Vector of individual losses for a global model.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def global_model_losses(\n    sm: Slisemap, indices: Optional[np.ndarray] = None, **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Train a global model.\n\n    Args:\n        sm: Slisemap object.\n        indices: Optional subsampling indices. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to LBFGS.\n\n    Returns:\n        Vector of individual losses for a global model.\n    \"\"\"\n    if indices is None:\n        X = sm._X\n        Y = sm._Y\n    else:\n        X = sm._X[indices]\n        Y = sm._Y[indices]\n    B = global_model(\n        X=X,\n        Y=Y,\n        local_model=sm.local_model,\n        local_loss=sm.local_loss,\n        coefficients=sm.q,\n        lasso=sm.lasso,\n        ridge=sm.ridge,\n    )\n    return sm.local_loss(sm.local_model(sm._X, B), sm._Y).detach()\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.print_diagnostics","title":"<code>print_diagnostics(diagnostics, summary=False)</code>","text":"<p>Print diagnostic results.</p> <p>Parameters:</p> Name Type Description Default <code>diagnostics</code> <code>Dict[str, ndarray]</code> <p>Dictionary of diagnostic results.</p> required <code>summary</code> <code>bool</code> <p>Print only one summary for all the diagnostics. Defaults to False.</p> <code>False</code> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def print_diagnostics(\n    diagnostics: Dict[str, np.ndarray], summary: bool = False\n) -&gt; None:\n    \"\"\"Print diagnostic results.\n\n    Args:\n        diagnostics: Dictionary of diagnostic results.\n        summary: Print only one summary for all the diagnostics. Defaults to False.\n    \"\"\"\n    if summary:\n        issues = reduce(lambda a, b: a + b.astype(int), diagnostics.values())\n        if np.sum(issues) == 0:\n            print(\"All data items passed all the diagnostics!\")\n        else:\n            print(\n                f\"{np.mean(issues &gt; 0) * 100 :.1f}% of the data items failed at least\",\n                \"one diagnostic and the average failure rate across all diagnostics\",\n                f\"was {np.sum(issues) / (len(issues) * len(diagnostics)) *100:.1f}%\",\n            )\n    else:\n        for name, mask in diagnostics.items():\n            rate = np.mean(mask) * 100\n            if rate == 0:\n                print(f\"All data items passed the `{name}` diagnostic!\")\n            else:\n                print(f\"{rate:.1f}% of the data items failed the `{name}` diagnostic!\")\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.plot_diagnostics","title":"<code>plot_diagnostics(Z, diagnostics, summary=False, title='Slisemap Diagnostics', show=True, **kwargs)</code>","text":"<p>Plot diagnostic results.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>Union[Slisemap, ndarray]</code> <p>The Slisemap object, or embedding matrix.</p> required <code>diagnostics</code> <code>Dict[str, ndarray]</code> <p>Dictionary of diagnostic results.</p> required <code>summary</code> <code>bool</code> <p>Combine multiple diagnostics into one plot. Defaults to False.</p> <code>False</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"Slisemap Diagnostics\".</p> <code>'Slisemap Diagnostics'</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional parameters to <code>seaborn.relplot</code>.</p> <p>Returns:</p> Type Description <code>Optional[FacetGrid]</code> <p><code>seaborn.FacetGrid</code> if <code>show=False</code>.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def plot_diagnostics(\n    Z: Union[Slisemap, np.ndarray],\n    diagnostics: Dict[str, np.ndarray],\n    summary: bool = False,\n    title: str = \"Slisemap Diagnostics\",\n    show: bool = True,\n    **kwargs: Any,\n) -&gt; Optional[sns.FacetGrid]:\n    \"\"\"Plot diagnostic results.\n\n    Args:\n        Z: The Slisemap object, or embedding matrix.\n        diagnostics: Dictionary of diagnostic results.\n        summary: Combine multiple diagnostics into one plot. Defaults to False.\n        title: Title of the plot. Defaults to \"Slisemap Diagnostics\".\n        show: Show the plot. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Additional parameters to `seaborn.relplot`.\n\n    Returns:\n        `seaborn.FacetGrid` if `show=False`.\n    \"\"\"\n    if isinstance(Z, Slisemap):\n        Z = Z.get_Z(rotate=True)\n    if len(diagnostics) == 1:\n        for name, mask in diagnostics.items():\n            df = dict_array({\"Z1\": Z[:, 0], \"Z2\": Z[:, 1], name: mask})\n            g = sns.relplot(\n                data=df, x=\"Z1\", y=\"Z2\", hue=name, style=name, kind=\"scatter\", **kwargs\n            )\n    elif summary:\n        issues = reduce(lambda a, b: a + b.astype(int), diagnostics.values())\n        df = dict_array(\n            {\"Z1\": Z[:, 0], \"Z2\": Z[:, 1], \"Problem\": issues &gt; 0, \"Severity\": issues}\n        )\n        g = sns.relplot(\n            data=df,\n            x=\"Z1\",\n            y=\"Z2\",\n            hue=\"Severity\",\n            style=\"Problem\",\n            kind=\"scatter\",\n            **kwargs,\n        )\n    else:\n        if \"col_wrap\" not in kwargs:\n            if len(diagnostics) &lt;= 4:\n                kwargs[\"col_wrap\"] = len(diagnostics)\n            elif len(diagnostics) &lt; 7 or len(diagnostics) == 9:\n                kwargs[\"col_wrap\"] = 3\n            else:\n                kwargs[\"col_wrap\"] = 4\n        df = dict_concat(\n            {\n                \"Z1\": Z[:, 0],\n                \"Z2\": Z[:, 1],\n                \"Problem\": mask,\n                \"Diagnostic\": f\"{diag} ({np.mean(mask) * 100:.1f} %)\",\n            }\n            for diag, mask in diagnostics.items()\n        )\n        g = sns.relplot(\n            data=df,\n            x=\"Z1\",\n            y=\"Z2\",\n            hue=\"Problem\",\n            style=\"Problem\",\n            kind=\"scatter\",\n            col=\"Diagnostic\",\n            **kwargs,\n        )\n    plt.suptitle(title)\n    if show:\n        plt.show()\n    else:\n        return g\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.distant_diagnostic","title":"<code>distant_diagnostic(sm, max_distance=10.0)</code>","text":"<p>Check if any data item in the embedding is too far way.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>max_distance</code> <code>float</code> <p>Maximum distance from origo in the embedding. Defaults to 10.0.</p> <code>10.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def distant_diagnostic(sm: Slisemap, max_distance: float = 10.0) -&gt; np.ndarray:\n    \"\"\"Check if any data item in the embedding is too far way.\n\n    Args:\n        sm: Trained Slisemap solution.\n        max_distance: Maximum distance from origo in the embedding. Defaults to 10.0.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    return np.sum(sm.get_Z() ** 2, 1) &gt; max_distance**2\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.heavyweight_diagnostic","title":"<code>heavyweight_diagnostic(sm, min_size=0.1)</code>","text":"<p>Check if any data item has a self-weight that is too large.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>min_size</code> <code>Union[float, int]</code> <p>Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def heavyweight_diagnostic(\n    sm: Slisemap, min_size: Union[float, int] = 0.1\n) -&gt; np.ndarray:\n    \"\"\"Check if any data item has a self-weight that is too large.\n\n    Args:\n        sm: Trained Slisemap solution.\n        min_size: Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    return tonp(sm.get_W(numpy=False).diag() &gt; _frac(sm.n, min_size))\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.lightweight_diagnostic","title":"<code>lightweight_diagnostic(sm, max_size=0.5)</code>","text":"<p>Check if any data item has a self-weight that is too small.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>max_size</code> <code>Union[float, int]</code> <p>Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def lightweight_diagnostic(\n    sm: Slisemap, max_size: Union[float, int] = 0.5\n) -&gt; np.ndarray:\n    \"\"\"Check if any data item has a self-weight that is too small.\n\n    Args:\n        sm: Trained Slisemap solution.\n        max_size: Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    return tonp(sm.get_W(numpy=False).diag() &lt; (1 / _size(sm.n, max_size)))\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.weight_neighbourhood_diagnostic","title":"<code>weight_neighbourhood_diagnostic(sm, min_size=0.1, max_size=0.5)</code>","text":"<p>Check if any data item has a neighbourhood that is too small/large by counting the number of non-lightweight neighbours.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>min_size</code> <code>Union[float, int]</code> <p>Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.</p> <code>0.1</code> <code>max_size</code> <code>Union[float, int]</code> <p>Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def weight_neighbourhood_diagnostic(\n    sm: Slisemap, min_size: Union[float, int] = 0.1, max_size: Union[float, int] = 0.5\n) -&gt; np.ndarray:\n    \"\"\"Check if any data item has a neighbourhood that is too small/large by counting the number of non-lightweight neighbours.\n\n    Args:\n        sm: Trained Slisemap solution.\n        min_size: Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.\n        max_size: Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    min_size = _size(sm.n, min_size)\n    max_size = _size(sm.n, max_size)\n    return tonp((sm.get_W(numpy=False) &gt; (1 / max_size)).sum(1) &lt; min_size)\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.loss_neighbourhood_diagnostic","title":"<code>loss_neighbourhood_diagnostic(sm, min_size=0.1, smoothing=True, median=False)</code>","text":"<p>Check if any data item has a neighbourhood that is too small/large by comparing local losses to global losses.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>min_size</code> <code>Union[float, int]</code> <p>Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.</p> <code>0.1</code> <code>smoothing</code> <code>bool</code> <p>Smooth the sorted losses to avoid sensitivity to outliers. Defaults to True.</p> <code>True</code> <code>median</code> <code>bool</code> <p>Compare against the median global loss instead of the mean global loss. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def loss_neighbourhood_diagnostic(\n    sm: Slisemap,\n    min_size: Union[float, int] = 0.1,\n    smoothing: bool = True,\n    median: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Check if any data item has a neighbourhood that is too small/large by comparing local losses to global losses.\n\n    Args:\n        sm: Trained Slisemap solution.\n        min_size: Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.\n        smoothing: Smooth the sorted losses to avoid sensitivity to outliers. Defaults to True.\n        median: Compare against the median global loss instead of the mean global loss. Defaults to False.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    min_size = _size(sm.n, min_size)\n    if median:\n        gloss = global_model_losses(sm).median().cpu().item()\n    else:\n        gloss = global_model_losses(sm).mean().cpu().item()\n    llosses = sm.get_L(numpy=False)\n    order = torch.argsort(sm.get_D(numpy=False))\n    result = np.zeros(sm.n, bool)\n    if smoothing:\n        filter = torch.as_tensor([[[0.25, 0.5, 0.25]]], **sm.tensorargs)\n    for i in range(sm.n):\n        lli = llosses[i, order[i]]\n        if smoothing:\n            lli = torch.conv1d(lli[None, None], filter, padding=\"same\")[0, 0]\n        first = torch.where(lli &gt; gloss)[0][:1]\n        if first.numel() &lt; 1:\n            result[i] = lli.numel() &lt; min_size\n        else:\n            result[i] = (first &lt; min_size).cpu().item()\n    return result\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.global_loss_diagnostic","title":"<code>global_loss_diagnostic(sm, bootstrap=10, sd=1.0)</code>","text":"<p>Check if any local model is actually a global model.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>bootstrap</code> <code>int</code> <p>Number of (bootstrap) global models to train. Defaults to 10.</p> <code>10</code> <code>sd</code> <code>float</code> <p>Number of standard deviations from the mean (of global models losses) to consider a local model global. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def global_loss_diagnostic(\n    sm: Slisemap, bootstrap: int = 10, sd: float = 1.0\n) -&gt; np.ndarray:\n    \"\"\"Check if any local model is actually a global model.\n\n    Args:\n        sm: Trained Slisemap solution.\n        bootstrap: Number of (bootstrap) global models to train. Defaults to 10.\n        sd: Number of standard deviations from the mean (of global models losses) to consider a local model global. Defaults to 1.0.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    glosses = [\n        global_model_losses(sm, np.random.randint(sm.n, size=sm.n)).sum().cpu().item()\n        for _ in range(bootstrap)\n    ]\n    treshold = np.mean(glosses) + np.std(glosses) * sd\n    llosses = sm.get_L(numpy=False).sum(1)\n    return tonp(llosses &lt; treshold)\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.quantile_loss_diagnostic","title":"<code>quantile_loss_diagnostic(sm, quantile=0.4)</code>","text":"<p>Check if any fidelity is worse than a quantile of all losses.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>quantile</code> <code>float</code> <p>The quantile percentage. Defaults to 0.4.</p> <code>0.4</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def quantile_loss_diagnostic(sm: Slisemap, quantile: float = 0.4) -&gt; np.ndarray:\n    \"\"\"Check if any fidelity is worse than a quantile of all losses.\n\n    Args:\n        sm: Trained Slisemap solution.\n        quantile: The quantile percentage. Defaults to 0.4.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    L = sm.get_L(numpy=False)\n    treshold = torch.quantile(L.ravel(), _frac(sm.n, quantile))\n    return tonp(L.diag() &gt; treshold)\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.optics_diagnostic","title":"<code>optics_diagnostic(sm, min_size=0.1, **kwargs)</code>","text":"<p>Use a clustering method (<code>sklearn.cluster.OPTICS</code>) to check for problematic data items in the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>min_size</code> <code>Union[float, int]</code> <p>Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.</p> <code>0.1</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>forwaded to OPTICS.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask of problematic data items.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def optics_diagnostic(\n    sm: Slisemap, min_size: Union[float, int] = 0.1, **kwargs: Any\n) -&gt; np.ndarray:\n    \"\"\"Use a clustering method (`sklearn.cluster.OPTICS`) to check for problematic data items in the embedding.\n\n    Args:\n        sm: Trained Slisemap solution.\n        min_size: Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.\n\n    Keyword Args:\n        **kwargs: forwaded to OPTICS.\n\n    Returns:\n        Boolean mask of problematic data items.\n    \"\"\"\n    from sklearn.cluster import OPTICS\n\n    optics = OPTICS(min_samples=min_size, metric=\"euclidean\", n_jobs=-1, **kwargs)\n    return optics.fit(sm.get_Z()).labels_ &lt; 0\n</code></pre>"},{"location":"slisemap.diagnostics/#slisemap.diagnostics.diagnose","title":"<code>diagnose(sm, min_size=0.1, max_size=0.5, max_distance=10.0, conservative=False)</code>","text":"<p>Run multiple diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>min_size</code> <code>Union[float, int]</code> <p>Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.</p> <code>0.1</code> <code>max_size</code> <code>Union[float, int]</code> <p>Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.</p> <code>0.5</code> <code>max_distance</code> <code>float</code> <p>Maximum distance from origo in the embedding. Defaults to 10.0.</p> <code>10.0</code> <code>conservative</code> <code>bool</code> <p>Only run the most conservative diagnostics. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary of the the diagnostics results.</p> Source code in <code>slisemap/diagnostics.py</code> <pre><code>def diagnose(\n    sm: Slisemap,\n    min_size: Union[float, int] = 0.1,\n    max_size: Union[float, int] = 0.5,\n    max_distance: float = 10.0,\n    conservative: bool = False,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run multiple diagnostics.\n\n    Args:\n        sm: Trained Slisemap solution.\n        min_size: Miniumum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.1.\n        max_size: Maximum neighbourhood/cluster size (as a fraction or absolute number). Defaults to 0.5.\n        max_distance: Maximum distance from origo in the embedding. Defaults to 10.0.\n        conservative: Only run the most conservative diagnostics. Defaults to False.\n\n    Returns:\n        Dictionary of the the diagnostics results.\n    \"\"\"\n    if conservative:\n        return {\n            \"Distant\": distant_diagnostic(sm, max_distance),\n            \"Heavyweight\": heavyweight_diagnostic(sm, min_size),\n            \"Lightweight\": lightweight_diagnostic(sm, max_size),\n            \"Weight Neighbourhood\": weight_neighbourhood_diagnostic(\n                sm, min_size, max_size\n            ),\n            \"Quantile Loss\": quantile_loss_diagnostic(sm, max_size),\n        }\n    else:\n        return {\n            \"Distant\": distant_diagnostic(sm, max_distance),\n            \"Heavyweight\": heavyweight_diagnostic(sm, min_size),\n            \"Lightweight\": lightweight_diagnostic(sm, max_size),\n            \"Weight Neighbourhood\": weight_neighbourhood_diagnostic(\n                sm, min_size, max_size\n            ),\n            \"Loss Neighbourhood\": loss_neighbourhood_diagnostic(sm, min_size),\n            \"Global Loss\": global_loss_diagnostic(sm),\n            \"Clustering\": optics_diagnostic(sm, min_size),\n            \"Quantile Loss\": quantile_loss_diagnostic(sm, max_size),\n        }\n</code></pre>"},{"location":"slisemap.escape/","title":"slisemap.escape","text":""},{"location":"slisemap.escape/#slisemap.escape","title":"<code>slisemap.escape</code>","text":"<p>Module that contains alternative escape heuristics.</p>"},{"location":"slisemap.escape/#slisemap.escape.escape_neighbourhood","title":"<code>escape_neighbourhood(X, Y, B, Z, local_model, local_loss, distance, kernel, radius=3.5, force_move=False, **_)</code>","text":"<p>Try to escape a local optimum by moving the data items.</p> <p>Move the data items to the neighbourhoods (embedding and local model) best suited for them. This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>B</code> <code>Tensor</code> <p>Local models.</p> required <code>Z</code> <code>Tensor</code> <p>Embedding matrix.</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function.</p> required <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances.</p> required <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>force_move</code> <code>bool</code> <p>Do not allow the items to pair with themselves. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>B</code> <code>Tensor</code> <p>Escaped <code>B</code>.</p> <code>Z</code> <code>Tensor</code> <p>Escaped <code>Z</code>.</p> Source code in <code>slisemap/escape.py</code> <pre><code>def escape_neighbourhood(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    kernel: Callable[[torch.Tensor], torch.Tensor],\n    radius: float = 3.5,\n    force_move: bool = False,\n    **_: Any,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Try to escape a local optimum by moving the data items.\n\n    Move the data items to the neighbourhoods (embedding and local model) best suited for them.\n    This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        B: Local models.\n        Z: Embedding matrix.\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function.\n        kernel: Kernel for embedding distances.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        force_move: Do not allow the items to pair with themselves. Defaults to True.\n\n    Returns:\n        B: Escaped `B`.\n        Z: Escaped `Z`.\n    \"\"\"\n    L = local_loss(local_model(X, B), Y)\n    if radius &gt; 0:\n        Z2 = Z * (radius / (torch.sqrt(torch.sum(Z**2) / Z.shape[0]) + 1e-8))\n        D = distance(Z2, Z2)\n    else:\n        D = distance(Z, Z)\n    W = kernel(D)\n    # K = torch.zeros_like(L)\n    # for i in range(L.shape[1]):\n    #     K[:, i] = torch.sum(W * L[:, i].ravel()[None, :], 1)\n    # K = torch.sum(W[:, :, None] * L[None, :, :], 1)\n    K = W @ L\n    if force_move:\n        _assert(\n            K.shape[0] == K.shape[1],\n            \"force_move only works if (X, Y) corresponds to (B, Z)\",\n            escape_neighbourhood,\n        )\n        K.fill_diagonal_(np.inf)\n    index = torch.argmin(K, 0)\n    return B.detach()[index].clone(), Z.detach()[index].clone()\n</code></pre>"},{"location":"slisemap.escape/#slisemap.escape.escape_greedy","title":"<code>escape_greedy(X, Y, B, Z, local_model, local_loss, distance, kernel, radius=3.5, force_move=False, **_)</code>","text":"<p>Try to escape a local optimum by moving the data items.</p> <p>Move the data items to a locations with optimal local models. This is done by finding another item (with an optimal local model) and copying its values for Z and B.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>B</code> <code>Tensor</code> <p>Local models.</p> required <code>Z</code> <code>Tensor</code> <p>Embedding matrix.</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function.</p> required <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances.</p> required <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>force_move</code> <code>bool</code> <p>Do not allow the items to pair with themselves. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>B</code> <code>Tensor</code> <p>Escaped <code>B</code>.</p> <code>Z</code> <code>Tensor</code> <p>Escaped <code>Z</code>.</p> Source code in <code>slisemap/escape.py</code> <pre><code>def escape_greedy(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    kernel: Callable[[torch.Tensor], torch.Tensor],\n    radius: float = 3.5,\n    force_move: bool = False,\n    **_: Any,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Try to escape a local optimum by moving the data items.\n\n    Move the data items to a locations with optimal local models.\n    This is done by finding another item (with an optimal local model) and copying its values for Z and B.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        B: Local models.\n        Z: Embedding matrix.\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function.\n        kernel: Kernel for embedding distances.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        force_move: Do not allow the items to pair with themselves. Defaults to True.\n\n    Returns:\n        B: Escaped `B`.\n        Z: Escaped `Z`.\n    \"\"\"\n    L = local_loss(local_model(X, B), Y)\n    if force_move:\n        _assert(\n            L.shape[0] == L.shape[1],\n            \"force_move only works if (X, Y) corresponds to (B, Z)\",\n            escape_greedy,\n        )\n        L.fill_diagonal_(np.inf)\n    index = torch.argmin(L, 0)\n    return B.detach()[index].clone(), Z.detach()[index].clone()\n</code></pre>"},{"location":"slisemap.escape/#slisemap.escape.escape_combined","title":"<code>escape_combined(X, Y, B, Z, local_model, local_loss, distance, kernel, radius=3.5, force_move=False, **_)</code>","text":"<p>Try to escape a local optimum by moving the data items.</p> <p>Move the data items to the neighbourhoods (embedding and local model) best suited for them. This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.</p> <p>This is a combination of escape_neighbourhood and escape_greedy.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>B</code> <code>Tensor</code> <p>Local models.</p> required <code>Z</code> <code>Tensor</code> <p>Embedding matrix.</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function.</p> required <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances.</p> required <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>force_move</code> <code>bool</code> <p>Do not allow the items to pair with themselves. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>B</code> <code>Tensor</code> <p>Escaped <code>B</code>.</p> <code>Z</code> <code>Tensor</code> <p>Escaped <code>Z</code>.</p> Source code in <code>slisemap/escape.py</code> <pre><code>def escape_combined(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    kernel: Callable[[torch.Tensor], torch.Tensor],\n    radius: float = 3.5,\n    force_move: bool = False,\n    **_: Any,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Try to escape a local optimum by moving the data items.\n\n    Move the data items to the neighbourhoods (embedding and local model) best suited for them.\n    This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.\n\n    This is a combination of escape_neighbourhood and escape_greedy.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        B: Local models.\n        Z: Embedding matrix.\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function.\n        kernel: Kernel for embedding distances.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        force_move: Do not allow the items to pair with themselves. Defaults to True.\n\n    Returns:\n        B: Escaped `B`.\n        Z: Escaped `Z`.\n    \"\"\"\n    L = local_loss(local_model(X, B), Y)\n    if radius &gt; 0:\n        Z2 = Z * (radius / (torch.sqrt(torch.sum(Z**2) / Z.shape[0]) + 1e-8))\n        D = distance(Z2, Z2)\n    else:\n        D = distance(Z, Z)\n    W = kernel(D)\n    K = W @ L + L\n    if force_move:\n        _assert(\n            K.shape[0] == K.shape[1],\n            \"force_move only works if (X, Y) corresponds to (B, Z)\",\n            escape_combined,\n        )\n        K.fill_diagonal_(np.inf)\n    index = torch.argmin(K, 0)\n    return B.detach()[index].clone(), Z.detach()[index].clone()\n</code></pre>"},{"location":"slisemap.escape/#slisemap.escape.escape_marginal","title":"<code>escape_marginal(X, Y, B, Z, local_model, local_loss, distance, kernel, radius=3.5, force_move=False, Xold=None, Yold=None, jit=True, **_)</code>","text":"<p>Try to escape a local optimum by moving the data items.</p> <p>Move the data items to locations with optimal marginal losses. This is done by finding another item (where the marginal loss is optimal) and copying its values for Z and B.</p> <p>This might produce better results than <code>escape_neighbourhood</code>, but is really slow.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>B</code> <code>Tensor</code> <p>Local models.</p> required <code>Z</code> <code>Tensor</code> <p>Embedding matrix.</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function.</p> required <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances.</p> required <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>force_move</code> <code>bool</code> <p>Do not allow the items to pair with themselves. Defaults to True.</p> <code>False</code> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function. Defaults to True.</p> <code>True</code> <code>Xold</code> <code>Optional[Tensor]</code> <p>Trained X. Defaults to X.</p> <code>None</code> <code>Yold</code> <code>Optional[Tensor]</code> <p>Trained Y. Defaults to Y.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>B</code> <code>Tensor</code> <p>Escaped <code>B</code>.</p> <code>Z</code> <code>Tensor</code> <p>Escaped <code>Z</code>.</p> Source code in <code>slisemap/escape.py</code> <pre><code>def escape_marginal(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    kernel: Callable[[torch.Tensor], torch.Tensor],\n    radius: float = 3.5,\n    force_move: bool = False,\n    Xold: Optional[torch.Tensor] = None,\n    Yold: Optional[torch.Tensor] = None,\n    jit: bool = True,\n    **_: Any,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Try to escape a local optimum by moving the data items.\n\n    Move the data items to locations with optimal marginal losses.\n    This is done by finding another item (where the marginal loss is optimal) and copying its values for Z and B.\n\n    This might produce better results than `escape_neighbourhood`, but is really slow.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        B: Local models.\n        Z: Embedding matrix.\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function.\n        kernel: Kernel for embedding distances.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        force_move: Do not allow the items to pair with themselves. Defaults to True.\n        jit: Just-In-Time compile the loss function. Defaults to True.\n        Xold: Trained X. Defaults to X.\n        Yold: Trained Y. Defaults to Y.\n\n    Returns:\n        B: Escaped `B`.\n        Z: Escaped `Z`.\n    \"\"\"\n    from slisemap.slisemap import make_marginal_loss\n\n    _assert(\n        not force_move or X.shape[0] == Z.shape[0],\n        \"force_move only works if (X, Y) corresponds to (B, Z)\",\n        escape_marginal,\n    )\n    _assert(\n        Xold is not None or X.shape[0] == Z.shape[0],\n        \"(Xold Yold) is required if (X, Y) does not correspond to (B, Z)\",\n        escape_marginal,\n    )\n    if radius &gt; 0:\n        Z2 = Z * (radius / (torch.sqrt(torch.sum(Z**2) / Z.shape[0]) + 1e-8))\n    else:\n        Z2 = Z\n    lf, set_new = make_marginal_loss(\n        X=X if Xold is None else Xold,\n        Y=Y if Yold is None else Yold,\n        B=B,\n        Z=Z2,\n        Xnew=X[:1],\n        Ynew=Y[:1],\n        local_model=local_model,\n        local_loss=local_loss,\n        distance=distance,\n        kernel=kernel,\n        radius=0.0,\n        lasso=0.0,\n        ridge=0.0,\n        jit=jit and (X.shape[0] &gt; 10 if Xold is None else Xold.shape[0] &gt; 10),\n    )\n    index = []\n    for i in range(X.shape[0]):\n        set_new(X[i : i + 1], Y[i : i + 1])\n        index.append(i)\n        best = np.inf\n        for j in range(Z.shape[0]):\n            if force_move and i == j:\n                continue\n            loss = lf(B[j : j + 1], Z2[j : j + 1])\n            if loss &lt; best:\n                best = loss\n                index[-1] = j\n    return B.detach()[index].clone(), Z.detach()[index].clone()\n</code></pre>"},{"location":"slisemap.local_models/","title":"slisemap.local_models","text":""},{"location":"slisemap.local_models/#slisemap.local_models","title":"<code>slisemap.local_models</code>","text":"<p>Module that contains the built-in alternatives for local white box models.</p> <p>These functions can also be used as templates for implementing your own.</p>"},{"location":"slisemap.local_models/#slisemap.local_models.local_predict","title":"<code>local_predict(X, B, local_model)</code>","text":"<p>Get individual predictions when every data item has a separate model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix [n, m].</p> required <code>B</code> <code>Tensor</code> <p>Coefficient matrix [n, q].</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function: [1, m], [1, q] -&gt; [1, 1, o].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Matrix of local predictions [n, o].</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def local_predict(\n    X: torch.Tensor,\n    B: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n) -&gt; torch.Tensor:\n    \"\"\"Get individual predictions when every data item has a separate model.\n\n    Args:\n        X: Data matrix [n, m].\n        B: Coefficient matrix [n, q].\n        local_model: Prediction function: [1, m], [1, q] -&gt; [1, 1, o].\n\n    Returns:\n        Matrix of local predictions [n, o].\n    \"\"\"\n    n = X.shape[0]\n    _assert(n == B.shape[0], \"X and B must have the same number of rows\", local_predict)\n    y = local_model(X[:1, :], B[:1, :])[0, 0, ...]\n    Y = torch.empty((n, *y.shape), dtype=y.dtype, device=y.device)\n    Y[0, ...] = y\n    for i in range(1, n):\n        Y[i, ...] = local_model(X[i : i + 1, :], B[i : i + 1, :])[0, 0, ...]\n    return Y\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.ALocalModel","title":"<code>ALocalModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for gathering all the functions needed for a local model (predict, loss, coefficients).</p> Source code in <code>slisemap/local_models.py</code> <pre><code>class ALocalModel(ABC):\n    \"\"\"Abstract class for gathering all the functions needed for a local model (predict, loss, coefficients).\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def predict(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Prediction function.\n\n        Args:\n            X: Data matrix.\n            B: Coefficient matrix.\n\n        Returns:\n            Y: Prediction matrix.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    @abstractmethod\n    def loss(Ytilde: torch.Tensor, Y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Loss function.\n\n        Args:\n            Ytilde: Prediction matrix.\n            Y: Target matrix\n\n        Returns:\n            L: Loss matrix.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    @abstractmethod\n    def coefficients(\n        X: Union[torch.Tensor, np.ndarray],\n        Y: Union[torch.Tensor, np.ndarray],\n        intercept: bool,\n    ) -&gt; int:\n        \"\"\"Get for the number of columns of B.\n\n        Args:\n            X: Data matrix.\n            Y: Target matrix.\n            intercept: Add intercept.\n\n        Returns:\n            Number of columns.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def regularisation(\n        X: torch.Tensor,\n        Y: torch.Tensor,\n        B: torch.Tensor,\n        Z: torch.Tensor,\n        Ytilde: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Regularisation function.\n\n        Args:\n            X: Data matrix.\n            Y: Target matrix.\n            B: Coefficient matrix.\n            Z: Embedding matrix.\n            Ytilde: Prediction matrix.\n\n        Returns:\n            Additional loss term.\n        \"\"\"\n        return 0.0\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.ALocalModel.predict","title":"<code>predict(X, B)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Prediction function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>B</code> <code>Tensor</code> <p>Coefficient matrix.</p> required <p>Returns:</p> Name Type Description <code>Y</code> <code>Tensor</code> <p>Prediction matrix.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef predict(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Prediction function.\n\n    Args:\n        X: Data matrix.\n        B: Coefficient matrix.\n\n    Returns:\n        Y: Prediction matrix.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.ALocalModel.loss","title":"<code>loss(Ytilde, Y)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Loss function.</p> <p>Parameters:</p> Name Type Description Default <code>Ytilde</code> <code>Tensor</code> <p>Prediction matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix</p> required <p>Returns:</p> Name Type Description <code>L</code> <code>Tensor</code> <p>Loss matrix.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef loss(Ytilde: torch.Tensor, Y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Loss function.\n\n    Args:\n        Ytilde: Prediction matrix.\n        Y: Target matrix\n\n    Returns:\n        L: Loss matrix.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.ALocalModel.coefficients","title":"<code>coefficients(X, Y, intercept)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Get for the number of columns of B.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Tensor, ndarray]</code> <p>Data matrix.</p> required <code>Y</code> <code>Union[Tensor, ndarray]</code> <p>Target matrix.</p> required <code>intercept</code> <code>bool</code> <p>Add intercept.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of columns.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef coefficients(\n    X: Union[torch.Tensor, np.ndarray],\n    Y: Union[torch.Tensor, np.ndarray],\n    intercept: bool,\n) -&gt; int:\n    \"\"\"Get for the number of columns of B.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        intercept: Add intercept.\n\n    Returns:\n        Number of columns.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.ALocalModel.regularisation","title":"<code>regularisation(X, Y, B, Z, Ytilde)</code>  <code>staticmethod</code>","text":"<p>Regularisation function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>B</code> <code>Tensor</code> <p>Coefficient matrix.</p> required <code>Z</code> <code>Tensor</code> <p>Embedding matrix.</p> required <code>Ytilde</code> <code>Tensor</code> <p>Prediction matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Additional loss term.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>@staticmethod\ndef regularisation(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    Ytilde: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Regularisation function.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        B: Coefficient matrix.\n        Z: Embedding matrix.\n        Ytilde: Prediction matrix.\n\n    Returns:\n        Additional loss term.\n    \"\"\"\n    return 0.0\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.linear_regression","title":"<code>linear_regression(X, B)</code>","text":"<p>Prediction function for (multiple) linear regression.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix [n_x, m].</p> required <code>B</code> <code>Tensor</code> <p>Coefficient Matrix [n_b, m * p].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction tensor [n_b, n_x, p]</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def linear_regression(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Prediction function for (multiple) linear regression.\n\n    Args:\n        X: Data matrix [n_x, m].\n        B: Coefficient Matrix [n_b, m * p].\n\n    Returns:\n        Prediction tensor [n_b, n_x, p]\n    \"\"\"\n    # return (B @ X.T)[:, :, None] # Only for single linear regression\n    return (B.view(B.shape[0], -1, X.shape[1]) @ X.T).transpose(1, 2)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.multiple_linear_regression","title":"<code>multiple_linear_regression(X, B)</code>","text":"<p>Prediction function for multiple linear regression. DEPRECATED.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix [n_x, m].</p> required <code>B</code> <code>Tensor</code> <p>Coefficient Matrix [n_b, m*p].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction tensor [n_b, n_x, p]</p> Deprecated <p>1.4: In favour of a combined <code>linear_regression</code></p> Source code in <code>slisemap/local_models.py</code> <pre><code>def multiple_linear_regression(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Prediction function for multiple linear regression. **DEPRECATED**.\n\n    Args:\n        X: Data matrix [n_x, m].\n        B: Coefficient Matrix [n_b, m*p].\n\n    Returns:\n        Prediction tensor [n_b, n_x, p]\n\n    Deprecated:\n        1.4: In favour of a combined `linear_regression`\n    \"\"\"\n    _deprecated(multiple_linear_regression, linear_regression)\n    return linear_regression(X, B)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.linear_regression_loss","title":"<code>linear_regression_loss(Ytilde, Y, B=None)</code>","text":"<p>Least squares loss function for (multiple) linear regresson.</p> <p>Parameters:</p> Name Type Description Default <code>Ytilde</code> <code>Tensor</code> <p>Predicted values [n_b, n_x, p].</p> required <code>Y</code> <code>Tensor</code> <p>Ground truth values [n_x, p].</p> required <code>B</code> <code>Optional[Tensor]</code> <p>Coefficient matrix. Deprecated. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss values [n_b, n_x].</p> Deprecated <p>1.6: B</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def linear_regression_loss(\n    Ytilde: torch.Tensor, Y: torch.Tensor, B: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Least squares loss function for (multiple) linear regresson.\n\n    Args:\n        Ytilde: Predicted values [n_b, n_x, p].\n        Y: Ground truth values [n_x, p].\n        B: Coefficient matrix. **Deprecated**. Defaults to None.\n\n    Returns:\n        Loss values [n_b, n_x].\n\n    Deprecated:\n        1.6: B\n    \"\"\"\n    return ((Ytilde - Y.expand(Ytilde.shape)) ** 2).sum(dim=-1)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.linear_regression_coefficients","title":"<code>linear_regression_coefficients(X, Y, intercept=False)</code>","text":"<p>Get the number of coefficients for a (multiple) linear regression.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Tensor, ndarray]</code> <p>Data matrix.</p> required <code>Y</code> <code>Union[Tensor, ndarray]</code> <p>Target matrix.</p> required <code>intercept</code> <code>bool</code> <p>Add an (additional) intercept to X. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of coefficients (columns of B).</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def linear_regression_coefficients(\n    X: Union[torch.Tensor, np.ndarray],\n    Y: Union[torch.Tensor, np.ndarray],\n    intercept: bool = False,\n) -&gt; int:\n    \"\"\"Get the number of coefficients for a (multiple) linear regression.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        intercept: Add an (additional) intercept to X. Defaults to False.\n\n    Returns:\n        Number of coefficients (columns of B).\n    \"\"\"\n    return (X.shape[1] + intercept) * (1 if len(Y.shape) &lt; 2 else Y.shape[1])\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.LinearRegression","title":"<code>LinearRegression</code>","text":"<p>             Bases: <code>ALocalModel</code></p> <p>A class that contains all the functions needed for linear regression.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>class LinearRegression(ALocalModel):\n    \"\"\"A class that contains all the functions needed for linear regression.\"\"\"\n\n    predict = linear_regression\n    loss = linear_regression_loss\n    coefficients = linear_regression_coefficients\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.absolute_error","title":"<code>absolute_error(Ytilde, Y, B=None)</code>","text":"<p>Absolute error function for (multiple) linear regresson.</p> <p>Parameters:</p> Name Type Description Default <code>Ytilde</code> <code>Tensor</code> <p>Predicted values [n_b, n_x, p].</p> required <code>Y</code> <code>Tensor</code> <p>Ground truth values [n_x, p].</p> required <code>B</code> <code>Optional[Tensor]</code> <p>Coefficient matrix. Deprecated. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss values [n_b, n_x].</p> Deprecated <p>1.6: B</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def absolute_error(\n    Ytilde: torch.Tensor, Y: torch.Tensor, B: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Absolute error function for (multiple) linear regresson.\n\n    Args:\n        Ytilde: Predicted values [n_b, n_x, p].\n        Y: Ground truth values [n_x, p].\n        B: Coefficient matrix. **Deprecated**. Defaults to None.\n\n    Returns:\n        Loss values [n_b, n_x].\n\n    Deprecated:\n        1.6: B\n    \"\"\"\n    return torch.abs(Ytilde - Y.expand(Ytilde.shape)).sum(dim=-1)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.LinearAbsoluteRegression","title":"<code>LinearAbsoluteRegression</code>","text":"<p>             Bases: <code>ALocalModel</code></p> <p>A class that contains all the functions needed for linear regression with absolute errors.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>class LinearAbsoluteRegression(ALocalModel):\n    \"\"\"A class that contains all the functions needed for linear regression with absolute errors.\"\"\"\n\n    predict = linear_regression\n    loss = absolute_error\n    coefficients = linear_regression_coefficients\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.logistic_regression","title":"<code>logistic_regression(X, B)</code>","text":"<p>Prediction function for (multinomial) logistic regression.</p> <p>Note that the number of coefficients is <code>m * (p-1)</code> due to the normalisation of softmax.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix [n_x, m].</p> required <code>B</code> <code>Tensor</code> <p>Coefficient Matrix [n_b, m*(p-1)].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction tensor [n_b, n_x, p]</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def logistic_regression(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Prediction function for (multinomial) logistic regression.\n\n    Note that the number of coefficients is `m * (p-1)` due to the normalisation of softmax.\n\n    Args:\n        X: Data matrix [n_x, m].\n        B: Coefficient Matrix [n_b, m*(p-1)].\n\n    Returns:\n        Prediction tensor [n_b, n_x, p]\n    \"\"\"\n    n_x, m = X.shape\n    n_b, o = B.shape\n    p = 1 + torch.div(o, m, rounding_mode=\"trunc\")\n    a = torch.zeros([n_b, n_x, p], device=B.device, dtype=B.dtype)\n    for i in range(p - 1):\n        a[:, :, i] = B[:, (i * m) : ((i + 1) * m)] @ X.T\n    return softmax(a, 2)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.logistic_regression_loss","title":"<code>logistic_regression_loss(Ytilde, Y, B=None)</code>","text":"<p>Squared Hellinger distance function for (multinomial) logistic regression.</p> <p>Parameters:</p> Name Type Description Default <code>Ytilde</code> <code>Tensor</code> <p>Predicted values [n_b, n_x, p].</p> required <code>Y</code> <code>Tensor</code> <p>Ground truth values [n_x, p].</p> required <code>B</code> <code>Optional[Tensor]</code> <p>Coefficient matrix. Deprecated. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss values [n_b, n_x].</p> Deprecated <p>1.6: B</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def logistic_regression_loss(\n    Ytilde: torch.Tensor, Y: torch.Tensor, B: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Squared Hellinger distance function for (multinomial) logistic regression.\n\n    Args:\n        Ytilde: Predicted values [n_b, n_x, p].\n        Y: Ground truth values [n_x, p].\n        B: Coefficient matrix. **Deprecated**. Defaults to None.\n\n    Returns:\n        Loss values [n_b, n_x].\n\n    Deprecated:\n        1.6: B\n    \"\"\"\n    _assert_no_trace(\n        lambda: (\n            Ytilde.shape[-1] &lt;= Y.shape[-1],\n            f\"Too few columns in Y: {Y.shape[-1]} &lt; {Ytilde.shape[-1]}\",\n        ),\n        logistic_regression_loss,\n    )\n    return ((Ytilde.sqrt() - Y.sqrt().expand(Ytilde.shape)) ** 2).sum(dim=-1) * 0.5\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.logistic_regression_coefficients","title":"<code>logistic_regression_coefficients(X, Y, intercept=False)</code>","text":"<p>Get the number of coefficients for a (multinomial) logistic regression.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Tensor, ndarray]</code> <p>Data matrix.</p> required <code>Y</code> <code>Union[Tensor, ndarray]</code> <p>Target matrix.</p> required <code>intercept</code> <code>bool</code> <p>Add an (additional) intercept to X. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of coefficients (columns of B).</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def logistic_regression_coefficients(\n    X: Union[torch.Tensor, np.ndarray],\n    Y: Union[torch.Tensor, np.ndarray],\n    intercept: bool = False,\n) -&gt; int:\n    \"\"\"Get the number of coefficients for a (multinomial) logistic regression.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        intercept: Add an (additional) intercept to X. Defaults to False.\n\n    Returns:\n        Number of coefficients (columns of B).\n    \"\"\"\n    return (X.shape[1] + intercept) * max(1, Y.shape[1] - 1)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.LogisticRegression","title":"<code>LogisticRegression</code>","text":"<p>             Bases: <code>ALocalModel</code></p> <p>A class that contains all the functions needed for logistic regression.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>class LogisticRegression(ALocalModel):\n    \"\"\"A class that contains all the functions needed for logistic regression.\"\"\"\n\n    predict = logistic_regression\n    loss = logistic_regression_loss\n    coefficients = logistic_regression_coefficients\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.logistic_regression_log","title":"<code>logistic_regression_log(X, B)</code>","text":"<p>Prediction function for (multinomial) logistic regression that returns the log of the prediction.</p> <p>Note that the number of coefficients is <code>m * (p-1)</code> due to the normalisation of softmax.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix [n_x, m].</p> required <code>B</code> <code>Tensor</code> <p>Coefficient Matrix [n_b, m*(p-1)].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction tensor [n_b, n_x, p]</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def logistic_regression_log(X: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Prediction function for (multinomial) logistic regression that returns the **log of the prediction**.\n\n    Note that the number of coefficients is `m * (p-1)` due to the normalisation of softmax.\n\n    Args:\n        X: Data matrix [n_x, m].\n        B: Coefficient Matrix [n_b, m*(p-1)].\n\n    Returns:\n        Prediction tensor [n_b, n_x, p]\n    \"\"\"\n    n_x, m = X.shape\n    n_b, o = B.shape\n    p = 1 + torch.div(o, m, rounding_mode=\"trunc\")\n    a = torch.zeros([n_b, n_x, p], device=B.device, dtype=B.dtype)\n    for i in range(p - 1):\n        a[:, :, i] = B[:, (i * m) : ((i + 1) * m)] @ X.T\n    return a - torch.logsumexp(a, 2, True)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.logistic_regression_log_loss","title":"<code>logistic_regression_log_loss(Ytilde, Y, B=None)</code>","text":"<p>Cross entropy loss function for (multinomial) logistic regression.</p> <p>Note that this loss function expects <code>Ytilde</code> to be the log of the predicted probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>Ytilde</code> <code>Tensor</code> <p>Predicted logits [n_b, n_x, p].</p> required <code>Y</code> <code>Tensor</code> <p>Ground truth values [n_x, p].</p> required <code>B</code> <code>Optional[Tensor]</code> <p>Coefficient matrix. Deprecated. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss values [n_b, n_x].</p> Deprecated <p>1.6: B</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def logistic_regression_log_loss(\n    Ytilde: torch.Tensor, Y: torch.Tensor, B: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Cross entropy loss function for (multinomial) logistic regression.\n\n    Note that this loss function expects `Ytilde` to be the **log of the predicted probabilities**.\n\n    Args:\n        Ytilde: Predicted logits [n_b, n_x, p].\n        Y: Ground truth values [n_x, p].\n        B: Coefficient matrix. **Deprecated**. Defaults to None.\n\n    Returns:\n        Loss values [n_b, n_x].\n\n    Deprecated:\n        1.6: B\n    \"\"\"\n    _assert_no_trace(\n        lambda: (\n            Ytilde.shape[-1] &lt;= Y.shape[-1],\n            f\"Too few columns in Y: {Y.shape[-1]} &lt; {Ytilde.shape[-1]}\",\n        ),\n        logistic_regression_loss,\n    )\n    return torch.sum(-Y * Ytilde - (1 - Y) * torch.log1p(-torch.exp(Ytilde)), -1)\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.LogisticLogRegression","title":"<code>LogisticLogRegression</code>","text":"<p>             Bases: <code>ALocalModel</code></p> <p>A class that contains all the functions needed for logistic regression.</p> <p>The predictions are in log-space rather than probabilities for numerical stability.</p> Source code in <code>slisemap/local_models.py</code> <pre><code>class LogisticLogRegression(ALocalModel):\n    \"\"\"A class that contains all the functions needed for logistic regression.\n\n    The predictions are in log-space rather than probabilities for numerical stability.\n    \"\"\"\n\n    predict = logistic_regression_log\n    loss = logistic_regression_log_loss\n    coefficients = logistic_regression_coefficients\n</code></pre>"},{"location":"slisemap.local_models/#slisemap.local_models.identify_local_model","title":"<code>identify_local_model(local_model, local_loss=None, coefficients=None, regularisation=None)</code>","text":"<p>Identify the \"predict\", \"loss\", and \"coefficients\" functions for a local model.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Union[LocalModelCollection, CallableLike[predict]]</code> <p>A instance/subclass of <code>ALocalModel</code>, a predict function, or a tuple of functions.</p> required <code>local_loss</code> <code>Optional[CallableLike[loss]]</code> <p>A loss function or None if it is part of <code>local_model</code>. Defaults to None.</p> <code>None</code> <code>coefficients</code> <code>Union[None, int, CallableLike[coefficients]]</code> <p>The number of coefficients, or a function giving that number, or None if it is part of <code>local_model</code>. Defaults to None.</p> <code>None</code> <code>regularisation</code> <code>Union[None, CallableLike[regularisation]]</code> <p>Additional regularisation function. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predict</code> <code>Callable</code> <p>\"prediction\" function (takes X and B and returns predicted Y for every X and B combination).</p> <code>loss</code> <code>Callable</code> <p>\"loss\" function (takes predicted Y and real Y and returns the loss).</p> <code>coefficients</code> <code>Callable</code> <p>\"coefficients\" function (takes X and Y and returns the number of coefficients for B).</p> <code>regularisation</code> <code>Callable</code> <p>\"regularisation\" function (takes X, Y, B, Z, and, Ytilde and returns an additional loss scalar).</p> Source code in <code>slisemap/local_models.py</code> <pre><code>def identify_local_model(\n    local_model: Union[LocalModelCollection, CallableLike[ALocalModel.predict]],\n    local_loss: Optional[CallableLike[ALocalModel.loss]] = None,\n    coefficients: Union[None, int, CallableLike[ALocalModel.coefficients]] = None,\n    regularisation: Union[None, CallableLike[ALocalModel.regularisation]] = None,\n) -&gt; Tuple[Callable, Callable, Callable, Callable]:\n    \"\"\"Identify the \"predict\", \"loss\", and \"coefficients\" functions for a local model.\n\n    Args:\n        local_model: A instance/subclass of `ALocalModel`, a predict function, or a tuple of functions.\n        local_loss: A loss function or None if it is part of `local_model`. Defaults to None.\n        coefficients: The number of coefficients, or a function giving that number, or None if it is part of `local_model`. Defaults to None.\n        regularisation: Additional regularisation function. Defaults to None.\n\n    Returns:\n        predict: \"prediction\" function (takes X and B and returns predicted Y for every X and B combination).\n        loss: \"loss\" function (takes predicted Y and real Y and returns the loss).\n        coefficients: \"coefficients\" function (takes X and Y and returns the number of coefficients for B).\n        regularisation: \"regularisation\" function (takes X, Y, B, Z, and, Ytilde and returns an additional loss scalar).\n    \"\"\"\n    pred_fn = None\n    loss_fn = None\n    coef_fn = linear_regression_coefficients\n    regu_fn = ALocalModel.regularisation\n    if isinstance(local_model, ALocalModel) or (\n        isinstance(local_model, type) and issubclass(local_model, ALocalModel)\n    ):\n        pred_fn = local_model.predict\n        loss_fn = local_model.loss\n        coef_fn = local_model.coefficients\n        regu_fn = local_model.regularisation\n    elif callable(local_model):\n        pred_fn = local_model\n        if local_model in (linear_regression, multiple_linear_regression):\n            loss_fn = linear_regression_loss\n            coef_fn = linear_regression_coefficients\n        elif local_model == logistic_regression:\n            loss_fn = logistic_regression_loss\n            coef_fn = logistic_regression_coefficients\n        elif local_model == logistic_regression_log:\n            loss_fn = logistic_regression_log_loss\n            coef_fn = logistic_regression_coefficients\n        else:\n            loss_fn = local_loss\n            coef_fn = coefficients\n    elif isinstance(local_model, Sequence):\n        pred_fn = local_model[0]\n        loss_fn = local_model[1] if len(local_model) &gt; 1 else loss_fn\n        coef_fn = local_model[2] if len(local_model) &gt; 2 else coef_fn\n        regu_fn = local_model[3] if len(local_model) &gt; 3 else regu_fn\n    else:\n        _warn(\"Could not identity the local model\", identify_local_model)\n        pred_fn = local_model\n    if local_loss is not None:\n        loss_fn = local_loss\n    if coefficients is not None:\n        coef_fn = coefficients\n    if regularisation is not None:\n        regu_fn = regularisation\n    if isinstance(coef_fn, int):\n        i_coef = coef_fn\n        coef_fn = lambda X, Y: i_coef  # noqa: E731\n    _assert(pred_fn is not None, \"`local_model` function missing\")\n    _assert(loss_fn is not None, \"`local_loss` function missing\")\n    return pred_fn, loss_fn, coef_fn, regu_fn\n</code></pre>"},{"location":"slisemap/","title":"slisemap","text":""},{"location":"slisemap/#slisemap","title":"<code>slisemap</code>","text":""},{"location":"slisemap/#slisemap--slisemap-combine-local-explanations-with-supervised-dimensionality-reduction","title":"SLISEMAP: Combine local explanations with supervised dimensionality reduction.","text":"<p>SLISEMAP is a supervised dimensionality reduction method, that takes data, in the form of vectors, and predictions from a \"black box\" regression or classification model as input. SLISEMAP then simultaneously finds local explanations for all data items and builds a (typically) two-dimensional global visualisation of the black box model such that data items with similar local explanations are projected nearby. The explanations consists of interpretable models that locally approximate the \"black box\" model.</p> <p>SLISEMAP uses PyTorch for efficient optimisation, and optional GPU-acceleration. For more information see the the repository, the documentation, or the paper.</p> <p>This module also includes the faster SLIPMAP variant, that uses \"prototypes\" to speed up the calculations (linear time and memory complexity instead of quadratic). SLIPMAP is largely compatible with SLISEMAP, just change the class name (<code>Slisemap</code> to <code>Slipmap</code>). For more information, see the sources above and the paper.</p>"},{"location":"slisemap/#slisemap--documentation","title":"Documentation","text":"<ul> <li>Slisemap</li> <li>Slipmap</li> </ul>"},{"location":"slisemap/#slisemap--citations","title":"Citations","text":"<p>Bj\u00f6rklund, A., M\u00e4kel\u00e4, J. &amp; Puolam\u00e4ki, K. (2023). SLISEMAP: Supervised dimensionality reduction through local explanations. Machine Learning 112, 1-43. DOI: 10.1007/s10994-022-06261-1.  </p> <p>Bj\u00f6rklund, A., Sepp\u00e4l\u00e4inen, L., &amp; Puolam\u00e4ki, K. (2024). SLIPMAP: Fast and Robust Manifold Visualisation for Explainable AI Advances in Intelligent Data Analysis XXII, pp. 223-235, LNCS 14642. DOI: 10.1007/978-3-031-58553-1_18.  </p>"},{"location":"slisemap/#slisemap--example-usage","title":"Example Usage","text":"<pre><code>from slisemap import Slipmap\nimport numpy as np\n\nX = np.array([[0.1,0.5,0.7], [0.8,0.9,1], [0.8,0.5,0.3], [0.1,0.2,0.3], [1,2,5], [2,3,4], [2,0,1]])\ny = np.array([1, 2, 3, 4, 1.5, 1.8, 1.7])\nsm = Slipmap(X, y, radius=2.0, lasso=1e-4, ridge=2e-4)\nsm.optimise()\nsm.plot()\n</code></pre>"},{"location":"slisemap.metrics/","title":"slisemap.metrics","text":""},{"location":"slisemap.metrics/#slisemap.metrics","title":"<code>slisemap.metrics</code>","text":"<p>Module that contains functions that can be used to evaluate SLISEMAP solutions.</p> <p>The functions take a solution (plus other arguments) and returns a single float. This float should either be minimised or maximised for best results (see individual functions).</p>"},{"location":"slisemap.metrics/#slisemap.metrics.nanmean","title":"<code>nanmean(x)</code>","text":"<p>Compute the mean, ignoring any nan.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def nanmean(x: np.ndarray) -&gt; float:\n    \"\"\"Compute the mean, ignoring any nan.\"\"\"\n    mask = np.isfinite(x)\n    if np.all(mask):\n        return np.mean(x)\n    elif not np.any(mask):\n        return np.nan\n    else:\n        return np.mean(x[mask])\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.euclidean_nearest_neighbours","title":"<code>euclidean_nearest_neighbours(D, index, k=0.1, include_self=True)</code>","text":"<p>Find the k nearest neighbours using euclidean distance.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix.</p> required <code>index</code> <code>int</code> <p>The item (row), for which to find neighbours.</p> required <code>k</code> <code>Union[int, float]</code> <p>The number of neighbours to find. Defaults to 0.1.</p> <code>0.1</code> <code>include_self</code> <code>bool</code> <p>include the item in its' neighbourhood. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LongTensor</code> <p>Vector of indices for the neighbours.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def euclidean_nearest_neighbours(\n    D: torch.Tensor, index: int, k: Union[int, float] = 0.1, include_self: bool = True\n) -&gt; torch.LongTensor:\n    \"\"\"Find the k nearest neighbours using euclidean distance.\n\n    Args:\n        D: Distance matrix.\n        index: The item (row), for which to find neighbours.\n        k: The number of neighbours to find. Defaults to 0.1.\n        include_self: include the item in its' neighbourhood. Defaults to True.\n\n    Returns:\n        Vector of indices for the neighbours.\n    \"\"\"\n    if isinstance(k, float) and 0 &lt; k &lt;= 1:\n        k = int(k * D.shape[0])\n    if include_self:\n        return torch.argsort(D[index])[:k]\n    else:\n        dist = D[index].clone()\n        if not include_self:\n            dist[index] = np.inf\n            if k == D.shape[0]:\n                k -= 1\n        return torch.argsort(dist)[:k]\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.kernel_neighbours","title":"<code>kernel_neighbours(D, index, epsilon=1.0, include_self=True)</code>","text":"<p>Find the neighbours using a softmax kernel.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix.</p> required <code>index</code> <code>int</code> <p>The item for which we want to find neighbours.</p> required <code>epsilon</code> <code>float</code> <p>Treshold for selecting the neighbourhood (will be divided by <code>n</code>). Defaults to 1.0.</p> <code>1.0</code> <code>include_self</code> <code>bool</code> <p>include the item in its' neighbourhood. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LongTensor</code> <p>Vector of indices for the neighbours.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def kernel_neighbours(\n    D: torch.Tensor, index: int, epsilon: float = 1.0, include_self: bool = True\n) -&gt; torch.LongTensor:\n    \"\"\"Find the neighbours using a softmax kernel.\n\n    Args:\n        D: Distance matrix.\n        index: The item for which we want to find neighbours.\n        epsilon: Treshold for selecting the neighbourhood (will be divided by `n`). Defaults to 1.0.\n        include_self: include the item in its' neighbourhood. Defaults to True.\n\n    Returns:\n        Vector of indices for the neighbours.\n    \"\"\"\n    K = torch.nn.functional.softmax(-D[index], 0)\n    epsilon2 = epsilon / K.numel()\n    if include_self:\n        return torch.where(epsilon2 &lt;= K)[0]\n    else:\n        mask = epsilon2 &lt;= K\n        mask[index] = False\n        return torch.where(mask)[0]\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.cluster_neighbours","title":"<code>cluster_neighbours(D, index, clusters, include_self=True)</code>","text":"<p>Find the neighbours with given clusters.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix (ignored).</p> required <code>index</code> <code>int</code> <p>The item for which we want to find neighbours.</p> required <code>clusters</code> <code>LongTensor</code> <p>Cluster id:s for the data items.</p> required <code>include_self</code> <code>bool</code> <p>include the item in its' neighbourhood. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LongTensor</code> <p>Vector of indices for the neighbours.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def cluster_neighbours(\n    D: torch.Tensor,\n    index: int,\n    clusters: torch.LongTensor,\n    include_self: bool = True,\n) -&gt; torch.LongTensor:\n    \"\"\"Find the neighbours with given clusters.\n\n    Args:\n        D: Distance matrix (ignored).\n        index: The item for which we want to find neighbours.\n        clusters: Cluster id:s for the data items.\n        include_self: include the item in its' neighbourhood. Defaults to True.\n\n    Returns:\n        Vector of indices for the neighbours.\n    \"\"\"\n    if include_self:\n        return torch.where(clusters == clusters[index])[0]\n    else:\n        mask = clusters == clusters[index]\n        mask[index] = False\n        return torch.where(mask)[0]\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.radius_neighbours","title":"<code>radius_neighbours(D, index, radius=None, quantile=0.2, include_self=True)</code>","text":"<p>Find the neighbours within a radius.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix (ignored).</p> required <code>index</code> <code>int</code> <p>The item for which we want to find neighbours.</p> required <code>radius</code> <code>Optional[float]</code> <p>The radius of the neighbourhood. Defaults to None.</p> <code>None</code> <code>quantile</code> <code>float</code> <p>If radius is None then radius is set to the quantile of D. Defaults to 0.2.</p> <code>0.2</code> <code>include_self</code> <code>bool</code> <p>include the item in its' neighbourhood. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LongTensor</code> <p>Vector of indices for the neighbours.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def radius_neighbours(\n    D: torch.Tensor,\n    index: int,\n    radius: Optional[float] = None,\n    quantile: float = 0.2,\n    include_self: bool = True,\n) -&gt; torch.LongTensor:\n    \"\"\"Find the neighbours within a radius.\n\n    Args:\n        D: Distance matrix (ignored).\n        index: The item for which we want to find neighbours.\n        radius: The radius of the neighbourhood. Defaults to None.\n        quantile: If radius is None then radius is set to the quantile of D. Defaults to 0.2.\n        include_self: include the item in its' neighbourhood. Defaults to True.\n\n    Returns:\n        Vector of indices for the neighbours.\n    \"\"\"\n    if radius is None:\n        radius = torch.quantile(D, quantile)\n    if include_self:\n        return torch.where(D[index] &lt;= radius)[0]\n    else:\n        mask = D[index] &lt;= radius\n        mask[index] = False\n        return torch.where(mask)[0]\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.Neighbours","title":"<code>Neighbours = Union[None, np.ndarray, torch.LongTensor, Callable[[torch.Tensor, int], torch.LongTensor]]</code>  <code>module-attribute</code>","text":"<p>Type annotation for specifying neighbouring items. Used in the get_neighbours function.</p> <ul> <li>If None, every item is or is not a neighbour.</li> <li>If a vector of cluster id:s, take neighbours from the same cluter.</li> <li>Or a function that gives neighbours (that takes a distance matrix and an index), primarily:<ul> <li>euclidean_nearest_neighbours</li> <li>kernel_neighbours</li> <li>cluster_neighbours</li> <li>radius_neighbours</li> </ul> </li> </ul>"},{"location":"slisemap.metrics/#slisemap.metrics.get_neighbours","title":"<code>get_neighbours(sm, neighbours, full_if_none=False, **kwargs)</code>","text":"<p>Create a function that takes the index of an item and returns the indices of its neighbours.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Union[Slisemap, Tensor]</code> <p>Trained Slisemap solution or an embedding vector (like Slisemap.Z).</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (return self), a vector of cluster id:s (take neighbours from the same cluter), or a function that gives neighbours (that takes a distance matrix and an index).</p> required <code>full_if_none</code> <code>bool</code> <p>If <code>neighbours</code> is None, return the whole dataset. Defaults to False.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>Callable[[int], LongTensor]</code> <p>Function that takes an index and returns neighbour indices.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def get_neighbours(\n    sm: Union[Slisemap, torch.Tensor],\n    neighbours: Neighbours,\n    full_if_none: bool = False,\n    **kwargs: Any,\n) -&gt; Callable[[int], torch.LongTensor]:\n    \"\"\"Create a function that takes the index of an item and returns the indices of its neighbours.\n\n    Args:\n        sm: Trained Slisemap solution or an embedding vector (like Slisemap.Z).\n        neighbours: Either None (return self), a vector of cluster id:s (take neighbours from the same cluter), or a function that gives neighbours (that takes a distance matrix and an index).\n        full_if_none: If `neighbours` is None, return the whole dataset. Defaults to False.\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        Function that takes an index and returns neighbour indices.\n    \"\"\"\n    if neighbours is None:\n        if full_if_none:\n            try:\n                n = sm.n\n            except AttributeError:\n                n = sm.shape[0]\n            return lambda i: torch.arange(n)\n        else:\n            return lambda i: torch.LongTensor([i])\n    if callable(neighbours):\n        try:\n            D = sm.get_D(numpy=False)\n        except AttributeError:\n            D = torch.cdist(sm, sm)\n        return lambda i: neighbours(D, i, **kwargs)\n    else:\n        neighbours = torch.as_tensor(neighbours)\n        return lambda i: cluster_neighbours(None, i, neighbours, **kwargs)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.slisemap_loss","title":"<code>slisemap_loss(sm)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the loss.</p> <p>Smaller is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def slisemap_loss(sm: Slisemap) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the loss.\n\n    Smaller is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n\n    Returns:\n        The loss value.\n    \"\"\"\n    return sm.value()\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.entropy","title":"<code>entropy(sm, aggregate=True, numpy=True)</code>","text":"<p>Compute row-wise entropy of the <code>W</code> matrix induced by <code>Z</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>aggregate</code> <code>bool</code> <p>Aggregate the row-wise entropies into one scalar. Defaults to True.</p> <code>True</code> <code>numpy</code> <code>bool</code> <p>Return a <code>numpy.ndarray</code> or <code>float</code> instead of a <code>torch.Tensor</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[float, ndarray, Tensor]</code> <p>The entropy.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def entropy(\n    sm: Slisemap, aggregate: bool = True, numpy: bool = True\n) -&gt; Union[float, np.ndarray, torch.Tensor]:\n    \"\"\"Compute row-wise entropy of the `W` matrix induced by `Z`.\n\n    Args:\n        sm: Trained Slisemap solution.\n        aggregate: Aggregate the row-wise entropies into one scalar. Defaults to True.\n        numpy: Return a `numpy.ndarray` or `float` instead of a `torch.Tensor`. Defaults to True.\n\n    Returns:\n        The entropy.\n    \"\"\"\n    W = sm.get_W(numpy=False)\n    entropy = -(W * W.log()).sum(dim=1)\n    if aggregate:\n        entropy = entropy.mean().exp() / sm.n\n        return entropy.cpu().item() if numpy else entropy\n    else:\n        return tonp(entropy) if numpy else entropy\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.slisemap_entropy","title":"<code>slisemap_entropy(sm)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the entropy. DEPRECATED.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The embedding entropy.</p> Deprecated <p>1.4: Use entropy instead.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def slisemap_entropy(sm: Slisemap) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the entropy. **DEPRECATED**.\n\n    Args:\n        sm: Trained Slisemap solution.\n\n    Returns:\n        The embedding entropy.\n\n    Deprecated:\n        1.4: Use [entropy][slisemap.metrics.entropy] instead.\n    \"\"\"\n    _deprecated(slisemap_entropy, entropy)\n    return entropy(sm, aggregate=True, numpy=True)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.fidelity","title":"<code>fidelity(sm, neighbours=None, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the fidelity (loss per item/neighbourhood).</p> <p>Smaller is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (only corresponding local model), a vector of cluster id:s, or a function that gives neighbours (see get_neighbours).</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean loss.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def fidelity(sm: Slisemap, neighbours: Neighbours = None, **kwargs: Any) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the fidelity (loss per item/neighbourhood).\n\n    Smaller is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        neighbours: Either None (only corresponding local model), a vector of cluster id:s, or a function that gives neighbours (see [get_neighbours][slisemap.metrics.get_neighbours]).\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        The mean loss.\n    \"\"\"\n    neighbours = get_neighbours(sm, neighbours, full_if_none=False, **kwargs)\n    results = np.zeros(sm.n)\n    L = sm.get_L(numpy=False)\n    for i in range(len(results)):\n        ni = neighbours(i)\n        if ni.numel() == 0:\n            results[i] = np.nan\n        else:\n            results[i] = torch.mean(L[i, ni]).cpu().detach().item()\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.coverage","title":"<code>coverage(sm, max_loss, neighbours=None, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the coverage.</p> <p>Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>max_loss</code> <code>float</code> <p>Maximum tolerable loss.</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (all), a vector of cluster id:s, or a function that gives neighbours (see get_neighbours).</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean fraction of items within the error bound.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def coverage(\n    sm: Slisemap, max_loss: float, neighbours: Neighbours = None, **kwargs: Any\n) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the coverage.\n\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        max_loss: Maximum tolerable loss.\n        neighbours: Either None (all), a vector of cluster id:s, or a function that gives neighbours (see [get_neighbours][slisemap.metrics.get_neighbours]).\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        The mean fraction of items within the error bound.\n    \"\"\"\n    if torch.all(torch.isnan(sm.get_B(numpy=False).sum(1))).cpu().item():\n        return np.nan\n    neighbours = get_neighbours(sm, neighbours, full_if_none=True, **kwargs)\n    results = np.zeros(sm.n)\n    L = sm.get_L(numpy=False)\n    for i in range(len(results)):\n        ni = neighbours(i)\n        if ni.numel() == 0:\n            results[i] = np.nan\n        else:\n            results[i] = np.mean(tonp(L[i, ni] &lt; max_loss))\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.median_loss","title":"<code>median_loss(sm, neighbours=None, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the median loss.</p> <p>Smaller is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (all), a vector of cluster id:s, or a function that gives neighbours (see get_neighbours).</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean median loss.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def median_loss(sm: Slisemap, neighbours: Neighbours = None, **kwargs: Any) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the median loss.\n\n    Smaller is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        neighbours: Either None (all), a vector of cluster id:s, or a function that gives neighbours (see [get_neighbours][slisemap.metrics.get_neighbours]).\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        The mean median loss.\n    \"\"\"\n    neighbours = get_neighbours(sm, neighbours, full_if_none=True, **kwargs)\n    results = np.zeros(sm.n)\n    L = sm.get_L(numpy=False)\n    for i in range(len(results)):\n        ni = neighbours(i)\n        if ni.numel() == 0:\n            results[i] = np.nan\n        else:\n            results[i] = _non_crashing_median(L[i, ni])\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.coherence","title":"<code>coherence(sm, neighbours=None, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the coherence (max change in prediction divided by the change in variable values).</p> <p>Smaller is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (all), a vector of cluster id:s, or a function that gives neighbours (see get_neighbours).</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean coherence.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def coherence(sm: Slisemap, neighbours: Neighbours = None, **kwargs: Any) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the coherence (max change in prediction divided by the change in variable values).\n\n    Smaller is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        neighbours: Either None (all), a vector of cluster id:s, or a function that gives neighbours (see [get_neighbours][slisemap.metrics.get_neighbours]).\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        The mean coherence.\n    \"\"\"\n    neighbours = get_neighbours(\n        sm, neighbours, full_if_none=True, include_self=False, **kwargs\n    )\n    results = np.zeros(sm.n)\n    P = sm.local_model(sm._X, sm.get_B(numpy=False))\n    for i in range(len(results)):\n        ni = neighbours(i)\n        if ni.numel() == 0:\n            results[i] = np.nan\n        else:\n            dP = torch.sum((P[None, i, i] - P[ni, i] - P[i, ni] + P[ni, ni]) ** 2, 1)\n            dX = torch.sum((sm._X[None, i, :] - sm._X[ni, :]) ** 2, 1) + 1e-8\n            results[i] = torch.sqrt(torch.max(dP / dX))\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.stability","title":"<code>stability(sm, neighbours=None, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the stability (max change in the local model divided by the change in variable values).</p> <p>Smaller is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>neighbours</code> <code>Neighbours</code> <p>Either None (all), a vector of cluster id:s, or a function that gives neighbours (see get_neighbours).</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments passed on to <code>neighbours</code> (if it is a function).</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean stability.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def stability(sm: Slisemap, neighbours: Neighbours = None, **kwargs: Any) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the stability (max change in the local model divided by the change in variable values).\n\n    Smaller is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        neighbours: Either None (all), a vector of cluster id:s, or a function that gives neighbours (see [get_neighbours][slisemap.metrics.get_neighbours]).\n\n    Keyword Args:\n        **kwargs: Arguments passed on to `neighbours` (if it is a function).\n\n    Returns:\n        The mean stability.\n    \"\"\"\n    neighbours = get_neighbours(\n        sm, neighbours, full_if_none=True, include_self=False, **kwargs\n    )\n    results = np.zeros(sm.n)\n    B = sm.get_B(numpy=False)\n    X = sm.get_X(numpy=False)\n    for i in range(len(results)):\n        ni = neighbours(i)\n        if ni.numel() == 0:\n            results[i] = np.nan\n        else:\n            dB = torch.sum((B[None, i, :] - B[ni, :]) ** 2, 1)\n            dX = torch.sum((X[None, i, :] - X[ni, :]) ** 2, 1) + 1e-8\n            results[i] = torch.sqrt(torch.max(dB / dX))\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.kmeans_matching","title":"<code>kmeans_matching(sm, clusters=range(2, 10), **kwargs)</code>","text":"<p>Evaluate SLISE by measuring how well clusters in Z and B overlap (using kmeans to find the clusters).</p> <p>The overlap is measured by finding the best matching clusters and dividing the size of intersect by the size of the union of each cluster pair. Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>clusters</code> <code>Union[int, Sequence[int]]</code> <p>The number of clusters. Defaults to range(2, 10).</p> <code>range(2, 10)</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>sklearn.KMeans</code>.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean cluster matching.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def kmeans_matching(\n    sm: Slisemap, clusters: Union[int, Sequence[int]] = range(2, 10), **kwargs: Any\n) -&gt; float:\n    \"\"\"Evaluate SLISE by measuring how well clusters in Z and B overlap (using kmeans to find the clusters).\n\n    The overlap is measured by finding the best matching clusters and dividing the size of intersect by the size of the union of each cluster pair.\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        clusters: The number of clusters. Defaults to range(2, 10).\n\n    Keyword Args:\n        **kwargs: Additional arguments to `sklearn.KMeans`.\n\n    Returns:\n        The mean cluster matching.\n    \"\"\"\n    from scipy.optimize import linear_sum_assignment\n\n    Z = sm.get_Z()\n    B = sm.get_B()\n    if np.all(np.var(Z, 0) &lt; 1e-8) or np.all(np.var(B, 0) &lt; 1e-8):\n        return np.nan  # Do not compare singular clusters\n    if not np.all(np.isfinite(Z)) or not np.all(np.isfinite(B)):\n        return np.nan\n    if isinstance(clusters, int):\n        clusters = range(clusters, clusters + 1)\n    results = []\n    for k in clusters:\n        cl_B = KMeans(n_clusters=k, **kwargs).fit(B)\n        cl_Z = KMeans(n_clusters=k, **kwargs).fit(Z)\n        sets_B = [set(np.where(cl_B.labels_ == i)[0]) for i in range(k)]\n        sets_Z = [set(np.where(cl_Z.labels_ == i)[0]) for i in range(k)]\n        mat = np.zeros((k, k))\n        for i, sB in enumerate(sets_B):\n            for j, sZ in enumerate(sets_Z):\n                mat[i, j] = len(sB.intersection(sZ)) / (len(sB.union(sZ)) + 1e-8)\n        # Hungarian algorithm to find the best match between the clusterings\n        rows, cols = linear_sum_assignment(mat, maximize=True)\n        results.append(mat[rows, cols].mean())\n    return nanmean(results)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.cluster_purity","title":"<code>cluster_purity(sm, clusters)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating how many items in the same cluster are neighbours.</p> <p>Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Union[Slisemap, ToTensor]</code> <p>Trained Slisemap solution or embedding matrix.</p> required <code>clusters</code> <code>Union[ndarray, LongTensor]</code> <p>Cluster ids.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean number of items sharing cluster that are neighbours.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def cluster_purity(\n    sm: Union[Slisemap, ToTensor], clusters: Union[np.ndarray, torch.LongTensor]\n) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating how many items in the same cluster are neighbours.\n\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution _or_ embedding matrix.\n        clusters: Cluster ids.\n\n    Returns:\n        The mean number of items sharing cluster that are neighbours.\n    \"\"\"\n    try:\n        Z = sm.get_Z(numpy=False)\n    except AttributeError:\n        Z = to_tensor(sm)\n    if isinstance(clusters, np.ndarray):\n        clusters = torch.as_tensor(clusters, device=Z.device)\n    res = np.zeros(Z.shape[0])\n    D = torch.cdist(Z, Z)\n    for i in range(len(res)):\n        mask = clusters[i] == clusters\n        knn = euclidean_nearest_neighbours(D, i, mask.sum())\n        res[i] = torch.sum(mask[knn]) / knn.shape[0]\n    return nanmean(res)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.kernel_purity","title":"<code>kernel_purity(sm, clusters, epsilon=1.0, losses=False)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating how many neighbours are in the same cluster.</p> <p>Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>clusters</code> <code>Union[ndarray, LongTensor]</code> <p>Cluster ids.</p> required <code>epsilon</code> <code>float</code> <p>Treshold for being a neighbour (<code>softmax(D) &lt; epsilon/n</code>). Defaults to 1.0.</p> <code>1.0</code> <code>losses</code> <code>bool</code> <p>Use losses instead of embedding distances. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The mean number of neighbours that are in the same cluster.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def kernel_purity(\n    sm: Slisemap,\n    clusters: Union[np.ndarray, torch.LongTensor],\n    epsilon: float = 1.0,\n    losses: bool = False,\n) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating how many neighbours are in the same cluster.\n\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        clusters: Cluster ids.\n        epsilon: Treshold for being a neighbour (`softmax(D) &lt; epsilon/n`). Defaults to 1.0.\n        losses: Use losses instead of embedding distances. Defaults to False.\n\n    Returns:\n        The mean number of neighbours that are in the same cluster.\n    \"\"\"\n    if isinstance(clusters, np.ndarray):\n        clusters = torch.tensor(clusters)\n    res = np.zeros(sm.n)\n    D = sm.get_L(numpy=False) if losses else sm.get_D(numpy=False)\n    for i in range(len(res)):\n        mask = clusters[i] == clusters\n        neig = kernel_neighbours(D, i, epsilon)\n        res[i] = torch.sum(mask[neig]) / neig.numel()\n    return nanmean(res)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.recall","title":"<code>recall(sm, epsilon_D=1.0, epsilon_L=1.0)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the recall.</p> <p>We define recall as the intersection between the loss and embedding neighbourhoods divided by the loss neighbourhood.</p> <p>Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>epsilon_D</code> <code>float</code> <p>Treshold for being an embedding neighbour (<code>softmax(D) &lt; epsilon/n</code>). Defaults to 1.0.</p> <code>1.0</code> <code>epsilon_L</code> <code>float</code> <p>Treshold for being a loss neighbour (<code>softmax(L) &lt; epsilon/n</code>). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The mean recall.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def recall(sm: Slisemap, epsilon_D: float = 1.0, epsilon_L: float = 1.0) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the recall.\n\n    We define recall as the intersection between the loss and embedding neighbourhoods divided by the loss neighbourhood.\n\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        epsilon_D: Treshold for being an embedding neighbour (`softmax(D) &lt; epsilon/n`). Defaults to 1.0.\n        epsilon_L: Treshold for being a loss neighbour (`softmax(L) &lt; epsilon/n`). Defaults to 1.0.\n\n    Returns:\n        The mean recall.\n    \"\"\"\n    res = np.zeros(sm.n)\n    D = sm.get_D(numpy=False)\n    L = sm.get_L(numpy=False)\n    for i in range(len(res)):\n        nL = kernel_neighbours(L, i, epsilon_L)\n        if nL.numel() == 0:\n            res[i] = np.nan\n        else:\n            nD = kernel_neighbours(D, i, epsilon_D)\n            inter = np.intersect1d(tonp(nD), tonp(nL), True)\n            res[i] = len(inter) / nL.numel()\n    return nanmean(res)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.precision","title":"<code>precision(sm, epsilon_D=1.0, epsilon_L=1.0)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the recall.</p> <p>We define recall as the intersection between the loss and embedding neighbourhoods divided by the embedding neighbourhood.</p> <p>Larger is better.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>epsilon_D</code> <code>float</code> <p>Treshold for being an embedding neighbour (<code>softmax(D) &lt; epsilon/n</code>). Defaults to 1.0.</p> <code>1.0</code> <code>epsilon_L</code> <code>float</code> <p>Treshold for being a loss neighbour (<code>softmax(L) &lt; epsilon/n</code>). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The mean precision.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def precision(sm: Slisemap, epsilon_D: float = 1.0, epsilon_L: float = 1.0) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the recall.\n\n    We define recall as the intersection between the loss and embedding neighbourhoods divided by the embedding neighbourhood.\n\n    Larger is better.\n\n    Args:\n        sm: Trained Slisemap solution.\n        epsilon_D: Treshold for being an embedding neighbour (`softmax(D) &lt; epsilon/n`). Defaults to 1.0.\n        epsilon_L: Treshold for being a loss neighbour (`softmax(L) &lt; epsilon/n`). Defaults to 1.0.\n\n    Returns:\n        The mean precision.\n    \"\"\"\n    res = np.zeros(sm.n)\n    D = sm.get_D(numpy=False)\n    L = sm.get_L(numpy=False)\n    for i in range(len(res)):\n        nD = kernel_neighbours(D, i, epsilon_D)\n        if nD.numel() == 0:\n            res[i] = np.nan\n        else:\n            nL = kernel_neighbours(L, i, epsilon_L)\n            inter = np.intersect1d(tonp(nD), tonp(nL), True)\n            res[i] = len(inter) / nD.numel()\n    return nanmean(res)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.relevance","title":"<code>relevance(sm, pred_fn, change)</code>","text":"<p>Evaluate a SLISEMAP solution by calculating the relevance.</p> <p>Smaller is better.</p> <p>TODO: This does not (currently) work for multi-class predictions</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>pred_fn</code> <code>Callable</code> <p>Function that gives y:s for new x:s (the \"black box model\").</p> required <code>change</code> <code>float</code> <p>How much should the prediction change?</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean number of mutated variables required to cause a large enough change in the prediction.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def relevance(sm: Slisemap, pred_fn: Callable, change: float) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by calculating the relevance.\n\n    Smaller is better.\n\n    TODO: This does not (currently) work for multi-class predictions\n\n    Args:\n        sm: Trained Slisemap solution.\n        pred_fn: Function that gives y:s for new x:s (the \"black box model\").\n        change: How much should the prediction change?\n\n    Returns:\n        The mean number of mutated variables required to cause a large enough change in the prediction.\n    \"\"\"\n    rel = np.ones(sm.n) * sm.m\n    B = sm.get_B(numpy=False)\n    for i in range(len(rel)):\n        b = B[i, :]\n        x = sm._X[i, :]\n        y = sm._Y[i, 0]\n        xmax = torch.max(sm._X, 0)[0]\n        xmin = torch.min(sm._X, 0)[0]\n        xinc = torch.where(b &gt; 0, xmax, xmin)\n        xdec = torch.where(b &lt; 0, xmax, xmin)\n        babs = torch.abs(b)\n        for i, bs in enumerate(torch.sort(babs)[0]):\n            yinc = pred_fn(torch.where(babs &gt;= bs, xinc, x))\n            ydec = pred_fn(torch.where(babs &gt;= bs, xdec, x))\n            if yinc - y &gt; change or y - ydec &gt; change:\n                rel[i] = i\n                break\n    return nanmean(rel)\n</code></pre>"},{"location":"slisemap.metrics/#slisemap.metrics.accuracy","title":"<code>accuracy(sm, X=None, Y=None, fidelity=True, optimise=False, fit_new=False, **kwargs)</code>","text":"<p>Evaluate a SLISEMAP solution by checking how well the fitted models work on new points.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Trained Slisemap solution.</p> required <code>X</code> <code>Optional[ToTensor]</code> <p>New data matrix (uses the training data if None). Defaults to None.</p> <code>None</code> <code>Y</code> <code>Optional[ToTensor]</code> <p>New target matrix (uses the training data if None). Defaults to None.</p> <code>None</code> <code>fidelity</code> <code>bool</code> <p>Return the mean local loss (fidelity) instead of the mean embedding weighted loss. Defaults to True.</p> <code>True</code> <code>optimise</code> <code>bool</code> <p>If <code>fit_new</code>, optimise the new points. Defaults to False.</p> <code>False</code> <code>fit_new</code> <code>bool</code> <p>Use <code>[Slisemap.fit_new][slisemap.slisemap.Slisemap.fit_new]</code> instead of <code>[Slisemap.predict][slisemap.slisemap.Slisemap.predict]</code> (if <code>fidelity=True</code>). Defaults to True.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to Slisemap.predict.</p> <p>Returns:</p> Type Description <code>float</code> <p>Mean loss for the new points.</p> Deprecated <p>1.6: <code>fit_new</code>, <code>fidelity</code>, <code>optimise</code>.</p> Source code in <code>slisemap/metrics.py</code> <pre><code>def accuracy(\n    sm: Slisemap,\n    X: Optional[ToTensor] = None,\n    Y: Optional[ToTensor] = None,\n    fidelity: bool = True,\n    optimise: bool = False,\n    fit_new: bool = False,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Evaluate a SLISEMAP solution by checking how well the fitted models work on new points.\n\n    Args:\n        sm: Trained Slisemap solution.\n        X: New data matrix (uses the training data if None). Defaults to None.\n        Y: New target matrix (uses the training data if None). Defaults to None.\n        fidelity: Return the mean local loss (fidelity) instead of the mean embedding weighted loss. Defaults to True.\n        optimise: If `fit_new`, optimise the new points. Defaults to False.\n        fit_new: Use `[Slisemap.fit_new][slisemap.slisemap.Slisemap.fit_new]` instead of `[Slisemap.predict][slisemap.slisemap.Slisemap.predict]` (if `fidelity=True`). Defaults to True.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to [Slisemap.predict][slisemap.slisemap.Slisemap.predict].\n\n    Returns:\n        Mean loss for the new points.\n\n    Deprecated:\n        1.6: `fit_new`, `fidelity`, `optimise`.\n    \"\"\"\n    if X is None or Y is None:\n        X = sm.get_X(intercept=False, numpy=False)\n        Y = sm.get_Y(numpy=False)\n    if fit_new:\n        _deprecated(\"accuracy(..., fit_new=True)\")\n    if not fidelity:\n        _deprecated(\"accuracy(..., fidelity=False)\")\n    if optimise:\n        _deprecated(\"accuracy(..., optimise=True)\")\n    if not fidelity:\n        loss = sm.fit_new(X, Y, loss=True, optimise=optimise, numpy=False, **kwargs)[2]\n        return loss.mean().cpu().item()\n    else:\n        X = sm._as_new_X(X)\n        Y = sm._as_new_Y(Y, X.shape[0])\n        if fit_new:\n            B, _ = sm.fit_new(\n                X, Y, loss=False, optimise=optimise, numpy=False, **kwargs\n            )\n            return sm.local_loss(sm.predict(X, B, numpy=False), Y).mean().cpu().item()\n        else:\n            loss = sm.local_loss(sm.predict(X, **kwargs, numpy=False), Y)\n            assert loss.shape == X.shape[:1]\n            return loss.mean().cpu().item()\n</code></pre>"},{"location":"slisemap.plot/","title":"slisemap.metrics","text":""},{"location":"slisemap.plot/#slisemap.plot","title":"<code>slisemap.plot</code>","text":"<p>Utility functions for plotting.</p>"},{"location":"slisemap.plot/#slisemap.plot.legend_inside_facet","title":"<code>legend_inside_facet(grid)</code>","text":"<p>Move the legend to within the facet grid if possible.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>FacetGrid</code> <p>Facet grid</p> required Source code in <code>slisemap/plot.py</code> <pre><code>def legend_inside_facet(grid: sns.FacetGrid) -&gt; None:\n    \"\"\"Move the legend to within the facet grid if possible.\n\n    Args:\n        grid: Facet grid\n    \"\"\"\n    col_wrap = grid._col_wrap\n    facets = grid._n_facets\n    if col_wrap &lt; facets and facets % col_wrap != 0:\n        w = 1 / col_wrap\n        h = 1 / ((facets - 1) // col_wrap + 1)\n        sns.move_legend(\n            grid,\n            \"center\",\n            bbox_to_anchor=(1 - w, h * 0.1, w * 0.9, h * 0.9),\n            frameon=False,\n        )\n        plt.tight_layout()\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_embedding","title":"<code>plot_embedding(Z, dimensions=('SLISEMAP 1', 'SLISEMAP 2'), title='Embedding', jitter=0.0, clusters=None, color=None, color_name='', color_norm=None, **kwargs)</code>","text":"<p>Plot an embedding in a scatterplot.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The embedding.</p> required <code>dimensions</code> <code>Sequence[str]</code> <p>Dimension names. Defaults to (\"SLISEMAP 1\", \"SLISEMAP 2\").</p> <code>('SLISEMAP 1', 'SLISEMAP 2')</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to \"Embedding\".</p> <code>'Embedding'</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Jitter amount. Defaults to 0.0.</p> <code>0.0</code> <code>clusters</code> <code>Optional[Sequence[int]]</code> <p>Cluster labels. Defaults to None.</p> <code>None</code> <code>color</code> <code>Optional[Sequence[float]]</code> <p>Variable for coloring. Defaults to None.</p> <code>None</code> <code>color_name</code> <code>str</code> <p>Variable name. Defaults to \"\".</p> <code>''</code> <code>color_norm</code> <code>Optional[Tuple[float]]</code> <p>Color scale limits. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.scatterplot</code>.</p> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_embedding(\n    Z: np.ndarray,\n    dimensions: Sequence[str] = (\"SLISEMAP 1\", \"SLISEMAP 2\"),\n    title: str = \"Embedding\",\n    jitter: Union[float, np.ndarray] = 0.0,\n    clusters: Optional[Sequence[int]] = None,\n    color: Optional[Sequence[float]] = None,\n    color_name: str = \"\",\n    color_norm: Optional[Tuple[float]] = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plot an embedding in a scatterplot.\n\n    Args:\n        Z: The embedding.\n        dimensions: Dimension names. Defaults to (\"SLISEMAP 1\", \"SLISEMAP 2\").\n        title: Plot title. Defaults to \"Embedding\".\n        jitter: Jitter amount. Defaults to 0.0.\n        clusters: Cluster labels. Defaults to None.\n        color: Variable for coloring. Defaults to None.\n        color_name: Variable name. Defaults to \"\".\n        color_norm: Color scale limits. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.scatterplot`.\n\n    Returns:\n        The plot.\n    \"\"\"\n    Z, dimensions = _prepare_Z(Z, dimensions, jitter, plot_embedding)\n    kwargs.setdefault(\"rasterized\", Z.shape[0] &gt; 2_000)\n    if clusters is None:\n        kwargs.setdefault(\"palette\", \"crest\")\n        if color is not None:\n            ax = sns.scatterplot(\n                x=Z[:, 0],\n                y=Z[:, 1],\n                hue=color,\n                hue_norm=color_norm,\n                legend=False,\n                **kwargs,\n            )\n        else:\n            ax = sns.scatterplot(x=Z[:, 0], y=Z[:, 1], **kwargs)\n    else:\n        kwargs.setdefault(\"palette\", \"bright\")\n        ax = sns.scatterplot(\n            x=Z[:, 0], y=Z[:, 1], hue=clusters, style=clusters, **kwargs\n        )\n        color_name = \"Cluster\"\n    if color_norm is not None:\n        ax.legend(*_create_legend(color_norm, kwargs[\"palette\"], 5), title=color_name)\n    elif color_name is not None and color_name != \"\":\n        ax.legend(title=color_name)\n    ax.set_xlabel(dimensions[0])\n    ax.set_ylabel(dimensions[1])\n    ax.axis(\"equal\")\n    ax.set_title(title)\n    return ax\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_matrix","title":"<code>plot_matrix(B, coefficients, title='Local models', palette='RdBu', xlabel='Data items sorted left to right', items=None, **kwargs)</code>","text":"<p>Plot local models in a heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>ndarray</code> <p>Local model coefficients.</p> required <code>coefficients</code> <code>Sequence[str]</code> <p>Coefficient names.</p> required <code>palette</code> <code>str</code> <p><code>seaborn</code> palette. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"Local models\".</p> <code>'Local models'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis. Defaults to \"Data items sorted left to right\".</p> <code>'Data items sorted left to right'</code> <code>items</code> <code>Optional[Sequence[str]]</code> <p>Ticklabels for the x-axis. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.heatmap</code>.</p> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_matrix(\n    B: np.ndarray,\n    coefficients: Sequence[str],\n    title: str = \"Local models\",\n    palette: str = \"RdBu\",\n    xlabel: str = \"Data items sorted left to right\",\n    items: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plot local models in a heatmap.\n\n    Args:\n        B: Local model coefficients.\n        coefficients: Coefficient names.\n        palette: `seaborn` palette. Defaults to \"RdBu\".\n        title: Title of the plot. Defaults to \"Local models\".\n        xlabel: Label for the x-axis. Defaults to \"Data items sorted left to right\".\n        items: Ticklabels for the x-axis. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.heatmap`.\n\n    Returns:\n        The plot.\n    \"\"\"\n    kwargs.setdefault(\"rasterized\", B.shape[0] * B.shape[1] &gt; 20_000)\n    ax = sns.heatmap(B.T, center=0, cmap=palette, robust=True, **kwargs)\n    ax.set_yticks(np.arange(len(coefficients)) + 0.5)\n    ax.set_yticklabels(coefficients, rotation=0)\n    ax.set_xlabel(xlabel)\n    if items is None:\n        ax.set_xticklabels([])\n    else:\n        ax.set_xticks(np.arange(len(items)) + 0.5)\n        ax.set_xticklabels(items)\n    ax.set_title(title)\n    return ax\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_barmodels","title":"<code>plot_barmodels(B, clusters, centers, coefficients, bars=True, palette='bright', xlabel='Coefficients', title='Cluster mean local model', **kwargs)</code>","text":"<p>Plot local models in a barplot.</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>ndarray</code> <p>Local model coefficients.</p> required <code>clusters</code> <code>ndarray</code> <p>Cluster labels.</p> required <code>centers</code> <code>ndarray</code> <p>Cluster centers.</p> required <code>coefficients</code> <code>Sequence[str]</code> <p>Coefficient names.</p> required <code>bars</code> <code>Union[bool, int, Sequence[str]]</code> <p>Number / list of variables to show (or a bool for all). Defaults to True.</p> <code>True</code> <code>palette</code> <code>str</code> <p><code>seaborn</code> palette. Defaults to \"bright\".</p> <code>'bright'</code> <code>xlabel</code> <code>Optional[str]</code> <p>Label for the x-axis. Defaults to \"Coefficients\".</p> <code>'Coefficients'</code> <code>title</code> <code>Optional[str]</code> <p>Title of the plot. Defaults to \"Cluster mean local model\".</p> <code>'Cluster mean local model'</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.barplot</code>.</p> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_barmodels(\n    B: np.ndarray,\n    clusters: np.ndarray,\n    centers: np.ndarray,\n    coefficients: Sequence[str],\n    bars: Union[bool, int, Sequence[str]] = True,\n    palette: str = \"bright\",\n    xlabel: Optional[str] = \"Coefficients\",\n    title: Optional[str] = \"Cluster mean local model\",\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plot local models in a barplot.\n\n    Args:\n        B: Local model coefficients.\n        clusters: Cluster labels.\n        centers: Cluster centers.\n        coefficients: Coefficient names.\n        bars: Number / list of variables to show (or a bool for all). Defaults to True.\n        palette: `seaborn` palette. Defaults to \"bright\".\n        xlabel: Label for the x-axis. Defaults to \"Coefficients\".\n        title: Title of the plot. Defaults to \"Cluster mean local model\".\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.barplot`.\n\n    Returns:\n        The plot.\n    \"\"\"\n    if isinstance(bars, Sequence):\n        mask = [coefficients.index(var) for var in bars]\n        coefficients = bars\n        B = B[:, mask]\n    elif isinstance(bars, bool):\n        pass\n    elif isinstance(bars, int):\n        influence = np.abs(centers)\n        influence = influence.max(0) + influence.mean(0)\n        mask = np.argsort(-influence)[:bars]\n        coefficients = np.asarray(coefficients)[mask]\n        B = B[:, mask]\n    kwargs.setdefault(\"rasterized\", B.shape[0] * B.shape[1] &gt; 20_000)\n    ax = sns.barplot(\n        y=np.tile(coefficients, B.shape[0]),\n        x=B.ravel(),\n        hue=np.repeat(clusters, B.shape[1]),\n        palette=palette,\n        orient=\"h\",\n        **kwargs,\n    )\n    ax.legend().remove()\n    lim = np.max(np.abs(ax.get_xlim()))\n    ax.set(xlabel=xlabel, ylabel=None, title=title, xlim=(-lim, lim))\n    return ax\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_embedding_facet","title":"<code>plot_embedding_facet(Z, dimensions, data, names, legend_title='Value', jitter=0.0, share_hue=True, equal_aspect=True, **kwargs)</code>","text":"<p>Plot (multiple) embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>Embeddings.</p> required <code>dimensions</code> <code>Sequence[str]</code> <p>Dimension names.</p> required <code>data</code> <code>ndarray</code> <p>Data matrix.</p> required <code>names</code> <code>Sequence[str]</code> <p>Column names.</p> required <code>legend_title</code> <code>str</code> <p>Legend title. Defaults to \"Value\".</p> <code>'Value'</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>jitter. Defaults to 0.0.</p> <code>0.0</code> <code>share_hue</code> <code>bool</code> <p>Share color legend between facets.</p> <code>True</code> <code>equal_aspect</code> <code>bool</code> <p>Set equal scale for the axes. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code>.</p> <p>Returns:</p> Type Description <code>FacetGrid</code> <p>The plot.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_embedding_facet(\n    Z: np.ndarray,\n    dimensions: Sequence[str],\n    data: np.ndarray,\n    names: Sequence[str],\n    legend_title: str = \"Value\",\n    jitter: Union[float, np.ndarray] = 0.0,\n    share_hue: bool = True,\n    equal_aspect: bool = True,\n    **kwargs: Any,\n) -&gt; sns.FacetGrid:\n    \"\"\"Plot (multiple) embeddings.\n\n    Args:\n        Z: Embeddings.\n        dimensions: Dimension names.\n        data: Data matrix.\n        names: Column names.\n        legend_title: Legend title. Defaults to \"Value\".\n        jitter: jitter. Defaults to 0.0.\n        share_hue: Share color legend between facets.\n        equal_aspect: Set equal scale for the axes. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.relplot`.\n\n    Returns:\n        The plot.\n    \"\"\"\n    Z, dimensions = _prepare_Z(Z, dimensions, jitter, plot_embedding_facet)\n    df = dict_concat(\n        {\n            \"var\": n,\n            legend_title: data[:, i],\n            dimensions[0]: Z[:, 0],\n            dimensions[1]: Z[:, 1],\n        }\n        for i, n in enumerate(names)\n    )\n    kwargs.setdefault(\"palette\", \"rocket\")\n    kwargs.setdefault(\"rasterized\", Z.shape[0] &gt; 2_000)\n    if share_hue:\n        kwargs.setdefault(\"kind\", \"scatter\")\n        g = sns.relplot(\n            data=df,\n            x=dimensions[0],\n            y=dimensions[1],\n            hue=legend_title,\n            col=\"var\",\n            **kwargs,\n        )\n    else:\n        fgkws = kwargs.pop(\"facet_kws\", {})\n        fgkws.setdefault(\"height\", 5)\n        for k in (\"height\", \"aspect\", \"col_wrap\"):\n            if k in kwargs:\n                fgkws[k] = kwargs.pop(k)\n        fgkws.setdefault(\"legend_out\", False)\n        g = sns.FacetGrid(data=df, col=\"var\", hue=legend_title, **fgkws)\n        for key, ax in g.axes_dict.items():\n            mask = df[\"var\"] == key\n            df2 = {k: v[mask] for k, v in df.items()}\n            sns.scatterplot(\n                data=df2,\n                hue=legend_title,\n                x=dimensions[0],\n                y=dimensions[1],\n                ax=ax,\n                **kwargs,\n            )\n    if equal_aspect:\n        g.set(aspect=\"equal\")\n    g.set_titles(\"{col_name}\")\n    return g\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_density_facet","title":"<code>plot_density_facet(data, names, clusters=None, **kwargs)</code>","text":"<p>Plot density plots.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Data matrix.</p> required <code>names</code> <code>Sequence[str]</code> <p>Column names.</p> required <code>clusters</code> <code>Optional[ndarray]</code> <p>Cluster labels. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.displot</code>.</p> <p>Returns:</p> Type Description <code>FacetGrid</code> <p>The plot.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_density_facet(\n    data: np.ndarray,\n    names: Sequence[str],\n    clusters: Optional[np.ndarray] = None,\n    **kwargs: Any,\n) -&gt; sns.FacetGrid:\n    \"\"\"Plot density plots.\n\n    Args:\n        data: Data matrix.\n        names: Column names.\n        clusters: Cluster labels. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.displot`.\n\n    Returns:\n        The plot.\n    \"\"\"\n    df = dict_concat(\n        {\"var\": n, \"Value\": data[:, i], \"Cluster\": clusters}\n        for i, n in enumerate(names)\n    )\n    if kwargs.setdefault(\"kind\", \"kde\") == \"kde\":\n        kwargs.setdefault(\"bw_adjust\", 0.75)\n        kwargs.setdefault(\"common_norm\", False)\n    if clusters is not None:\n        kwargs.setdefault(\"palette\", \"bright\")\n    kwargs.setdefault(\"facet_kws\", {\"sharex\": False, \"sharey\": False})\n    g = sns.displot(\n        data=df,\n        x=\"Value\",\n        hue=None if clusters is None else \"Cluster\",\n        col=\"var\",\n        **kwargs,\n    )\n    g.set_titles(\"{col_name}\")\n    g.set_xlabels(\"\")\n    return g\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_prototypes","title":"<code>plot_prototypes(Zp, *axs)</code>","text":"<p>Draw a grid of prototypes.</p> <p>Parameters:</p> Name Type Description Default <code>Zp</code> <code>ndarray</code> <p>Prototype coordinates.</p> required <code>*axs</code> <code>Axes</code> <p>Axes to draw on.</p> <code>()</code> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_prototypes(Zp: np.ndarray, *axs: plt.Axes) -&gt; None:\n    \"\"\"Draw a grid of prototypes.\n\n    Args:\n        Zp: Prototype coordinates.\n        *axs: Axes to draw on.\n    \"\"\"\n    Zp, _ = _prepare_Z(Zp, range(2), 0.0, plot_prototypes)\n    for ax in axs:\n        ax.scatter(Zp[:, 0], Zp[:, 1], edgecolors=\"grey\", facecolors=\"none\", alpha=0.7)\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_solution","title":"<code>plot_solution(Z, B, coefficients, dimensions, loss=None, clusters=None, centers=None, title='', bars=True, jitter=0.0, left_kwargs={}, right_kwargs={}, **kwargs)</code>","text":"<p>Plot a Slisemap solution.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>Embedding matrix.</p> required <code>B</code> <code>ndarray</code> <p>Local model coefficients.</p> required <code>coefficients</code> <code>Sequence[str]</code> <p>Coefficient names.</p> required <code>dimensions</code> <code>Sequence[str]</code> <p>Embedding names.</p> required <code>loss</code> <code>Optional[ndarray]</code> <p>Local loss vector. Defaults to None.</p> <code>None</code> <code>clusters</code> <code>Optional[ndarray]</code> <p>Cluster labels. Defaults to None.</p> <code>None</code> <code>centers</code> <code>Optional[ndarray]</code> <p>Cluster centroids. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to \"\".</p> <code>''</code> <code>bars</code> <code>Union[bool, int, Sequence[str]]</code> <p>Plot coefficients in a barplot instead of a heatmap. Defaults to True.</p> <code>True</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add noise to the embedding. Defaults to 0.0.</p> <code>0.0</code> <code>left_kwargs</code> <code>Dict[str, object]</code> <p>Keyword arguments to the left (embedding) plot. Defaults to {}.</p> <code>{}</code> <code>right_kwargs</code> <code>Dict[str, object]</code> <p>Keyword arguments to the right (matrix/bar) plot. Defaults to {}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>matplotlib.pyplot.subplots</code>.</p> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_solution(\n    Z: np.ndarray,\n    B: np.ndarray,\n    coefficients: Sequence[str],\n    dimensions: Sequence[str],\n    loss: Optional[np.ndarray] = None,\n    clusters: Optional[np.ndarray] = None,\n    centers: Optional[np.ndarray] = None,\n    title: str = \"\",\n    bars: Union[bool, int, Sequence[str]] = True,\n    jitter: Union[float, np.ndarray] = 0.0,\n    left_kwargs: Dict[str, object] = {},\n    right_kwargs: Dict[str, object] = {},\n    **kwargs: Any,\n) -&gt; figure.Figure:\n    \"\"\"Plot a Slisemap solution.\n\n    Args:\n        Z: Embedding matrix.\n        B: Local model coefficients.\n        coefficients: Coefficient names.\n        dimensions: Embedding names.\n        loss: Local loss vector. Defaults to None.\n        clusters: Cluster labels. Defaults to None.\n        centers: Cluster centroids. Defaults to None.\n        title: Plot title. Defaults to \"\".\n        bars: Plot coefficients in a barplot instead of a heatmap. Defaults to True.\n        jitter: Add noise to the embedding. Defaults to 0.0.\n        left_kwargs: Keyword arguments to the left (embedding) plot. Defaults to {}.\n        right_kwargs: Keyword arguments to the right (matrix/bar) plot. Defaults to {}.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `matplotlib.pyplot.subplots`.\n\n    Returns:\n        Figure\n    \"\"\"\n    kwargs.setdefault(\"figsize\", (12, 6))\n    fig, (ax1, ax2) = plt.subplots(1, 2, **kwargs)\n    if clusters is None:\n        plot_embedding(\n            Z,\n            dimensions,\n            jitter=jitter,\n            color=loss.ravel(),\n            color_name=None if loss is None else \"Local loss\",\n            color_norm=None if loss is None else tuple(np.quantile(loss, (0.0, 0.95))),\n            ax=ax1,\n            **left_kwargs,\n        )\n        B = B[np.argsort(Z[:, 0])]\n        plot_matrix(B, coefficients, ax=ax2, **right_kwargs)\n    else:\n        plot_embedding(\n            Z, dimensions, jitter=jitter, clusters=clusters, ax=ax1, **left_kwargs\n        )\n        if bars:\n            plot_barmodels(\n                B, clusters, centers, coefficients, bars=bars, ax=ax2, **right_kwargs\n            )\n        else:\n            plot_matrix(\n                centers,\n                coefficients,\n                title=\"Cluster mean local model\",\n                xlabel=\"Cluster\",\n                items=np.unique(clusters),\n                ax=ax2,\n                **right_kwargs,\n            )\n    sns.despine(fig)\n    plt.suptitle(title)\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_position","title":"<code>plot_position(Z, L, Zs, dimensions, title='', jitter=0.0, legend_inside=True, marker_size=1.0, **kwargs)</code>","text":"<p>Plot local losses for alternative locations for the selected item(s).</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>Embedding matrix.</p> required <code>L</code> <code>ndarray</code> <p>Loss matrix.</p> required <code>Zs</code> <code>Optional[ndarray]</code> <p>Selected coordinates.</p> required <code>dimensions</code> <code>Sequence[str]</code> <p>Embedding names.</p> required <code>title</code> <code>str</code> <p>Plot title. Defaults to \"\".</p> <code>''</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add random noise to the embedding. Defaults to 0.0.</p> <code>0.0</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside the grid (if there is an empty cell). Defaults to True.</p> <code>True</code> <code>marker_size</code> <code>float</code> <p>Multiply the point size with this value. Defaults to 1.0.</p> <code>1.0</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code>.</p> <p>Returns:</p> Type Description <code>FacetGrid</code> <p>FacetGrid</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_position(\n    Z: np.ndarray,\n    L: np.ndarray,\n    Zs: Optional[np.ndarray],\n    dimensions: Sequence[str],\n    title: str = \"\",\n    jitter: Union[float, np.ndarray] = 0.0,\n    legend_inside: bool = True,\n    marker_size: float = 1.0,\n    **kwargs: Any,\n) -&gt; sns.FacetGrid:\n    \"\"\"Plot local losses for alternative locations for the selected item(s).\n\n    Args:\n        Z: Embedding matrix.\n        L: Loss matrix.\n        Zs: Selected coordinates.\n        dimensions: Embedding names.\n        title: Plot title. Defaults to \"\".\n        jitter: Add random noise to the embedding. Defaults to 0.0.\n        legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n        marker_size: Multiply the point size with this value. Defaults to 1.0.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.relplot`.\n\n    Returns:\n        FacetGrid\n    \"\"\"\n    kwargs.setdefault(\"palette\", \"crest\")\n    kwargs.setdefault(\"col_wrap\", min(4, L.shape[1]))\n    if marker_size != 1.0:\n        kwargs.setdefault(\"s\", plt.rcParams[\"lines.markersize\"] * marker_size)\n    hue_norm = tuple(np.quantile(L, (0.0, 0.95)))\n    g = plot_embedding_facet(\n        Z,\n        dimensions,\n        L,\n        range(L.shape[1]),\n        legend_title=\"Local loss\",\n        hue_norm=hue_norm,\n        jitter=jitter,\n        legend=False,\n        **kwargs,\n    )\n    g.set_titles(\"\")\n    plt.suptitle(title)\n    # Legend\n    col_wrap = kwargs[\"col_wrap\"]\n    facets = g._n_facets\n    legend = {s: h for h, s in zip(*_create_legend(hue_norm, kwargs[\"palette\"], 6))}\n    inside = legend_inside and col_wrap &lt; facets and facets % col_wrap != 0\n    w = 1 / col_wrap\n    h = 1 / ((facets - 1) // col_wrap + 1)\n    if Zs is not None:\n        size = plt.rcParams[\"lines.markersize\"] * 18.0\n        for i, ax in enumerate(g.axes.ravel()):\n            ax.scatter(Zs[i, 0], Zs[i, 1], size, \"#fd8431\", \"X\")\n        g.add_legend(\n            legend,\n            \"Local loss\",\n            loc=\"lower center\" if inside else \"upper right\",\n            bbox_to_anchor=(1 - w, h * 0.35, w * 0.9, h * 0.6) if inside else None,\n        )\n        marker = Line2D(\n            [], [], linestyle=\"None\", color=\"#fd8431\", marker=\"X\", markersize=5\n        )\n        g.add_legend(\n            {\"\": marker},\n            \"Selected\",\n            loc=\"upper center\" if inside else \"lower right\",\n            bbox_to_anchor=(1 - w, h * 0.05, w * 0.9, h * 0.3) if inside else None,\n        )\n    else:\n        g.add_legend(\n            legend,\n            \"Local loss\",\n            loc=\"center\" if inside else \"center right\",\n            bbox_to_anchor=(1 - w, 0.05, w * 0.9, h * 0.9) if inside else None,\n        )\n    if inside:\n        plt.tight_layout()\n    else:\n        g.tight_layout()\n    return g\n</code></pre>"},{"location":"slisemap.plot/#slisemap.plot.plot_dist","title":"<code>plot_dist(X, Y, Z, loss, variables, targets, dimensions, title='', clusters=None, scatter=False, jitter=0.0, legend_inside=True, **kwargs)</code>","text":"<p>Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data matrix.</p> required <code>Y</code> <code>ndarray</code> <p>Target matrix.</p> required <code>Z</code> <code>ndarray</code> <p>Embedding matrix.</p> required <code>loss</code> <code>ndarray</code> <p>Local loss vector.</p> required <code>variables</code> <code>Sequence[str]</code> <p>Variable names.</p> required <code>targets</code> <code>Sequence[str]</code> <p>Target names.</p> required <code>dimensions</code> <code>Sequence[str]</code> <p>Embedding names.</p> required <code>title</code> <code>str</code> <p>Plot title. Defaults to \"\".</p> <code>''</code> <code>clusters</code> <code>Optional[ndarray]</code> <p>Cluster labels. Defaults to None.</p> <code>None</code> <code>scatter</code> <code>bool</code> <p>Plot a scatterplot instead of a density plot. Defaults to False.</p> <code>False</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add noise to the embedding. Defaults to 0.0.</p> <code>0.0</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside a facet, if possible.. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code> or <code>seaborn.scatterplot</code>.</p> <p>Returns:</p> Type Description <code>FacetGrid</code> <p>FacetGrid.</p> Source code in <code>slisemap/plot.py</code> <pre><code>def plot_dist(\n    X: np.ndarray,\n    Y: np.ndarray,\n    Z: np.ndarray,\n    loss: np.ndarray,\n    variables: Sequence[str],\n    targets: Sequence[str],\n    dimensions: Sequence[str],\n    title: str = \"\",\n    clusters: Optional[np.ndarray] = None,\n    scatter: bool = False,\n    jitter: Union[float, np.ndarray] = 0.0,\n    legend_inside: bool = True,\n    **kwargs: Any,\n) -&gt; sns.FacetGrid:\n    \"\"\"Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        Z: Embedding matrix.\n        loss: Local loss vector.\n        variables: Variable names.\n        targets: Target names.\n        dimensions: Embedding names.\n        title: Plot title. Defaults to \"\".\n        clusters: Cluster labels. Defaults to None.\n        scatter: Plot a scatterplot instead of a density plot. Defaults to False.\n        jitter: Add noise to the embedding. Defaults to 0.0.\n        legend_inside: Move the legend inside a facet, if possible.. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.relplot` or `seaborn.scatterplot`.\n\n    Returns:\n        FacetGrid.\n    \"\"\"\n    data = np.concatenate((X, Y, loss.ravel()[:, None]), 1)\n    labels = variables + targets + [\"Local loss\"]\n    kwargs.setdefault(\"col_wrap\", 4)\n    if scatter:\n        g = plot_embedding_facet(\n            Z, dimensions, data, labels, jitter=jitter, share_hue=False, **kwargs\n        )\n    else:\n        g = plot_density_facet(data, labels, clusters=clusters, **kwargs)\n    plt.suptitle(title)\n    if scatter or clusters is None or not legend_inside:\n        g.tight_layout()\n    else:\n        legend_inside_facet(g)\n    return g\n</code></pre>"},{"location":"slisemap.slipmap/","title":"slisemap.slipmap","text":""},{"location":"slisemap.slipmap/#slisemap.slipmap","title":"<code>slisemap.slipmap</code>","text":"<p>Prototype version of Slisemap.</p> <p>Instead of giving every data item its own local model we have a fixed grid of prototypes, where each prototype has a local model. This improves the scaling from quadratic to linear.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap","title":"<code>Slipmap</code>","text":"<p>Slipmap: Faster and more robust <code>[Slisemap][slisemap.slisemap.Slisemap]</code>.</p> <p>This class contains the data and the parameters needed for finding a Slipmap solution. It also contains the solution (remember to optimise() first) in the form of an embedding matrix, see get_Z(), and a matrix of coefficients for the local model, see get_Bp(). Other methods of note are the various plotting methods, the save() method, and the predict() method.</p> <p>The use of some regularisation is highly recommended. Slipmap comes with built-in lasso/L1 and ridge/L2 regularisation (if these are used it is also a good idea to normalise the data in advance).</p> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>The number of data items (<code>X.shape[0]</code>).</p> <code>m</code> <code>int</code> <p>The number of variables (<code>X.shape[1]</code>).</p> <code>o</code> <code>int</code> <p>The number of targets (<code>Y.shape[1]</code>).</p> <code>d</code> <code>int</code> <p>The number of embedding dimensions (<code>Z.shape[1]</code>).</p> <code>p</code> <code>int</code> <p>The number of prototypes (<code>Zp.shape[1]</code>).</p> <code>q</code> <code>int</code> <p>The number of coefficients (<code>Bp.shape[1]</code>).</p> <code>intercept</code> <code>bool</code> <p>Has an intercept term been added to <code>X</code>.</p> <code>radius</code> <code>float</code> <p>The radius of the embedding.</p> <code>lasso</code> <code>float</code> <p>Lasso regularisation coefficient.</p> <code>ridge</code> <code>float</code> <p>Ridge regularisation coefficient.</p> <code>local_model</code> <code>CallableLike[predict]</code> <p>Local model prediction function (see slisemap.local_models).</p> <code>local_loss</code> <code>CallableLike[loss]</code> <p>Local model loss function (see slisemap.local_models).</p> <code>regularisation</code> <code>CallableLike[regularisation]</code> <p>Additional regularisation function.</p> <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Distance function.</p> <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel function.</p> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function for increased performance (see <code>torch.jit.trace</code> for caveats).</p> <code>metadata</code> <p>A dictionary for storing variable names and other metadata (see slisemap.utils.Metadata).</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>class Slipmap:\n    \"\"\"__Slipmap__: Faster and more robust `[Slisemap][slisemap.slisemap.Slisemap]`.\n\n    This class contains the data and the parameters needed for finding a Slipmap solution.\n    It also contains the solution (remember to [optimise()][slisemap.slipmap.Slipmap.optimize] first) in the form of an embedding matrix, see [get_Z()][slisemap.slipmap.Slipmap.get_Z], and a matrix of coefficients for the local model, see [get_Bp()][slisemap.slipmap.Slipmap.get_Bp].\n    Other methods of note are the various plotting methods, the [save()][slisemap.slipmap.Slipmap.save] method, and the [predict()][slisemap.slipmap.Slipmap.predict] method.\n\n    The use of some regularisation is highly recommended. Slipmap comes with built-in lasso/L1 and ridge/L2 regularisation (if these are used it is also a good idea to normalise the data in advance).\n\n    Attributes:\n        n: The number of data items (`X.shape[0]`).\n        m: The number of variables (`X.shape[1]`).\n        o: The number of targets (`Y.shape[1]`).\n        d: The number of embedding dimensions (`Z.shape[1]`).\n        p: The number of prototypes (`Zp.shape[1]`).\n        q: The number of coefficients (`Bp.shape[1]`).\n        intercept: Has an intercept term been added to `X`.\n        radius: The radius of the embedding.\n        lasso: Lasso regularisation coefficient.\n        ridge: Ridge regularisation coefficient.\n        local_model: Local model prediction function (see [slisemap.local_models][]).\n        local_loss: Local model loss function (see [slisemap.local_models][]).\n        regularisation: Additional regularisation function.\n        distance: Distance function.\n        kernel: Kernel function.\n        jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats).\n        metadata: A dictionary for storing variable names and other metadata (see [slisemap.utils.Metadata][]).\n    \"\"\"\n\n    # Make Python faster and safer by not creating a Slipmap.__dict__\n    __slots__ = (\n        \"_X\",\n        \"_Y\",\n        \"_Z\",\n        \"_Bp\",\n        \"_Zp\",\n        \"_radius\",\n        \"_lasso\",\n        \"_ridge\",\n        \"_intercept\",\n        \"_local_model\",\n        \"_local_loss\",\n        \"_regularisation\",\n        \"_loss\",\n        \"_distance\",\n        \"_kernel\",\n        \"_jit\",\n        \"metadata\",\n    )\n\n    def __init__(\n        self,\n        X: ToTensor,\n        y: ToTensor,\n        radius: float = 2.0,\n        d: int = 2,\n        lasso: Optional[float] = None,\n        ridge: Optional[float] = None,\n        intercept: bool = True,\n        local_model: Union[\n            LocalModelCollection, CallableLike[ALocalModel.predict]\n        ] = LinearRegression,\n        local_loss: Optional[CallableLike[ALocalModel.loss]] = None,\n        coefficients: Union[None, int, CallableLike[ALocalModel.coefficients]] = None,\n        regularisation: Union[None, CallableLike[ALocalModel.regularisation]] = None,\n        distance: CallableLike[squared_distance] = squared_distance,\n        kernel: CallableLike[softmax_column_kernel] = softmax_column_kernel,\n        Z0: Optional[ToTensor] = None,\n        Bp0: Optional[ToTensor] = None,\n        Zp0: Optional[ToTensor] = None,\n        prototypes: Union[int, float] = 1.0,\n        jit: bool = True,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n    ) -&gt; None:\n        \"\"\"Create a Slipmap object.\n\n        Args:\n            X: Data matrix.\n            y: Target vector or matrix.\n            radius: The radius of the embedding Z. Defaults to 2.0.\n            d: The number of embedding dimensions. Defaults to 2.\n            lasso: Lasso regularisation coefficient. Defaults to 0.0.\n            ridge: Ridge regularisation coefficient. Defaults to 0.0.\n            intercept: Should an intercept term be added to `X`. Defaults to True.\n            local_model: Local model prediction function (see [slisemap.local_models.identify_local_model][]). Defaults to [LinearRegression][slisemap.local_models.LinearRegression].\n            local_loss: Local model loss function (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            coefficients: The number of local model coefficients (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            regularisation: Additional regularisation method (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            distance: Distance function. Defaults to [squared_distance][slisemap.utils.squared_distance].\n            kernel: Kernel function. Defaults to [softmax_column_kernel][slisemap.utils.softmax_column_kernel].\n            Z0: Initial embedding for the data. Defaults to PCA.\n            Bp0: Initial coefficients for the local models. Defaults to None.\n            Zp0: Initial embedding for the prototypes. Defaults to `[make_grid][slisemap.utils.make_grid](prototypes)`.\n            prototypes: Number of prototypes (if &gt; 6) or prototype density (if &lt; 6.0). Defaults to 1.0.\n            jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats). Defaults to True.\n            dtype: Floating type. Defaults to `torch.float32`.\n            device: Torch device. Defaults to None.\n        \"\"\"\n        for s in Slipmap.__slots__:\n            # Initialise all attributes (to avoid attribute errors)\n            setattr(self, s, None)\n        if lasso is None and ridge is None:\n            _warn(\n                \"Consider using regularisation!\\n\"\n                + \"\\tRegularisation is important for handling small neighbourhoods, and also makes the local models more local.\"\n                + \" Lasso (l1) and ridge (l2) regularisation is built-in, via the parameters ``lasso`` and ``ridge``.\"\n                + \" Set ``lasso=0`` to disable this warning (if no regularisation is really desired).\",\n                Slipmap,\n            )\n        local_model, local_loss, coefficients, regularisation = identify_local_model(\n            local_model, local_loss, coefficients, regularisation\n        )\n        self.lasso = 0.0 if lasso is None else lasso\n        self.ridge = 0.0 if ridge is None else ridge\n        self.kernel = kernel\n        self.distance = distance\n        self.local_model = local_model\n        self.local_loss = local_loss\n        self.regularisation = regularisation\n        self._radius = radius\n        self._intercept = intercept\n        self._jit = jit\n        self.metadata = Metadata(self)\n\n        if device is None and isinstance(X, torch.Tensor):\n            device = X.device\n        tensorargs = {\"device\": device, \"dtype\": dtype}\n\n        if Zp0 is None:\n            if prototypes &lt; 6.0:\n                # Interpret prototypes as a density (prototypes per unit square)\n                prototypes = radius**2 * 2 * np.pi * prototypes\n            self._Zp = torch.as_tensor(make_grid(prototypes, d=d), **tensorargs)\n        else:\n            self._Zp = to_tensor(Zp0, **tensorargs)[0]\n        _assert_shape(self._Zp, (self._Zp.shape[0], d), \"Zp0\", Slipmap)\n\n        self._X, X_rows, X_columns = to_tensor(X, **tensorargs)\n        if intercept:\n            self._X = torch.cat((self._X, torch.ones_like(self._X[:, :1])), 1)\n        n, m = self._X.shape\n        self.metadata.set_variables(X_columns, intercept)\n\n        self._Y, Y_rows, Y_columns = to_tensor(y, **tensorargs)\n        self.metadata.set_targets(Y_columns)\n        if len(self._Y.shape) == 1:\n            self._Y = self._Y[:, None]\n        _assert_shape(self._Y, (n, self._Y.shape[1]), \"Y\", Slipmap)\n\n        if Z0 is None:\n            self._Z = self._X @ PCA_rotation(self._X, d)\n            if self._Z.shape[1] &lt; d:\n                _warn(\n                    \"The number of embedding dimensions is larger than the number of data dimensions\",\n                    Slisemap,\n                )\n                Z0fill = torch.zeros(size=[n, d - self._Z.shape[1]], **tensorargs)\n                self._Z = torch.cat((self._Z, Z0fill), 1)\n            Z_rows = None\n        else:\n            self._Z, Z_rows, Z_columns = to_tensor(Z0, **tensorargs)\n            self.metadata.set_dimensions(Z_columns)\n\n            _assert_shape(self._Z, (n, d), \"Z0\", Slipmap)\n        self._normalise(True)\n\n        if callable(coefficients):\n            coefficients = coefficients(self._X, self._Y)\n        if Bp0 is None:\n            Bp0 = global_model(\n                X=self._X,\n                Y=self._Y,\n                local_model=self.local_model,\n                local_loss=self.local_loss,\n                coefficients=coefficients,\n                lasso=self.lasso,\n                ridge=self.ridge,\n            )\n            if not torch.all(torch.isfinite(Bp0)):\n                _warn(\n                    \"Optimising a global model as initialisation resulted in non-finite values. Consider using stronger regularisation (increase ``lasso`` or ``ridge``).\",\n                    Slipmap,\n                )\n                Bp0 = torch.zeros_like(Bp0)\n            self._Bp = Bp0.expand((self.p, coefficients)).clone()\n            B_rows = None\n        else:\n            self._Bp, B_rows, B_columns = to_tensor(Bp0, **tensorargs)\n            if self._Bp.shape[0] == 1:\n                self._Bp = self._Bp.expand((self.p, coefficients)).clone()\n            _assert_shape(self._Bp, (self.p, coefficients), \"Bp0\", Slipmap)\n            self.metadata.set_coefficients(B_columns)\n        self.metadata.set_rows(X_rows, Y_rows, B_rows, Z_rows)\n        if (\n            device is None\n            and self.n * self.m * self.p * self.o &gt; 500_000\n            and torch.cuda.is_available()\n        ):\n            self.cuda()\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"The number of data items.\"\"\"\n        return self._X.shape[0]\n\n    @property\n    def m(self) -&gt; int:\n        \"\"\"The number of variables (including potential intercept).\"\"\"\n        return self._X.shape[1]\n\n    @property\n    def o(self) -&gt; int:\n        \"\"\"The number of target variables (i.e. the number of classes).\"\"\"\n        return self._Y.shape[-1]\n\n    @property\n    def d(self) -&gt; int:\n        \"\"\"The number of embedding dimensions.\"\"\"\n        return self._Z.shape[1]\n\n    @property\n    def p(self) -&gt; int:\n        \"\"\"The number of prototypes.\"\"\"\n        return self._Zp.shape[0]\n\n    @property\n    def q(self) -&gt; int:\n        \"\"\"The number of local model coefficients.\"\"\"\n        return self._Bp.shape[1]\n\n    @property\n    def intercept(self) -&gt; bool:\n        \"\"\"Is an intercept column added to the data?.\"\"\"\n        return self._intercept\n\n    @property\n    def radius(self) -&gt; float:\n        \"\"\"The radius of the embedding.\"\"\"\n        return self._radius\n\n    @radius.setter\n    def radius(self, value: float) -&gt; None:\n        if self._radius != value:\n            _assert(value &gt;= 0, \"radius must not be negative\", Slipmap.radius)\n            self._radius = value\n            self._normalise(True)\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def lasso(self) -&gt; float:\n        \"\"\"Lasso regularisation strength.\"\"\"\n        return self._lasso\n\n    @lasso.setter\n    def lasso(self, value: float) -&gt; None:\n        if self._lasso != value:\n            _assert(value &gt;= 0, \"lasso must not be negative\", Slisemap.lasso)\n            self._lasso = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def ridge(self) -&gt; float:\n        \"\"\"Ridge regularisation strength.\"\"\"\n        return self._ridge\n\n    @ridge.setter\n    def ridge(self, value: float) -&gt; None:\n        if self._ridge != value:\n            _assert(value &gt;= 0, \"ridge must not be negative\", Slisemap.ridge)\n            self._ridge = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def local_model(self) -&gt; CallableLike[ALocalModel.predict]:\n        \"\"\"Local model prediction function. Takes in X[n, m] and B[n, q], and returns Ytilde[n, n, o].\"\"\"\n        return self._local_model\n\n    @local_model.setter\n    def local_model(self, value: CallableLike[ALocalModel.predict]) -&gt; None:\n        if self._local_model != value:\n            _assert(\n                callable(value), \"local_model must be callable\", Slisemap.local_model\n            )\n            self._local_model = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def local_loss(self) -&gt; CallableLike[ALocalModel.loss]:\n        \"\"\"Local model loss function. Takes in Ytilde[n, n, o] and Y[n, o] and returns L[n, n].\"\"\"\n        return self._local_loss\n\n    @local_loss.setter\n    def local_loss(self, value: CallableLike[ALocalModel.loss]) -&gt; None:\n        if self._local_loss != value:\n            _assert(callable(value), \"local_loss must be callable\", Slisemap.local_loss)\n            self._local_loss = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def regularisation(self) -&gt; CallableLike[ALocalModel.regularisation]:\n        \"\"\"Regularisation function. Takes in X, Y, Bp, Z, and Ytilde and returns an additional loss scalar.\"\"\"\n        return self._regularisation\n\n    @regularisation.setter\n    def regularisation(self, value: CallableLike[ALocalModel.regularisation]) -&gt; None:\n        if self._regularisation != value:\n            _assert(\n                callable(value),\n                \"regularisation function must be callable\",\n                Slisemap.regularisation,\n            )\n            self._regularisation = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def distance(self) -&gt; Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:\n        \"\"\"Distance function. Takes in Z[n1, d] and Z[n2, d], and returns D[n1, n2].\"\"\"\n        return self._distance\n\n    @distance.setter\n    def distance(\n        self, value: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n    ) -&gt; None:\n        if self._distance != value:\n            _assert(callable(value), \"distance must be callable\", Slisemap.distance)\n            self._distance = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def kernel(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        \"\"\"Kernel function. Takes in D[n, n] and returns W[n, n].\"\"\"\n        return self._kernel\n\n    @kernel.setter\n    def kernel(self, value: Callable[[torch.Tensor], torch.Tensor]) -&gt; None:\n        if self._kernel != value:\n            _assert(callable(value), \"kernel must be callable\", Slisemap.kernel)\n            self._kernel = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def jit(self) -&gt; bool:\n        \"\"\"Just-In-Time compile the loss function?\"\"\"  # noqa: D400\n        return self._jit\n\n    @jit.setter\n    def jit(self, value: bool) -&gt; None:\n        if self._jit != value:\n            self._jit = value\n            self._loss = None  # invalidate cached loss function\n\n    def get_Z(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the Z matrix (the embedding for all data items).\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The Z matrix `[n, d]`.\n        \"\"\"\n        self._normalise()\n        return tonp(self._Z) if numpy else self._Z\n\n    def get_B(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the B matrix (the coefficients of the closest local model for all data items).\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The B matrix `[n, q]`.\n        \"\"\"\n        B = self._Bp[self.get_closest(numpy=False)]\n        return tonp(B) if numpy else B\n\n    def get_Zp(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the Zp matrix (the embedding for the prototypes).\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The Zp matrix `[p, d]`.\n        \"\"\"\n        return tonp(self._Zp) if numpy else self._Zp\n\n    def get_Bp(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the Bp matrix (the local model coefficients for the prototypes).\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The Bp matrix `[p, q]`.\n        \"\"\"\n        return tonp(self._Bp) if numpy else self._Bp\n\n    def get_X(\n        self, intercept: bool = True, numpy: bool = True\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the data matrix.\n\n        Args:\n            intercept: Include the intercept column (if ``self.intercept == True``). Defaults to True.\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n        Returns:\n            The X matrix `[n, m]`.\n        \"\"\"\n        X = self._X if intercept or not self._intercept else self._X[:, :-1]\n        return tonp(X) if numpy else X\n\n    def get_Y(\n        self, ravel: bool = False, numpy: bool = True\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the target matrix.\n\n        Args:\n            ravel: Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n        Returns:\n            The Y matrix `[n, o]`.\n        \"\"\"\n        Y = self._Y.ravel() if ravel else self._Y\n        return tonp(Y) if numpy else Y\n\n    def get_D(\n        self,\n        proto_rows: bool = True,\n        proto_cols: bool = False,\n        Z: Optional[torch.Tensor] = None,\n        numpy: bool = True,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the embedding distance matrix.\n\n        Args:\n            proto_rows: Calculate the distances with the prototype embeddings on the rows. Defaults to True.\n            proto_cols: Calculate the distances with the prototype embeddings on the columns. Defaults to False.\n            Z: Optional replacement for the training Z. Defaults to None.\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The D matrix `[n or p, n or p]`.\n        \"\"\"\n        if proto_rows and proto_cols:\n            D = self._distance(self._Zp, self._Zp)\n        else:\n            Z = self.get_Z(numpy=False) if Z is None else Z\n            if proto_rows:\n                D = self._distance(self._Zp, Z)\n            elif proto_cols:\n                D = self._distance(Z, self._Zp)\n            else:\n                D = self._distance(Z, Z)\n        return tonp(D) if numpy else D\n\n    def get_W(\n        self,\n        proto_rows: bool = True,\n        proto_cols: bool = False,\n        Z: Optional[torch.Tensor] = None,\n        numpy: bool = True,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the weight matrix.\n\n        Args:\n            proto_rows: Calculate the weights with the prototype embeddings on the rows. Defaults to True.\n            proto_cols: Calculate the weights with the prototype embeddings on the columns. Defaults to False.\n            Z: Optional replacement for the training Z. Defaults to None.\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n        Returns:\n            The W matrix `[n or p, n or p]`.\n        \"\"\"\n        D = self.get_D(numpy=False, proto_rows=proto_rows, proto_cols=proto_cols, Z=Z)\n        W = self.kernel(D)\n        return tonp(W) if numpy else W\n\n    def get_L(\n        self,\n        X: Optional[ToTensor] = None,\n        Y: Optional[ToTensor] = None,\n        numpy: bool = True,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the loss matrix.\n\n        Args:\n            X: Optional replacement for the training X. Defaults to None.\n            Y: Optional replacement for the training Y. Defaults to None.\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n            The L matrix `[p, n]`.\n        \"\"\"\n        X = self._as_new_X(X)\n        Y = self._as_new_Y(Y, X.shape[0])\n        L = self.local_loss(self.local_model(X, self._Bp), Y)\n        return tonp(L) if numpy else L\n\n    def get_closest(\n        self, Z: Optional[torch.Tensor] = None, numpy: bool = True\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the closest prototype for each data item.\n\n        Args:\n            Z: Optional replacement for the training Z. Defaults to None.\n            numpy: Return the vector as a numpy (True) or pytorch (False) array. Defaults to True.\n\n        Returns:\n            Index vector `[n]`.\n        \"\"\"\n        D = self.get_D(numpy=False, Z=Z, proto_rows=True, proto_cols=False)\n        index = torch.argmin(D, 0)\n        return tonp(index) if numpy else index\n\n    def _as_new_X(self, X: Optional[ToTensor] = None) -&gt; torch.Tensor:\n        if X is None:\n            return self._X\n        X = torch.atleast_2d(to_tensor(X, **self.tensorargs)[0])\n        if self._intercept and X.shape[1] == self.m - 1:\n            X = torch.cat((X, torch.ones_like(X[:, :1])), 1)\n        _assert_shape(X, (X.shape[0], self.m), \"X\", Slipmap._as_new_X)\n        return X\n\n    def _as_new_Y(self, Y: Optional[ToTensor] = None, n: int = -1) -&gt; torch.Tensor:\n        if Y is None:\n            return self._Y\n        Y = to_tensor(Y, **self.tensorargs)[0]\n        if len(Y.shape) &lt; 2:\n            Y = torch.reshape(Y, (n, self.o))\n        _assert_shape(Y, (n if n &gt; 0 else Y.shape[0], self.o), \"Y\", Slipmap._as_new_Y)\n        return Y\n\n    @property\n    def tensorargs(self) -&gt; Dict[str, Any]:\n        \"\"\"When creating a new `torch.Tensor` add these keyword arguments to match the `dtype` and `device` of this Slisemap object.\"\"\"\n        return {\"device\": self._X.device, \"dtype\": self._X.dtype}\n\n    def cuda(self, **kwargs: Any) -&gt; None:\n        \"\"\"Move the tensors to CUDA memory (and run the calculations there).\n\n        Note that this resets the random state.\n\n        Keyword Args:\n            **kwargs: Optional arguments to ``torch.Tensor.cuda``\n        \"\"\"\n        X = self._X.cuda(**kwargs)\n        self._X = X\n        self._Y = self._Y.cuda(**kwargs)\n        self._Z = self._Z.cuda(**kwargs)\n        self._Bp = self._Bp.cuda(**kwargs)\n        self._Zp = self._Zp.cuda(**kwargs)\n        self._loss = None  # invalidate cached loss function\n\n    def cpu(self, **kwargs: Any) -&gt; None:\n        \"\"\"Move the tensors to CPU memory (and run the calculations there).\n\n        Note that this resets the random state.\n\n        Keyword Args:\n            **kwargs: Optional arguments to ``torch.Tensor.cpu``\n        \"\"\"\n        X = self._X.cpu(**kwargs)\n        self._X = X\n        self._Y = self._Y.cpu(**kwargs)\n        self._Z = self._Z.cpu(**kwargs)\n        self._Bp = self._Bp.cpu(**kwargs)\n        self._Zp = self._Zp.cpu(**kwargs)\n        self._loss = None  # invalidate cached loss function\n\n    def copy(self) -&gt; \"Slipmap\":\n        \"\"\"Make a copy of this Slipmap that references as much of the same torch-data as possible.\n\n        Returns\n            An almost shallow copy of this Slipmap object.\n        \"\"\"\n        other = copy(self)  # Shallow copy!\n        # Deep copy these:\n        other._Z = other._Z.clone().detach()\n        other._Bp = other._Bp.clone().detach()\n        return other\n\n    @classmethod\n    def convert(\n        cls, sm: Slisemap, keep_kernel: bool = False, **kwargs: Any\n    ) -&gt; \"Slipmap\":\n        \"\"\"Convert a Slisemap object into a Slipmap object.\n\n        Args:\n            sm: Slisemap object.\n            keep_kernel: Use the kernel and distance functions from the Slisemap object. Defaults to False.\n\n        Keyword Args:\n            **kwargs: Other parameters forwarded (overriding) to Slipmap.\n\n        Returns:\n            Slipmap object for the same data as the Slisemap object.\n        \"\"\"\n        if keep_kernel:\n            kwargs.setdefault(\"kernel\", sm.kernel)\n            kwargs.setdefault(\"distance\", sm.distance)\n        kwargs.setdefault(\"radius\", sm.radius)\n        kwargs.setdefault(\"d\", sm.d)\n        kwargs.setdefault(\"lasso\", sm.lasso)\n        kwargs.setdefault(\"ridge\", sm.ridge)\n        kwargs.setdefault(\"intercept\", sm.intercept)\n        kwargs.setdefault(\"local_model\", (sm.local_model, sm.local_loss, sm.q))\n        kwargs.setdefault(\"Z0\", sm.get_Z(scale=False, rotate=True, numpy=False))\n        kwargs.setdefault(\"jit\", sm.jit)\n        B = sm.get_B(False)\n        sp = Slipmap(\n            X=sm.get_X(numpy=False, intercept=False),\n            y=sm.get_Y(numpy=False),\n            Bp0=B[:1, ...],\n            **kwargs,\n            **sm.tensorargs,\n        )\n        D = sp.get_D(numpy=False, proto_rows=True, proto_cols=False)\n        sp._Bp[...] = B[torch.argmin(D, 1), ...]\n        return sp\n\n    def into(self, keep_kernel: bool = False) -&gt; Slisemap:\n        \"\"\"Convert a Slipmap object into a Slisemap object.\n\n        Args:\n            keep_kernel: Use the kernel from the Slipmap object. Defaults to False.\n\n        Returns:\n            Slisemap object for the same data as the Slipmap object.\n        \"\"\"\n        kwargs = {}\n        if keep_kernel:\n            kwargs[\"kernel\"] = self.kernel\n        return Slisemap(\n            X=self.get_X(numpy=False, intercept=False),\n            y=self.get_Y(numpy=False),\n            radius=self.radius,\n            d=self.d,\n            lasso=self.lasso,\n            ridge=self.ridge,\n            intercept=self.intercept,\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            coefficients=self.q,\n            distance=self.distance,\n            B0=self.get_B(numpy=False),\n            Z0=self.get_Z(numpy=False),\n            jit=self.jit,\n            **{**self.tensorargs, **kwargs},\n        )\n\n    def save(\n        self,\n        f: Union[str, PathLike, BinaryIO],\n        any_extension: bool = False,\n        compress: Union[bool, int] = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the Slipmap object to a file.\n\n        This method uses ``torch.save`` (which uses ``pickle`` for the non-pytorch properties).\n        This means that lambda-functions are not supported (unless a custom pickle module is used, see ``torch.save``).\n\n        Note that the random state is not saved, only the initial seed (if set).\n\n        The default file extension is \".sp\".\n\n        Args:\n            f: Either a Path-like object or a (writable) File-like object.\n            any_extension: Do not check the file extension. Defaults to False.\n            compress: Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Parameters forwarded to ``torch.save``.\n        \"\"\"\n        if not any_extension and isinstance(f, (str, PathLike)):  # noqa: SIM102\n            if not str(f).endswith(\".sp\"):\n                _warn(\n                    \"When saving Slipmap objects, consider using the '.sp' extension for consistency.\",\n                    Slipmap.save,\n                )\n        loss = self._loss\n        try:\n            self.metadata.root = None\n            self._Z = self._Z.detach()\n            self._Bp = self._Bp.detach()\n            self._loss = None\n            if isinstance(compress, int) and compress &gt; 0:\n                with lzma.open(f, \"wb\", preset=compress) as f2:\n                    torch.save(self, f2, **kwargs)\n            elif compress:\n                with lzma.open(f, \"wb\") as f2:\n                    torch.save(self, f2, **kwargs)\n            else:\n                torch.save(self, f, **kwargs)\n        finally:\n            self.metadata.root = self\n            self._loss = loss\n\n    @classmethod\n    def load(\n        cls,\n        f: Union[str, PathLike, BinaryIO],\n        device: Union[None, str, torch.device] = None,\n        map_location: Optional[object] = None,\n        **kwargs: Any,\n    ) -&gt; \"Slipmap\":\n        \"\"\"Load a Slipmap object from a file.\n\n        This function uses ``torch.load``, so the tensors are restored to their previous devices.\n        Use ``device=\"cpu\"`` to avoid assuming that the same device exists.\n        This is useful if the Slipmap object has been trained on a GPU, but the current computer lacks a GPU.\n\n        Note that this is a classmethod, use it with: ``Slipmap.load(...)``.\n\n        SAFETY: This function is based on `torch.load` which (by default) uses `pickle`.\n        Do not use `Slipmap.load` on untrusted files, since `pickle` can run arbitrary Python code.\n\n        Args:\n            f: Either a Path-like object or a (readable) File-like object.\n            device: Device to load the tensors to (or the original if None). Defaults to None.\n            map_location: The same as `device` (this is the name used by `torch.load`). Defaults to None.\n\n        Keyword Args:\n            **kwargs: Parameters forwarded to `torch.load`.\n\n        Returns:\n            The loaded Slipmap object.\n        \"\"\"\n        if device is None:\n            device = map_location\n        try:\n            with lzma.open(f, \"rb\") as f2:\n                sm = torch.load(f2, map_location=device, **kwargs)\n        except lzma.LZMAError:\n            sm: Slipmap = torch.load(f, map_location=device, **kwargs)\n        return sm\n\n    def __setstate__(self, data: Any) -&gt; None:\n        # Handling loading of Slipmap objects from older versions\n        if not isinstance(data, dict):\n            data = next(d for d in data if isinstance(d, dict))\n        for k, v in data.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError as e:\n                _warn(e, Slipmap.__setstate__)\n        if isinstance(getattr(self, \"metadata\", {}), Metadata):\n            self.metadata.root = self\n        else:\n            self.metadata = Metadata(self, **getattr(self, \"metadata\", {}))\n        if not hasattr(self, \"_regularisation\"):\n            self._regularisation = ALocalModel.regularisation\n\n    def _get_loss_fn(self, individual: bool = False) -&gt; CallableLike[ALocalModel.loss]:\n        \"\"\"Return the Slipmap loss function.\n\n        This function JITs and caches the loss function for efficiency.\n\n        Args:\n            individual: Make a loss function for individual losses. Defaults to False.\n\n        Returns:\n            Loss function `(X, Y, Z, Bp, Zp) -&gt; loss`.\n        \"\"\"\n        if not individual and self._loss is not None:\n            return self._loss\n\n        def loss(\n            X: torch.Tensor,\n            Y: torch.Tensor,\n            Z: torch.Tensor,\n            Bp: torch.Tensor,\n            Zp: torch.Tensor,\n        ) -&gt; torch.Tensor:\n            \"\"\"Slipmap loss function.\n\n            Args:\n                X: Data matrix [n, m].\n                Y: Target matrix [n, k].\n                Z: Embedding matrix [n, d].\n                Bp: Local models [o, p].\n                Zp: Prototype embeddings [o, d].\n\n            Returns:\n                The loss value.\n            \"\"\"\n            if self.radius &gt; 0.0:\n                epsilon = self.radius * torch.finfo(Z.dtype).eps / 2\n                scale = torch.sqrt(torch.sum(Z**2) / Z.shape[0])\n                Z = Z * (self.radius / (scale + epsilon))\n            W = self.kernel(self.distance(Zp, Z))\n            Ytilde = self.local_model(X, Bp)\n            L = self.local_loss(Ytilde, Y)\n            loss = torch.sum(W * L, dim=0 if individual else ())\n            if not individual and self.lasso &gt; 0.0:\n                loss += self.lasso * torch.sum(torch.abs(Bp))\n            if not individual and self.ridge &gt; 0.0:\n                loss += self.ridge * torch.sum(Bp**2)\n            if not individual and self.radius &gt; 0.0:\n                loss += 1e-4 * (scale - self.radius) ** 2\n            if not individual:\n                loss += self.regularisation(self._X, self._Y, Z, self._Bp, Ytilde)\n            return loss\n\n        if individual:\n            return loss\n        if self._jit:\n            # JITting the loss function improves the performance\n            ex = (self._X[:1], self._Y[:1], self._Z[:1], self._Bp[:1], self._Zp[:1])\n            loss = torch.jit.trace(loss, ex)\n        # Caching the loss function\n        self._loss = loss\n        return self._loss\n\n    def value(\n        self, individual: bool = False, numpy: bool = True\n    ) -&gt; Union[float, np.ndarray, torch.Tensor]:\n        \"\"\"Calculate the loss value.\n\n        Args:\n            individual: Give loss individual loss values for the data points. Defaults to False.\n            numpy: Return the predictions as a `numpy.ndarray` instead of `torch.Tensor`. Defaults to True.\n\n        Returns:\n            The loss value(s).\n        \"\"\"\n        loss = self._get_loss_fn(individual)\n        loss = loss(X=self._X, Y=self._Y, Z=self._Z, Bp=self._Bp, Zp=self._Zp)\n        if individual:\n            return tonp(loss) if numpy else loss\n        else:\n            return loss.cpu().item() if numpy else loss\n\n    def _normalise(self, both: bool = False) -&gt; None:\n        \"\"\"Normalise Z.\"\"\"\n        if self.radius &gt; 0:\n            epsilon = self.radius * torch.finfo(self._Z.dtype).eps / 2\n            if both:  # Normalise the prototype embedding\n                scale = torch.sqrt(torch.sum(self._Zp**2) / self.p)\n                proto_rad = self.radius * np.sqrt(2)\n                if not np.allclose(proto_rad, scale.cpu().item()):\n                    self._Zp = self._Zp * (proto_rad / (scale + epsilon))\n            z_sum = torch.sum(self._Z**2, 1, True)\n            scale = torch.sqrt(z_sum.mean())\n            if not np.allclose(scale.cpu().item(), self.radius):\n                self._Z *= self.radius / (scale + epsilon)\n\n    def lbfgs(\n        self,\n        max_iter: int = 500,\n        verbose: bool = False,\n        *,\n        only_B: bool = False,\n        only_Z: bool = False,\n        **kwargs: Any,\n    ) -&gt; float:\n        \"\"\"Optimise Slipmap using LBFGS.\n\n        Args:\n            max_iter: Maximum number of LBFGS iterations. Defaults to 500.\n            verbose: Print status messages. Defaults to False.\n            only_B: Only optimise Bp. Defaults to False.\n            only_Z: Only optimise Z. Defaults to False.\n\n        Keyword Args:\n            **kwargs: Keyword arguments forwarded to [LBFGS][slisemap.utils.LBFGS].\n\n        Returns:\n            The loss value.\n        \"\"\"\n        if only_B == only_Z:\n            only_B = only_Z = True\n        Bp = self._Bp\n        Z = self._Z\n        if only_B:\n            Bp = Bp.clone().requires_grad_(True)\n        if only_Z:\n            Z = Z.clone().requires_grad_(True)\n\n        loss_ = self._get_loss_fn()\n        loss_fn = lambda: loss_(self._X, self._Y, Z, Bp, self._Zp)  # noqa: E731\n        pre_loss = loss_fn().cpu().detach().item()\n\n        opt = [Bp] if not only_Z else ([Z] if not only_B else [Z, Bp])\n        LBFGS(loss_fn, opt, max_iter=max_iter, verbose=verbose, **kwargs)\n        post_loss = loss_fn().cpu().detach().item()\n\n        if post_loss &lt; pre_loss:\n            if only_Z:\n                self._Z = Z.detach()\n                self._normalise()\n            if only_B:\n                self._Bp = Bp.detach()\n            return post_loss\n        else:\n            if verbose:\n                print(\"Slipmap.lbfgs: No improvement found\")\n            return pre_loss\n\n    def escape(\n        self, lerp: float = 0.9, outliers: bool = True, B_iter: int = 10\n    ) -&gt; None:\n        \"\"\"Escape from a local optimum by moving each data item embedding towards the most suitable prototype embedding.\n\n        Args:\n            lerp: Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.9.\n            outliers: Check for and reset embeddings outside the prototype grid. Defaults to True.\n            B_iter: Optimise B for `B_iter` number of LBFGS iterations. Set `B_iter=0` to disable. Defaults to 10.\n        \"\"\"\n        if lerp &lt;= 0.0:\n            _warn(\"Escaping with `lerp &lt;= 0` does nothing!\", Slipmap.escape)\n            return\n        L = self.get_L(numpy=False)\n        W = self.get_W(numpy=False, proto_rows=True, proto_cols=True)\n        index = torch.argmin(W @ L, 0)\n        if lerp &gt;= 1.0:\n            self._Z = self._Zp[index].clone()\n        else:\n            if outliers:  # Check for and reset outliers in the embedding\n                scale = torch.sum(self._Z**2, 1)\n                radius = torch.max(torch.sum(self._Zp**2, 1))\n                if torch.any(scale &gt;= radius):\n                    self._Z[scale &gt;= radius] = 0.0\n            self._Z = (1.0 - lerp) * self._Z + lerp * self._Zp[index]\n        self._normalise()\n        if B_iter &gt; 0:\n            self.lbfgs(max_iter=B_iter, only_B=True)\n\n    def optimize(\n        self,\n        patience: int = 2,\n        max_escapes: int = 100,\n        max_iter: int = 500,\n        only_B: bool = False,\n        verbose: Literal[0, 1, 2] = 0,\n        escape_kws: Dict[str, object] = {},\n        **kwargs: Any,\n    ) -&gt; float:\n        \"\"\"Optimise Slipmap by alternating between [Slipmap.lbfgs][slisemap.slipmap.Slipmap.lbfgs] and [Slipmap.escape][slisemap.slipmap.Slipmap.escape] until convergence.\n\n        Statistics for the optimisation can be found in `self.metadata[\"optimize_time\"]` and `self.metadata[\"optimize_loss\"]`.\n\n        Args:\n            patience: Number of escapes without improvement before stopping. Defaults to 2.\n            max_escapes: aximum numbers optimisation rounds. Defaults to 100.\n            max_iter: Maximum number of LBFGS iterations per round. Defaults to 500.\n            only_B: Only optimise the local models, not the embedding. Defaults to False.\n            verbose: Print status messages (0: no, 1: some, 2: all). Defaults to 0.\n            escape_kws: Optional keyword arguments to `Slipmap.escape`. Defaults to {}.\n\n        Keyword Args:\n            **kwargs: Keyword arguments forwaded to `Slipmap.lbfgs`.\n\n        Returns:\n            The loss value.\n        \"\"\"\n        loss = np.repeat(np.inf, 2)\n        time = timer()\n        loss[0] = self.lbfgs(\n            max_iter=max_iter,\n            only_B=True,\n            increase_tolerance=not only_B,\n            verbose=verbose &gt; 1,\n            **kwargs,\n        )\n        history = [loss[0]]\n        if verbose:\n            i = 0\n            print(f\"Slipmap.optimise LBFGS  {0:2d}: {loss[0]:.2f}\")\n        if only_B:\n            self.metadata[\"optimize_time\"] = timer() - time\n            self.metadata[\"optimize_loss\"] = history\n            return loss[0]\n        cc = CheckConvergence(patience, max_escapes)\n        while not cc.has_converged(loss, self.copy, verbose=verbose &gt; 1):\n            self.escape(**escape_kws)\n            loss[1] = self.value()\n            if verbose:\n                print(f\"Slipmap.optimise Escape {i:2d}: {loss[1]:.2f}\")\n            loss[0] = self.lbfgs(\n                max_iter, increase_tolerance=True, verbose=verbose &gt; 1, **kwargs\n            )\n            history.append(loss[1])\n            history.append(loss[0])\n            if verbose:\n                i += 1\n                print(f\"Slipmap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n        self._Z = cc.optimal._Z\n        self._Bp = cc.optimal._Bp\n        loss = self.lbfgs(\n            max_iter * 2, increase_tolerance=False, verbose=verbose &gt; 1, **kwargs\n        )\n        history.append(loss)\n        self.metadata[\"optimize_time\"] = timer() - time\n        self.metadata[\"optimize_loss\"] = history\n        if verbose:\n            print(f\"Slipmap.optimise Final    : {loss:.2f}\")\n        return loss\n\n    optimise = optimize\n\n    def predict(\n        self,\n        X: ToTensor,\n        weighted: bool = True,\n        numpy: bool = True,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Predict the outcome for new data items.\n\n        This function uses the nearest neighbour in X space to find the embedding.\n        Then the prediction is made with the local model (of the closest prototype).\n\n        Args:\n            X: Data matrix.\n            weighted: Use a weighted model instead of just the nearest. Defaults to True\n            numpy: Return the predictions as a `numpy.ndarray` instead of `torch.Tensor`. Defaults to True.\n\n        Returns:\n            Predicted Y:s.\n        \"\"\"\n        X = self._as_new_X(X)\n        xnn = torch.cdist(X, self._X).argmin(1)\n        if weighted:\n            Y = self.local_model(X, self._Bp)\n            D = self.get_D(True, False, numpy=False)[:, xnn]\n            W = softmax_column_kernel(D)\n            Y = torch.sum(W[..., None] * Y, 0)\n        else:\n            B = self.get_B(False)[xnn, :]\n            Y = local_predict(X, B, self.local_model)\n        return tonp(Y) if numpy else Y\n\n    def get_model_clusters(\n        self,\n        clusters: int,\n        B: Optional[np.ndarray] = None,\n        Z: Optional[np.ndarray] = None,\n        random_state: int = 42,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Cluster the local model coefficients using k-means (from scikit-learn).\n\n        This method (with a fixed random seed) is used for plotting Slipmap solutions.\n\n        Args:\n            clusters: Number of clusters.\n            B: B matrix. Defaults to `self.get_B()`.\n            Z: Z matrix. Defaults to `self.get_Z()`.\n            random_state: random_state for the KMeans clustering. Defaults to 42.\n\n        Keyword Args:\n            **kwargs: Additional arguments to `sklearn.cluster.KMeans` or `sklearn.cluster.MiniBatchKMeans` if `self.n &gt;= 1024`.\n\n        Returns:\n            labels: Vector of cluster labels.\n            centres: Matrix of cluster centres.\n        \"\"\"\n        B = B if B is not None else self.get_B()\n        Z = Z if Z is not None else self.get_Z()\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # Some sklearn versions warn about changing defaults for KMeans\n            kwargs.setdefault(\"random_state\", random_state)\n            if self.n &gt;= 1024:\n                km = MiniBatchKMeans(clusters, **kwargs).fit(B)\n            else:\n                km = KMeans(clusters, **kwargs).fit(B)\n        ord = np.argsort([Z[km.labels_ == k, 0].mean() for k in range(clusters)])\n        return np.argsort(ord)[km.labels_], km.cluster_centers_[ord]\n\n    def plot(\n        self,\n        title: str = \"\",\n        clusters: Union[None, int, np.ndarray] = None,\n        bars: Union[bool, int, Sequence[str]] = True,\n        jitter: Union[float, np.ndarray] = 0.0,\n        show: bool = True,\n        bar: Union[None, bool, int] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[Figure]:\n        \"\"\"Plot the Slipmap solution using seaborn.\n\n        Args:\n            title: Title of the plot. Defaults to \"\".\n            clusters: Can be None (plot individual losses), an int (plot k-means clusters of Bp), or an array of known cluster id:s. Defaults to None.\n            bars: If `clusters is not None`, plot the local models in a bar plot. If ``bar`` is an int then only plot the most influential variables. Defaults to True.\n            jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n            show: Show the plot. Defaults to True.\n            bar: Alternative spelling for `bars`. Defaults to None.\n\n        Keyword Args:\n            **kwargs: Additional arguments to [plot_solution][slisemap.plot.plot_solution] and `plt.subplots`.\n\n        Returns:\n            Matplotlib figure if `show=False`.\n        \"\"\"\n        if bar is not None:\n            bars = bar\n        B = self.get_B()\n        Z = self.get_Z()\n        if clusters is None:\n            loss = tonp(self.local_loss(self.predict(self._X, numpy=False), self._Y))\n            clusters = None\n            centers = None\n        else:\n            loss = None\n            if isinstance(clusters, int):\n                clusters, centers = self.get_model_clusters(clusters, B, Z)\n            else:\n                clusters = np.asarray(clusters)\n                centers = np.stack(\n                    [np.mean(B[clusters == c, :], 0) for c in np.unique(clusters)], 0\n                )\n        fig = plot_solution(\n            Z=Z,\n            B=B,\n            loss=loss,\n            clusters=clusters,\n            centers=centers,\n            coefficients=self.metadata.get_coefficients(),\n            dimensions=self.metadata.get_dimensions(long=True),\n            title=title,\n            bars=bars,\n            jitter=jitter,\n            **kwargs,\n        )\n        plot_prototypes(self.get_Zp(), fig.axes[0])\n        if show:\n            plt.show()\n        else:\n            return fig\n\n    def plot_position(\n        self,\n        X: Optional[ToTensor] = None,\n        Y: Optional[ToTensor] = None,\n        index: Union[None, int, Sequence[int]] = None,\n        title: str = \"\",\n        jitter: Union[float, np.ndarray] = 0.0,\n        legend_inside: bool = True,\n        show: bool = True,\n        **kwargs: Any,\n    ) -&gt; Optional[sns.FacetGrid]:\n        \"\"\"Plot local losses for alternative locations for the selected item(s).\n\n        Indicate the selected item(s) either via `X` and `Y` or via `index`.\n\n        Args:\n            X: Data matrix for the selected data item(s). Defaults to None.\n            Y: Response matrix for the selected data item(s). Defaults to None.\n            index: Index/indices of the selected data item(s). Defaults to None.\n            title: Title of the plot. Defaults to \"\".\n            jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n            legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n            show: Show the plot. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Additional arguments to `seaborn.relplot`.\n\n        Returns:\n            `seaborn.FacetGrid` if `show=False`.\n        \"\"\"\n        if index is None:\n            _assert(\n                X is not None and Y is not None,\n                \"Either index or X and Y must be given\",\n                Slipmap.plot_position,\n            )\n            L = self.get_L(X=X, Y=Y)\n        else:\n            if isinstance(index, int):\n                index = [index]\n            L = self.get_L()[:, index]\n        g = plot_position(\n            Z=self.get_Zp(),\n            L=L,\n            Zs=self.get_Z()[index, :] if index is not None else None,\n            dimensions=self.metadata.get_dimensions(long=True),\n            title=title,\n            jitter=jitter,\n            legend_inside=legend_inside,\n            marker_size=6.0,\n            **kwargs,\n        )\n        # plot_prototypes(self.get_Zp(), *g.axes.flat)\n        if show:\n            plt.show()\n        else:\n            return g\n\n    def plot_dist(\n        self,\n        title: str = \"\",\n        clusters: Union[None, int, np.ndarray] = None,\n        unscale: bool = True,\n        scatter: bool = False,\n        jitter: float = 0.0,\n        legend_inside: bool = True,\n        show: bool = True,\n        **kwargs: Any,\n    ) -&gt; Optional[sns.FacetGrid]:\n        \"\"\"Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.\n\n        Args:\n            title: Title of the plot. Defaults to \"\".\n            clusters: Number of cluster or vector of cluster labels. Defaults to None.\n            scatter: Use scatterplots instead of density plots (clusters are ignored). Defaults to False.\n            unscale: Unscale `X` and `Y` if scaling metadata has been given (see `Slisemap.metadata.set_scale_X`). Defaults to True.\n            jitter: Add jitter to the scatterplots. Defaults to 0.0.\n            legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n            show: Show the plot. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Additional arguments to `seaborn.relplot` or `seaborn.scatterplot`.\n\n        Returns:\n            `seaborn.FacetGrid` if `show=False`.\n        \"\"\"\n        X = self.get_X(intercept=False)\n        Y = self.get_Y()\n        if unscale:\n            X = self.metadata.unscale_X(X)\n            Y = self.metadata.unscale_Y(Y)\n        loss = tonp(self.local_loss(self.predict(self._X, numpy=False), self._Y))\n        if isinstance(clusters, int):\n            clusters, _ = self.get_model_clusters(clusters)\n        g = plot_dist(\n            X=X,\n            Y=Y,\n            Z=self.get_Z(),\n            loss=loss,\n            variables=self.metadata.get_variables(False),\n            targets=self.metadata.get_targets(),\n            dimensions=self.metadata.get_dimensions(long=True),\n            title=title,\n            clusters=clusters,\n            scatter=scatter,\n            jitter=jitter,\n            legend_inside=legend_inside,\n            **kwargs,\n        )\n        if scatter:\n            plot_prototypes(self.get_Zp(), *g.axes.flat)\n        if show:\n            plt.show()\n        else:\n            return g\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.__init__","title":"<code>__init__(X, y, radius=2.0, d=2, lasso=None, ridge=None, intercept=True, local_model=LinearRegression, local_loss=None, coefficients=None, regularisation=None, distance=squared_distance, kernel=softmax_column_kernel, Z0=None, Bp0=None, Zp0=None, prototypes=1.0, jit=True, dtype=torch.float32, device=None)</code>","text":"<p>Create a Slipmap object.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ToTensor</code> <p>Data matrix.</p> required <code>y</code> <code>ToTensor</code> <p>Target vector or matrix.</p> required <code>radius</code> <code>float</code> <p>The radius of the embedding Z. Defaults to 2.0.</p> <code>2.0</code> <code>d</code> <code>int</code> <p>The number of embedding dimensions. Defaults to 2.</p> <code>2</code> <code>lasso</code> <code>Optional[float]</code> <p>Lasso regularisation coefficient. Defaults to 0.0.</p> <code>None</code> <code>ridge</code> <code>Optional[float]</code> <p>Ridge regularisation coefficient. Defaults to 0.0.</p> <code>None</code> <code>intercept</code> <code>bool</code> <p>Should an intercept term be added to <code>X</code>. Defaults to True.</p> <code>True</code> <code>local_model</code> <code>Union[LocalModelCollection, CallableLike[predict]]</code> <p>Local model prediction function (see slisemap.local_models.identify_local_model). Defaults to LinearRegression.</p> <code>LinearRegression</code> <code>local_loss</code> <code>Optional[CallableLike[loss]]</code> <p>Local model loss function (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>coefficients</code> <code>Union[None, int, CallableLike[coefficients]]</code> <p>The number of local model coefficients (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>regularisation</code> <code>Union[None, CallableLike[regularisation]]</code> <p>Additional regularisation method (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>distance</code> <code>CallableLike[squared_distance]</code> <p>Distance function. Defaults to squared_distance.</p> <code>squared_distance</code> <code>kernel</code> <code>CallableLike[softmax_column_kernel]</code> <p>Kernel function. Defaults to softmax_column_kernel.</p> <code>softmax_column_kernel</code> <code>Z0</code> <code>Optional[ToTensor]</code> <p>Initial embedding for the data. Defaults to PCA.</p> <code>None</code> <code>Bp0</code> <code>Optional[ToTensor]</code> <p>Initial coefficients for the local models. Defaults to None.</p> <code>None</code> <code>Zp0</code> <code>Optional[ToTensor]</code> <p>Initial embedding for the prototypes. Defaults to <code>[make_grid][slisemap.utils.make_grid](prototypes)</code>.</p> <code>None</code> <code>prototypes</code> <code>Union[int, float]</code> <p>Number of prototypes (if &gt; 6) or prototype density (if &lt; 6.0). Defaults to 1.0.</p> <code>1.0</code> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function for increased performance (see <code>torch.jit.trace</code> for caveats). Defaults to True.</p> <code>True</code> <code>dtype</code> <code>dtype</code> <p>Floating type. Defaults to <code>torch.float32</code>.</p> <code>float32</code> <code>device</code> <code>Optional[device]</code> <p>Torch device. Defaults to None.</p> <code>None</code> Source code in <code>slisemap/slipmap.py</code> <pre><code>def __init__(\n    self,\n    X: ToTensor,\n    y: ToTensor,\n    radius: float = 2.0,\n    d: int = 2,\n    lasso: Optional[float] = None,\n    ridge: Optional[float] = None,\n    intercept: bool = True,\n    local_model: Union[\n        LocalModelCollection, CallableLike[ALocalModel.predict]\n    ] = LinearRegression,\n    local_loss: Optional[CallableLike[ALocalModel.loss]] = None,\n    coefficients: Union[None, int, CallableLike[ALocalModel.coefficients]] = None,\n    regularisation: Union[None, CallableLike[ALocalModel.regularisation]] = None,\n    distance: CallableLike[squared_distance] = squared_distance,\n    kernel: CallableLike[softmax_column_kernel] = softmax_column_kernel,\n    Z0: Optional[ToTensor] = None,\n    Bp0: Optional[ToTensor] = None,\n    Zp0: Optional[ToTensor] = None,\n    prototypes: Union[int, float] = 1.0,\n    jit: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n) -&gt; None:\n    \"\"\"Create a Slipmap object.\n\n    Args:\n        X: Data matrix.\n        y: Target vector or matrix.\n        radius: The radius of the embedding Z. Defaults to 2.0.\n        d: The number of embedding dimensions. Defaults to 2.\n        lasso: Lasso regularisation coefficient. Defaults to 0.0.\n        ridge: Ridge regularisation coefficient. Defaults to 0.0.\n        intercept: Should an intercept term be added to `X`. Defaults to True.\n        local_model: Local model prediction function (see [slisemap.local_models.identify_local_model][]). Defaults to [LinearRegression][slisemap.local_models.LinearRegression].\n        local_loss: Local model loss function (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        coefficients: The number of local model coefficients (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        regularisation: Additional regularisation method (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        distance: Distance function. Defaults to [squared_distance][slisemap.utils.squared_distance].\n        kernel: Kernel function. Defaults to [softmax_column_kernel][slisemap.utils.softmax_column_kernel].\n        Z0: Initial embedding for the data. Defaults to PCA.\n        Bp0: Initial coefficients for the local models. Defaults to None.\n        Zp0: Initial embedding for the prototypes. Defaults to `[make_grid][slisemap.utils.make_grid](prototypes)`.\n        prototypes: Number of prototypes (if &gt; 6) or prototype density (if &lt; 6.0). Defaults to 1.0.\n        jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats). Defaults to True.\n        dtype: Floating type. Defaults to `torch.float32`.\n        device: Torch device. Defaults to None.\n    \"\"\"\n    for s in Slipmap.__slots__:\n        # Initialise all attributes (to avoid attribute errors)\n        setattr(self, s, None)\n    if lasso is None and ridge is None:\n        _warn(\n            \"Consider using regularisation!\\n\"\n            + \"\\tRegularisation is important for handling small neighbourhoods, and also makes the local models more local.\"\n            + \" Lasso (l1) and ridge (l2) regularisation is built-in, via the parameters ``lasso`` and ``ridge``.\"\n            + \" Set ``lasso=0`` to disable this warning (if no regularisation is really desired).\",\n            Slipmap,\n        )\n    local_model, local_loss, coefficients, regularisation = identify_local_model(\n        local_model, local_loss, coefficients, regularisation\n    )\n    self.lasso = 0.0 if lasso is None else lasso\n    self.ridge = 0.0 if ridge is None else ridge\n    self.kernel = kernel\n    self.distance = distance\n    self.local_model = local_model\n    self.local_loss = local_loss\n    self.regularisation = regularisation\n    self._radius = radius\n    self._intercept = intercept\n    self._jit = jit\n    self.metadata = Metadata(self)\n\n    if device is None and isinstance(X, torch.Tensor):\n        device = X.device\n    tensorargs = {\"device\": device, \"dtype\": dtype}\n\n    if Zp0 is None:\n        if prototypes &lt; 6.0:\n            # Interpret prototypes as a density (prototypes per unit square)\n            prototypes = radius**2 * 2 * np.pi * prototypes\n        self._Zp = torch.as_tensor(make_grid(prototypes, d=d), **tensorargs)\n    else:\n        self._Zp = to_tensor(Zp0, **tensorargs)[0]\n    _assert_shape(self._Zp, (self._Zp.shape[0], d), \"Zp0\", Slipmap)\n\n    self._X, X_rows, X_columns = to_tensor(X, **tensorargs)\n    if intercept:\n        self._X = torch.cat((self._X, torch.ones_like(self._X[:, :1])), 1)\n    n, m = self._X.shape\n    self.metadata.set_variables(X_columns, intercept)\n\n    self._Y, Y_rows, Y_columns = to_tensor(y, **tensorargs)\n    self.metadata.set_targets(Y_columns)\n    if len(self._Y.shape) == 1:\n        self._Y = self._Y[:, None]\n    _assert_shape(self._Y, (n, self._Y.shape[1]), \"Y\", Slipmap)\n\n    if Z0 is None:\n        self._Z = self._X @ PCA_rotation(self._X, d)\n        if self._Z.shape[1] &lt; d:\n            _warn(\n                \"The number of embedding dimensions is larger than the number of data dimensions\",\n                Slisemap,\n            )\n            Z0fill = torch.zeros(size=[n, d - self._Z.shape[1]], **tensorargs)\n            self._Z = torch.cat((self._Z, Z0fill), 1)\n        Z_rows = None\n    else:\n        self._Z, Z_rows, Z_columns = to_tensor(Z0, **tensorargs)\n        self.metadata.set_dimensions(Z_columns)\n\n        _assert_shape(self._Z, (n, d), \"Z0\", Slipmap)\n    self._normalise(True)\n\n    if callable(coefficients):\n        coefficients = coefficients(self._X, self._Y)\n    if Bp0 is None:\n        Bp0 = global_model(\n            X=self._X,\n            Y=self._Y,\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            coefficients=coefficients,\n            lasso=self.lasso,\n            ridge=self.ridge,\n        )\n        if not torch.all(torch.isfinite(Bp0)):\n            _warn(\n                \"Optimising a global model as initialisation resulted in non-finite values. Consider using stronger regularisation (increase ``lasso`` or ``ridge``).\",\n                Slipmap,\n            )\n            Bp0 = torch.zeros_like(Bp0)\n        self._Bp = Bp0.expand((self.p, coefficients)).clone()\n        B_rows = None\n    else:\n        self._Bp, B_rows, B_columns = to_tensor(Bp0, **tensorargs)\n        if self._Bp.shape[0] == 1:\n            self._Bp = self._Bp.expand((self.p, coefficients)).clone()\n        _assert_shape(self._Bp, (self.p, coefficients), \"Bp0\", Slipmap)\n        self.metadata.set_coefficients(B_columns)\n    self.metadata.set_rows(X_rows, Y_rows, B_rows, Z_rows)\n    if (\n        device is None\n        and self.n * self.m * self.p * self.o &gt; 500_000\n        and torch.cuda.is_available()\n    ):\n        self.cuda()\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>The number of data items.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.m","title":"<code>m: int</code>  <code>property</code>","text":"<p>The number of variables (including potential intercept).</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.o","title":"<code>o: int</code>  <code>property</code>","text":"<p>The number of target variables (i.e. the number of classes).</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.d","title":"<code>d: int</code>  <code>property</code>","text":"<p>The number of embedding dimensions.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.p","title":"<code>p: int</code>  <code>property</code>","text":"<p>The number of prototypes.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.q","title":"<code>q: int</code>  <code>property</code>","text":"<p>The number of local model coefficients.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.intercept","title":"<code>intercept: bool</code>  <code>property</code>","text":"<p>Is an intercept column added to the data?.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.radius","title":"<code>radius: float</code>  <code>property</code> <code>writable</code>","text":"<p>The radius of the embedding.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.lasso","title":"<code>lasso: float</code>  <code>property</code> <code>writable</code>","text":"<p>Lasso regularisation strength.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.ridge","title":"<code>ridge: float</code>  <code>property</code> <code>writable</code>","text":"<p>Ridge regularisation strength.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.local_model","title":"<code>local_model: CallableLike[ALocalModel.predict]</code>  <code>property</code> <code>writable</code>","text":"<p>Local model prediction function. Takes in X[n, m] and B[n, q], and returns Ytilde[n, n, o].</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.local_loss","title":"<code>local_loss: CallableLike[ALocalModel.loss]</code>  <code>property</code> <code>writable</code>","text":"<p>Local model loss function. Takes in Ytilde[n, n, o] and Y[n, o] and returns L[n, n].</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.regularisation","title":"<code>regularisation: CallableLike[ALocalModel.regularisation]</code>  <code>property</code> <code>writable</code>","text":"<p>Regularisation function. Takes in X, Y, Bp, Z, and Ytilde and returns an additional loss scalar.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.distance","title":"<code>distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>  <code>property</code> <code>writable</code>","text":"<p>Distance function. Takes in Z[n1, d] and Z[n2, d], and returns D[n1, n2].</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.kernel","title":"<code>kernel: Callable[[torch.Tensor], torch.Tensor]</code>  <code>property</code> <code>writable</code>","text":"<p>Kernel function. Takes in D[n, n] and returns W[n, n].</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.jit","title":"<code>jit: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Just-In-Time compile the loss function?</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_Z","title":"<code>get_Z(numpy=True)</code>","text":"<p>Get the Z matrix (the embedding for all data items).</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The Z matrix <code>[n, d]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_Z(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the Z matrix (the embedding for all data items).\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The Z matrix `[n, d]`.\n    \"\"\"\n    self._normalise()\n    return tonp(self._Z) if numpy else self._Z\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_B","title":"<code>get_B(numpy=True)</code>","text":"<p>Get the B matrix (the coefficients of the closest local model for all data items).</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The B matrix <code>[n, q]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_B(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the B matrix (the coefficients of the closest local model for all data items).\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The B matrix `[n, q]`.\n    \"\"\"\n    B = self._Bp[self.get_closest(numpy=False)]\n    return tonp(B) if numpy else B\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_Zp","title":"<code>get_Zp(numpy=True)</code>","text":"<p>Get the Zp matrix (the embedding for the prototypes).</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The Zp matrix <code>[p, d]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_Zp(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the Zp matrix (the embedding for the prototypes).\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The Zp matrix `[p, d]`.\n    \"\"\"\n    return tonp(self._Zp) if numpy else self._Zp\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_Bp","title":"<code>get_Bp(numpy=True)</code>","text":"<p>Get the Bp matrix (the local model coefficients for the prototypes).</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The Bp matrix <code>[p, q]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_Bp(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the Bp matrix (the local model coefficients for the prototypes).\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The Bp matrix `[p, q]`.\n    \"\"\"\n    return tonp(self._Bp) if numpy else self._Bp\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_X","title":"<code>get_X(intercept=True, numpy=True)</code>","text":"<p>Get the data matrix.</p> <p>Parameters:</p> Name Type Description Default <code>intercept</code> <code>bool</code> <p>Include the intercept column (if <code>self.intercept == True</code>). Defaults to True.</p> <code>True</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The X matrix <code>[n, m]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_X(\n    self, intercept: bool = True, numpy: bool = True\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the data matrix.\n\n    Args:\n        intercept: Include the intercept column (if ``self.intercept == True``). Defaults to True.\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n    Returns:\n        The X matrix `[n, m]`.\n    \"\"\"\n    X = self._X if intercept or not self._intercept else self._X[:, :-1]\n    return tonp(X) if numpy else X\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_Y","title":"<code>get_Y(ravel=False, numpy=True)</code>","text":"<p>Get the target matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ravel</code> <code>bool</code> <p>Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.</p> <code>False</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The Y matrix <code>[n, o]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_Y(\n    self, ravel: bool = False, numpy: bool = True\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the target matrix.\n\n    Args:\n        ravel: Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n    Returns:\n        The Y matrix `[n, o]`.\n    \"\"\"\n    Y = self._Y.ravel() if ravel else self._Y\n    return tonp(Y) if numpy else Y\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_D","title":"<code>get_D(proto_rows=True, proto_cols=False, Z=None, numpy=True)</code>","text":"<p>Get the embedding distance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>proto_rows</code> <code>bool</code> <p>Calculate the distances with the prototype embeddings on the rows. Defaults to True.</p> <code>True</code> <code>proto_cols</code> <code>bool</code> <p>Calculate the distances with the prototype embeddings on the columns. Defaults to False.</p> <code>False</code> <code>Z</code> <code>Optional[Tensor]</code> <p>Optional replacement for the training Z. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The D matrix <code>[n or p, n or p]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_D(\n    self,\n    proto_rows: bool = True,\n    proto_cols: bool = False,\n    Z: Optional[torch.Tensor] = None,\n    numpy: bool = True,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the embedding distance matrix.\n\n    Args:\n        proto_rows: Calculate the distances with the prototype embeddings on the rows. Defaults to True.\n        proto_cols: Calculate the distances with the prototype embeddings on the columns. Defaults to False.\n        Z: Optional replacement for the training Z. Defaults to None.\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The D matrix `[n or p, n or p]`.\n    \"\"\"\n    if proto_rows and proto_cols:\n        D = self._distance(self._Zp, self._Zp)\n    else:\n        Z = self.get_Z(numpy=False) if Z is None else Z\n        if proto_rows:\n            D = self._distance(self._Zp, Z)\n        elif proto_cols:\n            D = self._distance(Z, self._Zp)\n        else:\n            D = self._distance(Z, Z)\n    return tonp(D) if numpy else D\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_W","title":"<code>get_W(proto_rows=True, proto_cols=False, Z=None, numpy=True)</code>","text":"<p>Get the weight matrix.</p> <p>Parameters:</p> Name Type Description Default <code>proto_rows</code> <code>bool</code> <p>Calculate the weights with the prototype embeddings on the rows. Defaults to True.</p> <code>True</code> <code>proto_cols</code> <code>bool</code> <p>Calculate the weights with the prototype embeddings on the columns. Defaults to False.</p> <code>False</code> <code>Z</code> <code>Optional[Tensor]</code> <p>Optional replacement for the training Z. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The W matrix <code>[n or p, n or p]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_W(\n    self,\n    proto_rows: bool = True,\n    proto_cols: bool = False,\n    Z: Optional[torch.Tensor] = None,\n    numpy: bool = True,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the weight matrix.\n\n    Args:\n        proto_rows: Calculate the weights with the prototype embeddings on the rows. Defaults to True.\n        proto_cols: Calculate the weights with the prototype embeddings on the columns. Defaults to False.\n        Z: Optional replacement for the training Z. Defaults to None.\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n    Returns:\n        The W matrix `[n or p, n or p]`.\n    \"\"\"\n    D = self.get_D(numpy=False, proto_rows=proto_rows, proto_cols=proto_cols, Z=Z)\n    W = self.kernel(D)\n    return tonp(W) if numpy else W\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_L","title":"<code>get_L(X=None, Y=None, numpy=True)</code>","text":"<p>Get the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ToTensor]</code> <p>Optional replacement for the training X. Defaults to None.</p> <code>None</code> <code>Y</code> <code>Optional[ToTensor]</code> <p>Optional replacement for the training Y. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The L matrix <code>[p, n]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_L(\n    self,\n    X: Optional[ToTensor] = None,\n    Y: Optional[ToTensor] = None,\n    numpy: bool = True,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the loss matrix.\n\n    Args:\n        X: Optional replacement for the training X. Defaults to None.\n        Y: Optional replacement for the training Y. Defaults to None.\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n        The L matrix `[p, n]`.\n    \"\"\"\n    X = self._as_new_X(X)\n    Y = self._as_new_Y(Y, X.shape[0])\n    L = self.local_loss(self.local_model(X, self._Bp), Y)\n    return tonp(L) if numpy else L\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_closest","title":"<code>get_closest(Z=None, numpy=True)</code>","text":"<p>Get the closest prototype for each data item.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>Optional[Tensor]</code> <p>Optional replacement for the training Z. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the vector as a numpy (True) or pytorch (False) array. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Index vector <code>[n]</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_closest(\n    self, Z: Optional[torch.Tensor] = None, numpy: bool = True\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the closest prototype for each data item.\n\n    Args:\n        Z: Optional replacement for the training Z. Defaults to None.\n        numpy: Return the vector as a numpy (True) or pytorch (False) array. Defaults to True.\n\n    Returns:\n        Index vector `[n]`.\n    \"\"\"\n    D = self.get_D(numpy=False, Z=Z, proto_rows=True, proto_cols=False)\n    index = torch.argmin(D, 0)\n    return tonp(index) if numpy else index\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.tensorargs","title":"<code>tensorargs: Dict[str, Any]</code>  <code>property</code>","text":"<p>When creating a new <code>torch.Tensor</code> add these keyword arguments to match the <code>dtype</code> and <code>device</code> of this Slisemap object.</p>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.cuda","title":"<code>cuda(**kwargs)</code>","text":"<p>Move the tensors to CUDA memory (and run the calculations there).</p> <p>Note that this resets the random state.</p> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional arguments to <code>torch.Tensor.cuda</code></p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def cuda(self, **kwargs: Any) -&gt; None:\n    \"\"\"Move the tensors to CUDA memory (and run the calculations there).\n\n    Note that this resets the random state.\n\n    Keyword Args:\n        **kwargs: Optional arguments to ``torch.Tensor.cuda``\n    \"\"\"\n    X = self._X.cuda(**kwargs)\n    self._X = X\n    self._Y = self._Y.cuda(**kwargs)\n    self._Z = self._Z.cuda(**kwargs)\n    self._Bp = self._Bp.cuda(**kwargs)\n    self._Zp = self._Zp.cuda(**kwargs)\n    self._loss = None  # invalidate cached loss function\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.cpu","title":"<code>cpu(**kwargs)</code>","text":"<p>Move the tensors to CPU memory (and run the calculations there).</p> <p>Note that this resets the random state.</p> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional arguments to <code>torch.Tensor.cpu</code></p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def cpu(self, **kwargs: Any) -&gt; None:\n    \"\"\"Move the tensors to CPU memory (and run the calculations there).\n\n    Note that this resets the random state.\n\n    Keyword Args:\n        **kwargs: Optional arguments to ``torch.Tensor.cpu``\n    \"\"\"\n    X = self._X.cpu(**kwargs)\n    self._X = X\n    self._Y = self._Y.cpu(**kwargs)\n    self._Z = self._Z.cpu(**kwargs)\n    self._Bp = self._Bp.cpu(**kwargs)\n    self._Zp = self._Zp.cpu(**kwargs)\n    self._loss = None  # invalidate cached loss function\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.copy","title":"<code>copy()</code>","text":"<p>Make a copy of this Slipmap that references as much of the same torch-data as possible.</p> <p>Returns     An almost shallow copy of this Slipmap object.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def copy(self) -&gt; \"Slipmap\":\n    \"\"\"Make a copy of this Slipmap that references as much of the same torch-data as possible.\n\n    Returns\n        An almost shallow copy of this Slipmap object.\n    \"\"\"\n    other = copy(self)  # Shallow copy!\n    # Deep copy these:\n    other._Z = other._Z.clone().detach()\n    other._Bp = other._Bp.clone().detach()\n    return other\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.convert","title":"<code>convert(sm, keep_kernel=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert a Slisemap object into a Slipmap object.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Slisemap</code> <p>Slisemap object.</p> required <code>keep_kernel</code> <code>bool</code> <p>Use the kernel and distance functions from the Slisemap object. Defaults to False.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Other parameters forwarded (overriding) to Slipmap.</p> <p>Returns:</p> Type Description <code>Slipmap</code> <p>Slipmap object for the same data as the Slisemap object.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>@classmethod\ndef convert(\n    cls, sm: Slisemap, keep_kernel: bool = False, **kwargs: Any\n) -&gt; \"Slipmap\":\n    \"\"\"Convert a Slisemap object into a Slipmap object.\n\n    Args:\n        sm: Slisemap object.\n        keep_kernel: Use the kernel and distance functions from the Slisemap object. Defaults to False.\n\n    Keyword Args:\n        **kwargs: Other parameters forwarded (overriding) to Slipmap.\n\n    Returns:\n        Slipmap object for the same data as the Slisemap object.\n    \"\"\"\n    if keep_kernel:\n        kwargs.setdefault(\"kernel\", sm.kernel)\n        kwargs.setdefault(\"distance\", sm.distance)\n    kwargs.setdefault(\"radius\", sm.radius)\n    kwargs.setdefault(\"d\", sm.d)\n    kwargs.setdefault(\"lasso\", sm.lasso)\n    kwargs.setdefault(\"ridge\", sm.ridge)\n    kwargs.setdefault(\"intercept\", sm.intercept)\n    kwargs.setdefault(\"local_model\", (sm.local_model, sm.local_loss, sm.q))\n    kwargs.setdefault(\"Z0\", sm.get_Z(scale=False, rotate=True, numpy=False))\n    kwargs.setdefault(\"jit\", sm.jit)\n    B = sm.get_B(False)\n    sp = Slipmap(\n        X=sm.get_X(numpy=False, intercept=False),\n        y=sm.get_Y(numpy=False),\n        Bp0=B[:1, ...],\n        **kwargs,\n        **sm.tensorargs,\n    )\n    D = sp.get_D(numpy=False, proto_rows=True, proto_cols=False)\n    sp._Bp[...] = B[torch.argmin(D, 1), ...]\n    return sp\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.into","title":"<code>into(keep_kernel=False)</code>","text":"<p>Convert a Slipmap object into a Slisemap object.</p> <p>Parameters:</p> Name Type Description Default <code>keep_kernel</code> <code>bool</code> <p>Use the kernel from the Slipmap object. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Slisemap</code> <p>Slisemap object for the same data as the Slipmap object.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def into(self, keep_kernel: bool = False) -&gt; Slisemap:\n    \"\"\"Convert a Slipmap object into a Slisemap object.\n\n    Args:\n        keep_kernel: Use the kernel from the Slipmap object. Defaults to False.\n\n    Returns:\n        Slisemap object for the same data as the Slipmap object.\n    \"\"\"\n    kwargs = {}\n    if keep_kernel:\n        kwargs[\"kernel\"] = self.kernel\n    return Slisemap(\n        X=self.get_X(numpy=False, intercept=False),\n        y=self.get_Y(numpy=False),\n        radius=self.radius,\n        d=self.d,\n        lasso=self.lasso,\n        ridge=self.ridge,\n        intercept=self.intercept,\n        local_model=self.local_model,\n        local_loss=self.local_loss,\n        coefficients=self.q,\n        distance=self.distance,\n        B0=self.get_B(numpy=False),\n        Z0=self.get_Z(numpy=False),\n        jit=self.jit,\n        **{**self.tensorargs, **kwargs},\n    )\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.save","title":"<code>save(f, any_extension=False, compress=True, **kwargs)</code>","text":"<p>Save the Slipmap object to a file.</p> <p>This method uses <code>torch.save</code> (which uses <code>pickle</code> for the non-pytorch properties). This means that lambda-functions are not supported (unless a custom pickle module is used, see <code>torch.save</code>).</p> <p>Note that the random state is not saved, only the initial seed (if set).</p> <p>The default file extension is \".sp\".</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Union[str, PathLike, BinaryIO]</code> <p>Either a Path-like object or a (writable) File-like object.</p> required <code>any_extension</code> <code>bool</code> <p>Do not check the file extension. Defaults to False.</p> <code>False</code> <code>compress</code> <code>Union[bool, int]</code> <p>Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Parameters forwarded to <code>torch.save</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def save(\n    self,\n    f: Union[str, PathLike, BinaryIO],\n    any_extension: bool = False,\n    compress: Union[bool, int] = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the Slipmap object to a file.\n\n    This method uses ``torch.save`` (which uses ``pickle`` for the non-pytorch properties).\n    This means that lambda-functions are not supported (unless a custom pickle module is used, see ``torch.save``).\n\n    Note that the random state is not saved, only the initial seed (if set).\n\n    The default file extension is \".sp\".\n\n    Args:\n        f: Either a Path-like object or a (writable) File-like object.\n        any_extension: Do not check the file extension. Defaults to False.\n        compress: Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Parameters forwarded to ``torch.save``.\n    \"\"\"\n    if not any_extension and isinstance(f, (str, PathLike)):  # noqa: SIM102\n        if not str(f).endswith(\".sp\"):\n            _warn(\n                \"When saving Slipmap objects, consider using the '.sp' extension for consistency.\",\n                Slipmap.save,\n            )\n    loss = self._loss\n    try:\n        self.metadata.root = None\n        self._Z = self._Z.detach()\n        self._Bp = self._Bp.detach()\n        self._loss = None\n        if isinstance(compress, int) and compress &gt; 0:\n            with lzma.open(f, \"wb\", preset=compress) as f2:\n                torch.save(self, f2, **kwargs)\n        elif compress:\n            with lzma.open(f, \"wb\") as f2:\n                torch.save(self, f2, **kwargs)\n        else:\n            torch.save(self, f, **kwargs)\n    finally:\n        self.metadata.root = self\n        self._loss = loss\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.load","title":"<code>load(f, device=None, map_location=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load a Slipmap object from a file.</p> <p>This function uses <code>torch.load</code>, so the tensors are restored to their previous devices. Use <code>device=\"cpu\"</code> to avoid assuming that the same device exists. This is useful if the Slipmap object has been trained on a GPU, but the current computer lacks a GPU.</p> <p>Note that this is a classmethod, use it with: <code>Slipmap.load(...)</code>.</p> <p>SAFETY: This function is based on <code>torch.load</code> which (by default) uses <code>pickle</code>. Do not use <code>Slipmap.load</code> on untrusted files, since <code>pickle</code> can run arbitrary Python code.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Union[str, PathLike, BinaryIO]</code> <p>Either a Path-like object or a (readable) File-like object.</p> required <code>device</code> <code>Union[None, str, device]</code> <p>Device to load the tensors to (or the original if None). Defaults to None.</p> <code>None</code> <code>map_location</code> <code>Optional[object]</code> <p>The same as <code>device</code> (this is the name used by <code>torch.load</code>). Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Parameters forwarded to <code>torch.load</code>.</p> <p>Returns:</p> Type Description <code>Slipmap</code> <p>The loaded Slipmap object.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    f: Union[str, PathLike, BinaryIO],\n    device: Union[None, str, torch.device] = None,\n    map_location: Optional[object] = None,\n    **kwargs: Any,\n) -&gt; \"Slipmap\":\n    \"\"\"Load a Slipmap object from a file.\n\n    This function uses ``torch.load``, so the tensors are restored to their previous devices.\n    Use ``device=\"cpu\"`` to avoid assuming that the same device exists.\n    This is useful if the Slipmap object has been trained on a GPU, but the current computer lacks a GPU.\n\n    Note that this is a classmethod, use it with: ``Slipmap.load(...)``.\n\n    SAFETY: This function is based on `torch.load` which (by default) uses `pickle`.\n    Do not use `Slipmap.load` on untrusted files, since `pickle` can run arbitrary Python code.\n\n    Args:\n        f: Either a Path-like object or a (readable) File-like object.\n        device: Device to load the tensors to (or the original if None). Defaults to None.\n        map_location: The same as `device` (this is the name used by `torch.load`). Defaults to None.\n\n    Keyword Args:\n        **kwargs: Parameters forwarded to `torch.load`.\n\n    Returns:\n        The loaded Slipmap object.\n    \"\"\"\n    if device is None:\n        device = map_location\n    try:\n        with lzma.open(f, \"rb\") as f2:\n            sm = torch.load(f2, map_location=device, **kwargs)\n    except lzma.LZMAError:\n        sm: Slipmap = torch.load(f, map_location=device, **kwargs)\n    return sm\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.value","title":"<code>value(individual=False, numpy=True)</code>","text":"<p>Calculate the loss value.</p> <p>Parameters:</p> Name Type Description Default <code>individual</code> <code>bool</code> <p>Give loss individual loss values for the data points. Defaults to False.</p> <code>False</code> <code>numpy</code> <code>bool</code> <p>Return the predictions as a <code>numpy.ndarray</code> instead of <code>torch.Tensor</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[float, ndarray, Tensor]</code> <p>The loss value(s).</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def value(\n    self, individual: bool = False, numpy: bool = True\n) -&gt; Union[float, np.ndarray, torch.Tensor]:\n    \"\"\"Calculate the loss value.\n\n    Args:\n        individual: Give loss individual loss values for the data points. Defaults to False.\n        numpy: Return the predictions as a `numpy.ndarray` instead of `torch.Tensor`. Defaults to True.\n\n    Returns:\n        The loss value(s).\n    \"\"\"\n    loss = self._get_loss_fn(individual)\n    loss = loss(X=self._X, Y=self._Y, Z=self._Z, Bp=self._Bp, Zp=self._Zp)\n    if individual:\n        return tonp(loss) if numpy else loss\n    else:\n        return loss.cpu().item() if numpy else loss\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.lbfgs","title":"<code>lbfgs(max_iter=500, verbose=False, *, only_B=False, only_Z=False, **kwargs)</code>","text":"<p>Optimise Slipmap using LBFGS.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <code>int</code> <p>Maximum number of LBFGS iterations. Defaults to 500.</p> <code>500</code> <code>verbose</code> <code>bool</code> <p>Print status messages. Defaults to False.</p> <code>False</code> <code>only_B</code> <code>bool</code> <p>Only optimise Bp. Defaults to False.</p> <code>False</code> <code>only_Z</code> <code>bool</code> <p>Only optimise Z. Defaults to False.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to LBFGS.</p> <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def lbfgs(\n    self,\n    max_iter: int = 500,\n    verbose: bool = False,\n    *,\n    only_B: bool = False,\n    only_Z: bool = False,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Optimise Slipmap using LBFGS.\n\n    Args:\n        max_iter: Maximum number of LBFGS iterations. Defaults to 500.\n        verbose: Print status messages. Defaults to False.\n        only_B: Only optimise Bp. Defaults to False.\n        only_Z: Only optimise Z. Defaults to False.\n\n    Keyword Args:\n        **kwargs: Keyword arguments forwarded to [LBFGS][slisemap.utils.LBFGS].\n\n    Returns:\n        The loss value.\n    \"\"\"\n    if only_B == only_Z:\n        only_B = only_Z = True\n    Bp = self._Bp\n    Z = self._Z\n    if only_B:\n        Bp = Bp.clone().requires_grad_(True)\n    if only_Z:\n        Z = Z.clone().requires_grad_(True)\n\n    loss_ = self._get_loss_fn()\n    loss_fn = lambda: loss_(self._X, self._Y, Z, Bp, self._Zp)  # noqa: E731\n    pre_loss = loss_fn().cpu().detach().item()\n\n    opt = [Bp] if not only_Z else ([Z] if not only_B else [Z, Bp])\n    LBFGS(loss_fn, opt, max_iter=max_iter, verbose=verbose, **kwargs)\n    post_loss = loss_fn().cpu().detach().item()\n\n    if post_loss &lt; pre_loss:\n        if only_Z:\n            self._Z = Z.detach()\n            self._normalise()\n        if only_B:\n            self._Bp = Bp.detach()\n        return post_loss\n    else:\n        if verbose:\n            print(\"Slipmap.lbfgs: No improvement found\")\n        return pre_loss\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.escape","title":"<code>escape(lerp=0.9, outliers=True, B_iter=10)</code>","text":"<p>Escape from a local optimum by moving each data item embedding towards the most suitable prototype embedding.</p> <p>Parameters:</p> Name Type Description Default <code>lerp</code> <code>float</code> <p>Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.9.</p> <code>0.9</code> <code>outliers</code> <code>bool</code> <p>Check for and reset embeddings outside the prototype grid. Defaults to True.</p> <code>True</code> <code>B_iter</code> <code>int</code> <p>Optimise B for <code>B_iter</code> number of LBFGS iterations. Set <code>B_iter=0</code> to disable. Defaults to 10.</p> <code>10</code> Source code in <code>slisemap/slipmap.py</code> <pre><code>def escape(\n    self, lerp: float = 0.9, outliers: bool = True, B_iter: int = 10\n) -&gt; None:\n    \"\"\"Escape from a local optimum by moving each data item embedding towards the most suitable prototype embedding.\n\n    Args:\n        lerp: Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.9.\n        outliers: Check for and reset embeddings outside the prototype grid. Defaults to True.\n        B_iter: Optimise B for `B_iter` number of LBFGS iterations. Set `B_iter=0` to disable. Defaults to 10.\n    \"\"\"\n    if lerp &lt;= 0.0:\n        _warn(\"Escaping with `lerp &lt;= 0` does nothing!\", Slipmap.escape)\n        return\n    L = self.get_L(numpy=False)\n    W = self.get_W(numpy=False, proto_rows=True, proto_cols=True)\n    index = torch.argmin(W @ L, 0)\n    if lerp &gt;= 1.0:\n        self._Z = self._Zp[index].clone()\n    else:\n        if outliers:  # Check for and reset outliers in the embedding\n            scale = torch.sum(self._Z**2, 1)\n            radius = torch.max(torch.sum(self._Zp**2, 1))\n            if torch.any(scale &gt;= radius):\n                self._Z[scale &gt;= radius] = 0.0\n        self._Z = (1.0 - lerp) * self._Z + lerp * self._Zp[index]\n    self._normalise()\n    if B_iter &gt; 0:\n        self.lbfgs(max_iter=B_iter, only_B=True)\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.optimize","title":"<code>optimize(patience=2, max_escapes=100, max_iter=500, only_B=False, verbose=0, escape_kws={}, **kwargs)</code>","text":"<p>Optimise Slipmap by alternating between Slipmap.lbfgs and Slipmap.escape until convergence.</p> <p>Statistics for the optimisation can be found in <code>self.metadata[\"optimize_time\"]</code> and <code>self.metadata[\"optimize_loss\"]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of escapes without improvement before stopping. Defaults to 2.</p> <code>2</code> <code>max_escapes</code> <code>int</code> <p>aximum numbers optimisation rounds. Defaults to 100.</p> <code>100</code> <code>max_iter</code> <code>int</code> <p>Maximum number of LBFGS iterations per round. Defaults to 500.</p> <code>500</code> <code>only_B</code> <code>bool</code> <p>Only optimise the local models, not the embedding. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>Literal[0, 1, 2]</code> <p>Print status messages (0: no, 1: some, 2: all). Defaults to 0.</p> <code>0</code> <code>escape_kws</code> <code>Dict[str, object]</code> <p>Optional keyword arguments to <code>Slipmap.escape</code>. Defaults to {}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwaded to <code>Slipmap.lbfgs</code>.</p> <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def optimize(\n    self,\n    patience: int = 2,\n    max_escapes: int = 100,\n    max_iter: int = 500,\n    only_B: bool = False,\n    verbose: Literal[0, 1, 2] = 0,\n    escape_kws: Dict[str, object] = {},\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Optimise Slipmap by alternating between [Slipmap.lbfgs][slisemap.slipmap.Slipmap.lbfgs] and [Slipmap.escape][slisemap.slipmap.Slipmap.escape] until convergence.\n\n    Statistics for the optimisation can be found in `self.metadata[\"optimize_time\"]` and `self.metadata[\"optimize_loss\"]`.\n\n    Args:\n        patience: Number of escapes without improvement before stopping. Defaults to 2.\n        max_escapes: aximum numbers optimisation rounds. Defaults to 100.\n        max_iter: Maximum number of LBFGS iterations per round. Defaults to 500.\n        only_B: Only optimise the local models, not the embedding. Defaults to False.\n        verbose: Print status messages (0: no, 1: some, 2: all). Defaults to 0.\n        escape_kws: Optional keyword arguments to `Slipmap.escape`. Defaults to {}.\n\n    Keyword Args:\n        **kwargs: Keyword arguments forwaded to `Slipmap.lbfgs`.\n\n    Returns:\n        The loss value.\n    \"\"\"\n    loss = np.repeat(np.inf, 2)\n    time = timer()\n    loss[0] = self.lbfgs(\n        max_iter=max_iter,\n        only_B=True,\n        increase_tolerance=not only_B,\n        verbose=verbose &gt; 1,\n        **kwargs,\n    )\n    history = [loss[0]]\n    if verbose:\n        i = 0\n        print(f\"Slipmap.optimise LBFGS  {0:2d}: {loss[0]:.2f}\")\n    if only_B:\n        self.metadata[\"optimize_time\"] = timer() - time\n        self.metadata[\"optimize_loss\"] = history\n        return loss[0]\n    cc = CheckConvergence(patience, max_escapes)\n    while not cc.has_converged(loss, self.copy, verbose=verbose &gt; 1):\n        self.escape(**escape_kws)\n        loss[1] = self.value()\n        if verbose:\n            print(f\"Slipmap.optimise Escape {i:2d}: {loss[1]:.2f}\")\n        loss[0] = self.lbfgs(\n            max_iter, increase_tolerance=True, verbose=verbose &gt; 1, **kwargs\n        )\n        history.append(loss[1])\n        history.append(loss[0])\n        if verbose:\n            i += 1\n            print(f\"Slipmap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n    self._Z = cc.optimal._Z\n    self._Bp = cc.optimal._Bp\n    loss = self.lbfgs(\n        max_iter * 2, increase_tolerance=False, verbose=verbose &gt; 1, **kwargs\n    )\n    history.append(loss)\n    self.metadata[\"optimize_time\"] = timer() - time\n    self.metadata[\"optimize_loss\"] = history\n    if verbose:\n        print(f\"Slipmap.optimise Final    : {loss:.2f}\")\n    return loss\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.predict","title":"<code>predict(X, weighted=True, numpy=True)</code>","text":"<p>Predict the outcome for new data items.</p> <p>This function uses the nearest neighbour in X space to find the embedding. Then the prediction is made with the local model (of the closest prototype).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ToTensor</code> <p>Data matrix.</p> required <code>weighted</code> <code>bool</code> <p>Use a weighted model instead of just the nearest. Defaults to True</p> <code>True</code> <code>numpy</code> <code>bool</code> <p>Return the predictions as a <code>numpy.ndarray</code> instead of <code>torch.Tensor</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Predicted Y:s.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def predict(\n    self,\n    X: ToTensor,\n    weighted: bool = True,\n    numpy: bool = True,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Predict the outcome for new data items.\n\n    This function uses the nearest neighbour in X space to find the embedding.\n    Then the prediction is made with the local model (of the closest prototype).\n\n    Args:\n        X: Data matrix.\n        weighted: Use a weighted model instead of just the nearest. Defaults to True\n        numpy: Return the predictions as a `numpy.ndarray` instead of `torch.Tensor`. Defaults to True.\n\n    Returns:\n        Predicted Y:s.\n    \"\"\"\n    X = self._as_new_X(X)\n    xnn = torch.cdist(X, self._X).argmin(1)\n    if weighted:\n        Y = self.local_model(X, self._Bp)\n        D = self.get_D(True, False, numpy=False)[:, xnn]\n        W = softmax_column_kernel(D)\n        Y = torch.sum(W[..., None] * Y, 0)\n    else:\n        B = self.get_B(False)[xnn, :]\n        Y = local_predict(X, B, self.local_model)\n    return tonp(Y) if numpy else Y\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.get_model_clusters","title":"<code>get_model_clusters(clusters, B=None, Z=None, random_state=42, **kwargs)</code>","text":"<p>Cluster the local model coefficients using k-means (from scikit-learn).</p> <p>This method (with a fixed random seed) is used for plotting Slipmap solutions.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>int</code> <p>Number of clusters.</p> required <code>B</code> <code>Optional[ndarray]</code> <p>B matrix. Defaults to <code>self.get_B()</code>.</p> <code>None</code> <code>Z</code> <code>Optional[ndarray]</code> <p>Z matrix. Defaults to <code>self.get_Z()</code>.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>random_state for the KMeans clustering. Defaults to 42.</p> <code>42</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>sklearn.cluster.KMeans</code> or <code>sklearn.cluster.MiniBatchKMeans</code> if <code>self.n &gt;= 1024</code>.</p> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray</code> <p>Vector of cluster labels.</p> <code>centres</code> <code>ndarray</code> <p>Matrix of cluster centres.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def get_model_clusters(\n    self,\n    clusters: int,\n    B: Optional[np.ndarray] = None,\n    Z: Optional[np.ndarray] = None,\n    random_state: int = 42,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Cluster the local model coefficients using k-means (from scikit-learn).\n\n    This method (with a fixed random seed) is used for plotting Slipmap solutions.\n\n    Args:\n        clusters: Number of clusters.\n        B: B matrix. Defaults to `self.get_B()`.\n        Z: Z matrix. Defaults to `self.get_Z()`.\n        random_state: random_state for the KMeans clustering. Defaults to 42.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `sklearn.cluster.KMeans` or `sklearn.cluster.MiniBatchKMeans` if `self.n &gt;= 1024`.\n\n    Returns:\n        labels: Vector of cluster labels.\n        centres: Matrix of cluster centres.\n    \"\"\"\n    B = B if B is not None else self.get_B()\n    Z = Z if Z is not None else self.get_Z()\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", FutureWarning)\n        # Some sklearn versions warn about changing defaults for KMeans\n        kwargs.setdefault(\"random_state\", random_state)\n        if self.n &gt;= 1024:\n            km = MiniBatchKMeans(clusters, **kwargs).fit(B)\n        else:\n            km = KMeans(clusters, **kwargs).fit(B)\n    ord = np.argsort([Z[km.labels_ == k, 0].mean() for k in range(clusters)])\n    return np.argsort(ord)[km.labels_], km.cluster_centers_[ord]\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.plot","title":"<code>plot(title='', clusters=None, bars=True, jitter=0.0, show=True, bar=None, **kwargs)</code>","text":"<p>Plot the Slipmap solution using seaborn.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>clusters</code> <code>Union[None, int, ndarray]</code> <p>Can be None (plot individual losses), an int (plot k-means clusters of Bp), or an array of known cluster id:s. Defaults to None.</p> <code>None</code> <code>bars</code> <code>Union[bool, int, Sequence[str]]</code> <p>If <code>clusters is not None</code>, plot the local models in a bar plot. If <code>bar</code> is an int then only plot the most influential variables. Defaults to True.</p> <code>True</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.</p> <code>0.0</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <code>bar</code> <code>Union[None, bool, int]</code> <p>Alternative spelling for <code>bars</code>. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to plot_solution and <code>plt.subplots</code>.</p> <p>Returns:</p> Type Description <code>Optional[Figure]</code> <p>Matplotlib figure if <code>show=False</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def plot(\n    self,\n    title: str = \"\",\n    clusters: Union[None, int, np.ndarray] = None,\n    bars: Union[bool, int, Sequence[str]] = True,\n    jitter: Union[float, np.ndarray] = 0.0,\n    show: bool = True,\n    bar: Union[None, bool, int] = None,\n    **kwargs: Any,\n) -&gt; Optional[Figure]:\n    \"\"\"Plot the Slipmap solution using seaborn.\n\n    Args:\n        title: Title of the plot. Defaults to \"\".\n        clusters: Can be None (plot individual losses), an int (plot k-means clusters of Bp), or an array of known cluster id:s. Defaults to None.\n        bars: If `clusters is not None`, plot the local models in a bar plot. If ``bar`` is an int then only plot the most influential variables. Defaults to True.\n        jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n        show: Show the plot. Defaults to True.\n        bar: Alternative spelling for `bars`. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Additional arguments to [plot_solution][slisemap.plot.plot_solution] and `plt.subplots`.\n\n    Returns:\n        Matplotlib figure if `show=False`.\n    \"\"\"\n    if bar is not None:\n        bars = bar\n    B = self.get_B()\n    Z = self.get_Z()\n    if clusters is None:\n        loss = tonp(self.local_loss(self.predict(self._X, numpy=False), self._Y))\n        clusters = None\n        centers = None\n    else:\n        loss = None\n        if isinstance(clusters, int):\n            clusters, centers = self.get_model_clusters(clusters, B, Z)\n        else:\n            clusters = np.asarray(clusters)\n            centers = np.stack(\n                [np.mean(B[clusters == c, :], 0) for c in np.unique(clusters)], 0\n            )\n    fig = plot_solution(\n        Z=Z,\n        B=B,\n        loss=loss,\n        clusters=clusters,\n        centers=centers,\n        coefficients=self.metadata.get_coefficients(),\n        dimensions=self.metadata.get_dimensions(long=True),\n        title=title,\n        bars=bars,\n        jitter=jitter,\n        **kwargs,\n    )\n    plot_prototypes(self.get_Zp(), fig.axes[0])\n    if show:\n        plt.show()\n    else:\n        return fig\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.plot_position","title":"<code>plot_position(X=None, Y=None, index=None, title='', jitter=0.0, legend_inside=True, show=True, **kwargs)</code>","text":"<p>Plot local losses for alternative locations for the selected item(s).</p> <p>Indicate the selected item(s) either via <code>X</code> and <code>Y</code> or via <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ToTensor]</code> <p>Data matrix for the selected data item(s). Defaults to None.</p> <code>None</code> <code>Y</code> <code>Optional[ToTensor]</code> <p>Response matrix for the selected data item(s). Defaults to None.</p> <code>None</code> <code>index</code> <code>Union[None, int, Sequence[int]]</code> <p>Index/indices of the selected data item(s). Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.</p> <code>0.0</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside the grid (if there is an empty cell). Defaults to True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code>.</p> <p>Returns:</p> Type Description <code>Optional[FacetGrid]</code> <p><code>seaborn.FacetGrid</code> if <code>show=False</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def plot_position(\n    self,\n    X: Optional[ToTensor] = None,\n    Y: Optional[ToTensor] = None,\n    index: Union[None, int, Sequence[int]] = None,\n    title: str = \"\",\n    jitter: Union[float, np.ndarray] = 0.0,\n    legend_inside: bool = True,\n    show: bool = True,\n    **kwargs: Any,\n) -&gt; Optional[sns.FacetGrid]:\n    \"\"\"Plot local losses for alternative locations for the selected item(s).\n\n    Indicate the selected item(s) either via `X` and `Y` or via `index`.\n\n    Args:\n        X: Data matrix for the selected data item(s). Defaults to None.\n        Y: Response matrix for the selected data item(s). Defaults to None.\n        index: Index/indices of the selected data item(s). Defaults to None.\n        title: Title of the plot. Defaults to \"\".\n        jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n        legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n        show: Show the plot. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.relplot`.\n\n    Returns:\n        `seaborn.FacetGrid` if `show=False`.\n    \"\"\"\n    if index is None:\n        _assert(\n            X is not None and Y is not None,\n            \"Either index or X and Y must be given\",\n            Slipmap.plot_position,\n        )\n        L = self.get_L(X=X, Y=Y)\n    else:\n        if isinstance(index, int):\n            index = [index]\n        L = self.get_L()[:, index]\n    g = plot_position(\n        Z=self.get_Zp(),\n        L=L,\n        Zs=self.get_Z()[index, :] if index is not None else None,\n        dimensions=self.metadata.get_dimensions(long=True),\n        title=title,\n        jitter=jitter,\n        legend_inside=legend_inside,\n        marker_size=6.0,\n        **kwargs,\n    )\n    # plot_prototypes(self.get_Zp(), *g.axes.flat)\n    if show:\n        plt.show()\n    else:\n        return g\n</code></pre>"},{"location":"slisemap.slipmap/#slisemap.slipmap.Slipmap.plot_dist","title":"<code>plot_dist(title='', clusters=None, unscale=True, scatter=False, jitter=0.0, legend_inside=True, show=True, **kwargs)</code>","text":"<p>Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>clusters</code> <code>Union[None, int, ndarray]</code> <p>Number of cluster or vector of cluster labels. Defaults to None.</p> <code>None</code> <code>scatter</code> <code>bool</code> <p>Use scatterplots instead of density plots (clusters are ignored). Defaults to False.</p> <code>False</code> <code>unscale</code> <code>bool</code> <p>Unscale <code>X</code> and <code>Y</code> if scaling metadata has been given (see <code>Slisemap.metadata.set_scale_X</code>). Defaults to True.</p> <code>True</code> <code>jitter</code> <code>float</code> <p>Add jitter to the scatterplots. Defaults to 0.0.</p> <code>0.0</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside the grid (if there is an empty cell). Defaults to True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code> or <code>seaborn.scatterplot</code>.</p> <p>Returns:</p> Type Description <code>Optional[FacetGrid]</code> <p><code>seaborn.FacetGrid</code> if <code>show=False</code>.</p> Source code in <code>slisemap/slipmap.py</code> <pre><code>def plot_dist(\n    self,\n    title: str = \"\",\n    clusters: Union[None, int, np.ndarray] = None,\n    unscale: bool = True,\n    scatter: bool = False,\n    jitter: float = 0.0,\n    legend_inside: bool = True,\n    show: bool = True,\n    **kwargs: Any,\n) -&gt; Optional[sns.FacetGrid]:\n    \"\"\"Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.\n\n    Args:\n        title: Title of the plot. Defaults to \"\".\n        clusters: Number of cluster or vector of cluster labels. Defaults to None.\n        scatter: Use scatterplots instead of density plots (clusters are ignored). Defaults to False.\n        unscale: Unscale `X` and `Y` if scaling metadata has been given (see `Slisemap.metadata.set_scale_X`). Defaults to True.\n        jitter: Add jitter to the scatterplots. Defaults to 0.0.\n        legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n        show: Show the plot. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `seaborn.relplot` or `seaborn.scatterplot`.\n\n    Returns:\n        `seaborn.FacetGrid` if `show=False`.\n    \"\"\"\n    X = self.get_X(intercept=False)\n    Y = self.get_Y()\n    if unscale:\n        X = self.metadata.unscale_X(X)\n        Y = self.metadata.unscale_Y(Y)\n    loss = tonp(self.local_loss(self.predict(self._X, numpy=False), self._Y))\n    if isinstance(clusters, int):\n        clusters, _ = self.get_model_clusters(clusters)\n    g = plot_dist(\n        X=X,\n        Y=Y,\n        Z=self.get_Z(),\n        loss=loss,\n        variables=self.metadata.get_variables(False),\n        targets=self.metadata.get_targets(),\n        dimensions=self.metadata.get_dimensions(long=True),\n        title=title,\n        clusters=clusters,\n        scatter=scatter,\n        jitter=jitter,\n        legend_inside=legend_inside,\n        **kwargs,\n    )\n    if scatter:\n        plot_prototypes(self.get_Zp(), *g.axes.flat)\n    if show:\n        plt.show()\n    else:\n        return g\n</code></pre>"},{"location":"slisemap.slisemap/","title":"slisemap.slisemap","text":""},{"location":"slisemap.slisemap/#slisemap.slisemap","title":"<code>slisemap.slisemap</code>","text":"<p>Module that contains the <code>Slisemap</code> class.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap","title":"<code>Slisemap</code>","text":"<p>Slisemap: Combine local explanations with dimensionality reduction.</p> <p>This class contains the data and the parameters needed for finding a Slisemap solution. It also contains the solution (remember to optimise() first) in the form of an embedding matrix, see get_Z(), and a matrix of coefficients for the local model, see get_B(). Other methods of note are the various plotting methods, the save() method, and the fit_new() method.</p> <p>The use of some regularisation is highly recommended. Slisemap comes with built-in lasso/L1 and ridge/L2 regularisation (if these are used it is also a good idea to normalise the data in advance).</p> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>The number of data items (<code>X.shape[0]</code>).</p> <code>m</code> <code>int</code> <p>The number of variables (<code>X.shape[1]</code>).</p> <code>o</code> <code>int</code> <p>The number of targets (<code>Y.shape[1]</code>).</p> <code>d</code> <code>int</code> <p>The number of embedding dimensions (<code>Z.shape[1]</code>).</p> <code>q</code> <code>int</code> <p>The number of coefficients (<code>B.shape[1]</code>).</p> <code>intercept</code> <code>bool</code> <p>Has an intercept term been added to <code>X</code>.</p> <code>radius</code> <code>float</code> <p>The radius of the embedding.</p> <code>lasso</code> <code>float</code> <p>Lasso regularisation coefficient.</p> <code>ridge</code> <code>float</code> <p>Ridge regularisation coefficient.</p> <code>z_norm</code> <code>float</code> <p>Z normalisation regularisation coefficient.</p> <code>local_model</code> <code>CallableLike[predict]</code> <p>Local model prediction function (see slisemap.local_models).</p> <code>local_loss</code> <code>CallableLike[loss]</code> <p>Local model loss function (see slisemap.local_models).</p> <code>regularisation</code> <code>CallableLike[regularisation]</code> <p>Additional regularisation function.</p> <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Distance function.</p> <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel function.</p> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function for increased performance (see <code>torch.jit.trace</code> for caveats).</p> <code>metadata</code> <code>Metadata</code> <p>A dictionary for storing variable names and other metadata (see slisemap.utils.Metadata).</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>class Slisemap:\n    \"\"\"__Slisemap__: Combine local explanations with dimensionality reduction.\n\n    This class contains the data and the parameters needed for finding a Slisemap solution.\n    It also contains the solution (remember to [optimise()][slisemap.slisemap.Slisemap.optimise] first) in the form of an embedding matrix, see [get_Z()][slisemap.slisemap.Slisemap.get_Z], and a matrix of coefficients for the local model, see [get_B()][slisemap.slisemap.Slisemap.get_B].\n    Other methods of note are the various plotting methods, the [save()][slisemap.slisemap.Slisemap.save] method, and the [fit_new()][slisemap.slisemap.Slisemap.fit_new] method.\n\n    The use of some regularisation is highly recommended. Slisemap comes with built-in lasso/L1 and ridge/L2 regularisation (if these are used it is also a good idea to normalise the data in advance).\n\n    Attributes:\n        n: The number of data items (`X.shape[0]`).\n        m: The number of variables (`X.shape[1]`).\n        o: The number of targets (`Y.shape[1]`).\n        d: The number of embedding dimensions (`Z.shape[1]`).\n        q: The number of coefficients (`B.shape[1]`).\n        intercept: Has an intercept term been added to `X`.\n        radius: The radius of the embedding.\n        lasso: Lasso regularisation coefficient.\n        ridge: Ridge regularisation coefficient.\n        z_norm: Z normalisation regularisation coefficient.\n        local_model: Local model prediction function (see [slisemap.local_models][]).\n        local_loss: Local model loss function (see [slisemap.local_models][]).\n        regularisation: Additional regularisation function.\n        distance: Distance function.\n        kernel: Kernel function.\n        jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats).\n        metadata: A dictionary for storing variable names and other metadata (see [slisemap.utils.Metadata][]).\n    \"\"\"\n\n    # Make Python faster and safer by not creating a Slisemap.__dict__\n    __slots__ = (\n        \"_X\",\n        \"_Y\",\n        \"_Z\",\n        \"_B\",\n        \"_radius\",\n        \"_lasso\",\n        \"_ridge\",\n        \"_z_norm\",\n        \"_intercept\",\n        \"_local_model\",\n        \"_local_loss\",\n        \"_regularisation\",\n        \"_loss\",\n        \"_distance\",\n        \"_kernel\",\n        \"_jit\",\n        \"metadata\",\n        \"_Z0\",  # deprecated\n        \"_B0\",  # deprecated\n        \"_random_state\",  # deprecated\n    )\n\n    def __init__(\n        self,\n        X: ToTensor,\n        y: ToTensor,\n        radius: float = 3.5,\n        d: int = 2,\n        lasso: Optional[float] = None,\n        ridge: Optional[float] = None,\n        z_norm: float = 0.01,\n        intercept: bool = True,\n        local_model: Union[\n            LocalModelCollection, CallableLike[ALocalModel.predict]\n        ] = LinearRegression,\n        local_loss: Optional[CallableLike[ALocalModel.loss]] = None,\n        coefficients: Union[None, int, CallableLike[ALocalModel.coefficients]] = None,\n        regularisation: Union[None, CallableLike[ALocalModel.regularisation]] = None,\n        distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = torch.cdist,\n        kernel: Callable[[torch.Tensor], torch.Tensor] = softmax_row_kernel,\n        B0: Optional[ToTensor] = None,\n        Z0: Optional[ToTensor] = None,\n        jit: bool = True,\n        random_state: Optional[int] = None,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n        cuda: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Create a Slisemap object.\n\n        Args:\n            X: Data matrix.\n            y: Target vector or matrix.\n            radius: The radius of the embedding Z. Defaults to 3.5.\n            d: The number of embedding dimensions. Defaults to 2.\n            lasso: Lasso regularisation coefficient. Defaults to 0.0.\n            ridge: Ridge regularisation coefficient. Defaults to 0.0.\n            z_norm: Z normalisation regularisation coefficient. Defaults to 0.01.\n            intercept: Should an intercept term be added to `X`. Defaults to True.\n            local_model: Local model prediction function (see [slisemap.local_models.identify_local_model][]). Defaults to [LinearRegression][slisemap.local_models.LinearRegression].\n            local_loss: Local model loss function (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            coefficients: The number of local model coefficients (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            regularisation: Additional regularisation method (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n            distance: Distance function. Defaults to `torch.cdist` (Euclidean distance).\n            kernel: Kernel function. Defaults to [softmax_row_kernel][slisemap.utils.softmax_row_kernel].\n            B0: Initial value for B (random if None). Defaults to None.\n            Z0: Initial value for Z (PCA if None). Defaults to None.\n            jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats). Defaults to True.\n            random_state: Set an explicit seed for the random number generator (i.e. `torch.manual_seed`). Defaults to None.\n            dtype: Floating type. Defaults to `torch.float32`.\n            device: Torch device. Defaults to None.\n            cuda: Use cuda if available. Defaults to True, if the data is large enough.\n\n        Deprecated:\n            1.6: Use `device` instead of `cuda` to force a specific device.\n            1.6: The `random_state` has been moved to the escape function.\n        \"\"\"\n        for s in Slisemap.__slots__:\n            # Initialise all attributes (to avoid attribute errors)\n            setattr(self, s, None)\n        if lasso is None and ridge is None:\n            _warn(\n                \"Consider using regularisation!\\n\"\n                + \"Regularisation is important for handling small neighbourhoods, and also makes the local models more local. \"\n                + \"Lasso (l1) and ridge (l2) regularisation is built-in, via the parameters `lasso` and `ridge`. \"\n                + \"Set `lasso=0` to disable this warning (if no regularisation is really desired).\",\n                Slisemap,\n            )\n        local_model, local_loss, coefficients, regularisation = identify_local_model(\n            local_model, local_loss, coefficients, regularisation\n        )\n        self.lasso = 0.0 if lasso is None else lasso\n        self.ridge = 0.0 if ridge is None else ridge\n        self.kernel = kernel\n        self.distance = distance\n        self.local_model = local_model\n        self.local_loss = local_loss\n        self.regularisation = regularisation\n        self.z_norm = z_norm\n        self.radius = radius\n        self._intercept = intercept\n        self._jit = jit\n        self.metadata: Metadata = Metadata(self)\n\n        if cuda is not None:\n            _deprecated(\"cuda\", \"device\")\n        if device is None:\n            if cuda is None and isinstance(X, torch.Tensor):\n                device = X.device\n            elif cuda is True:\n                device = torch.device(\"cuda\")\n        tensorargs = {\"device\": device, \"dtype\": dtype}\n\n        self._X, X_rows, X_columns = to_tensor(X, **tensorargs)\n        if intercept:\n            self._X = torch.cat((self._X, torch.ones_like(self._X[:, :1])), 1)\n        n, m = self._X.shape\n        self.metadata.set_variables(X_columns, intercept)\n\n        self._Y, Y_rows, Y_columns = to_tensor(y, **tensorargs)\n        self.metadata.set_targets(Y_columns)\n        if len(self._Y.shape) == 1:\n            self._Y = self._Y[:, None]\n        _assert_shape(self._Y, (n, self._Y.shape[1]), \"Y\", Slisemap)\n\n        if random_state is not None:\n            self.random_state = random_state\n\n        if Z0 is None:\n            self._Z0 = self._X @ PCA_rotation(self._X, d)\n            if self._Z0.shape[1] &lt; d:\n                _warn(\n                    \"The number of embedding dimensions is larger than the number of data dimensions\",\n                    Slisemap,\n                )\n                Z0fill = torch.zeros(size=[n, d - self._Z0.shape[1]], **tensorargs)\n                self._Z0 = torch.cat((self._Z0, Z0fill), 1)\n            Z_rows = None\n        else:\n            self._Z0, Z_rows, Z_columns = to_tensor(Z0, **tensorargs)\n            self.metadata.set_dimensions(Z_columns)\n            _assert_shape(self._Z0, (n, d), \"Z0\", Slisemap)\n        if radius &gt; 0:\n            norm = 1 / (torch.sqrt(torch.sum(self._Z0**2) / self._Z0.shape[0]) + 1e-8)\n            self._Z0 = self._Z0 * norm\n        self._Z = self._Z0.detach().clone()\n\n        if callable(coefficients):\n            coefficients = coefficients(self._X, self._Y)\n        if B0 is None:\n            B0 = global_model(\n                X=self._X,\n                Y=self._Y,\n                local_model=self.local_model,\n                local_loss=self.local_loss,\n                coefficients=coefficients,\n                lasso=self.lasso,\n                ridge=self.ridge,\n            ).detach()\n            if not torch.all(torch.isfinite(B0)):\n                _warn(\n                    \"Optimising a global model as initialisation resulted in non-finite values. Consider using stronger regularisation (increase `lasso` or `ridge`).\",\n                    Slisemap,\n                )\n                B0 = torch.zeros_like(B0)\n            self._B0 = B0.expand((n, coefficients))\n            B_rows = None\n        else:\n            self._B0, B_rows, B_columns = to_tensor(B0, **tensorargs)\n            if self._B0.shape[0] == 1:\n                self._B0 = self._B0.expand((self.n, coefficients))\n            _assert_shape(self._B0, (n, coefficients), \"B0\", Slisemap)\n            self.metadata.set_coefficients(B_columns)\n        self._B = self._B0.clone()\n        self.metadata.set_rows(X_rows, Y_rows, B_rows, Z_rows)\n\n        if (\n            device is None\n            and self.n**2 * self.m * self.o &gt; 1_000_000\n            and torch.cuda.is_available()\n        ):\n            self.cuda()\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"The number of data items.\"\"\"\n        return self._X.shape[0]\n\n    @property\n    def m(self) -&gt; int:\n        \"\"\"The number of variables (including potential intercept).\"\"\"\n        return self._X.shape[1]\n\n    @property\n    def o(self) -&gt; int:\n        \"\"\"The number of target variables (i.e. the number of classes).\"\"\"\n        return self._Y.shape[-1]\n\n    @property\n    def d(self) -&gt; int:\n        \"\"\"The number of embedding dimensions.\"\"\"\n        return self._Z.shape[1]\n\n    @d.setter\n    def d(self, value: int) -&gt; None:\n        # Deprecated since 1.6.\n        _assert(\n            value &gt; 0, \"The number of embedding dimensions must be positive\", Slisemap.d\n        )\n        _deprecated(\"Set `Slisemap.d`\", \"Create a new Slisemap instead\")\n        if self.d != value:\n            self._loss = None  # invalidate cached loss function\n            self._Z = self._Z.detach()\n            if self.d &gt; value:\n                self._Z = self._Z @ PCA_rotation(self._Z, value)\n            else:\n                zn = [self._Z, torch.zeros((self.n, value - self.d), **self.tensorargs)]\n                self._Z = torch.concat(zn, 1)\n\n    @property\n    def q(self) -&gt; int:\n        \"\"\"The number of local model coefficients.\"\"\"\n        return self._B.shape[1]\n\n    @property\n    def intercept(self) -&gt; bool:\n        \"\"\"Is an intercept column added to the data?.\"\"\"\n        return self._intercept\n\n    @property\n    def radius(self) -&gt; float:\n        \"\"\"The radius of the embedding.\"\"\"\n        return self._radius\n\n    @radius.setter\n    def radius(self, value: float) -&gt; None:\n        if self._radius != value:\n            _assert(value &gt;= 0, \"radius must not be negative\", Slisemap.radius)\n            self._radius = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def lasso(self) -&gt; float:\n        \"\"\"Lasso regularisation strength.\"\"\"\n        return self._lasso\n\n    @lasso.setter\n    def lasso(self, value: float) -&gt; None:\n        if self._lasso != value:\n            _assert(value &gt;= 0, \"lasso must not be negative\", Slisemap.lasso)\n            self._lasso = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def ridge(self) -&gt; float:\n        \"\"\"Ridge regularisation strength.\"\"\"\n        return self._ridge\n\n    @ridge.setter\n    def ridge(self, value: float) -&gt; None:\n        if self._ridge != value:\n            _assert(value &gt;= 0, \"ridge must not be negative\", Slisemap.ridge)\n            self._ridge = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def z_norm(self) -&gt; float:\n        \"\"\"Z normalisation regularisation strength.\"\"\"\n        return self._z_norm\n\n    @z_norm.setter\n    def z_norm(self, value: float) -&gt; None:\n        if self._z_norm != value:\n            _assert(value &gt;= 0, \"z_norm must not be negative\", Slisemap.z_norm)\n            self._z_norm = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def local_model(self) -&gt; CallableLike[ALocalModel.predict]:\n        \"\"\"Local model prediction function. Takes in X[n, m] and B[n, q], and returns Ytilde[n, n, o].\"\"\"\n        return self._local_model\n\n    @local_model.setter\n    def local_model(self, value: CallableLike[ALocalModel.predict]) -&gt; None:\n        if self._local_model != value:\n            _assert(\n                callable(value), \"local_model must be callable\", Slisemap.local_model\n            )\n            self._local_model = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def local_loss(self) -&gt; CallableLike[ALocalModel.loss]:\n        \"\"\"Local model loss function. Takes in Ytilde[n, n, o] and Y[n, o] and returns L[n, n].\"\"\"\n        return self._local_loss\n\n    @local_loss.setter\n    def local_loss(self, value: CallableLike[ALocalModel.loss]) -&gt; None:\n        if self._local_loss != value:\n            _assert(callable(value), \"local_loss must be callable\", Slisemap.local_loss)\n            self._local_loss = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def regularisation(self) -&gt; CallableLike[ALocalModel.regularisation]:\n        \"\"\"Regularisation function. Takes in X, Y, Bp, Z, and Ytilde and returns an additional loss scalar.\"\"\"\n        return self._regularisation\n\n    @regularisation.setter\n    def regularisation(self, value: CallableLike[ALocalModel.regularisation]) -&gt; None:\n        if self._regularisation != value:\n            _assert(\n                callable(value),\n                \"regularisation function must be callable\",\n                Slisemap.regularisation,\n            )\n            self._regularisation = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def distance(self) -&gt; Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:\n        \"\"\"Distance function. Takes in Z[n1, d] and Z[n2, d], and returns D[n1, n2].\"\"\"\n        return self._distance\n\n    @distance.setter\n    def distance(\n        self, value: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n    ) -&gt; None:\n        if self._distance != value:\n            _assert(callable(value), \"distance must be callable\", Slisemap.distance)\n            self._distance = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def kernel(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        \"\"\"Kernel function. Takes in D[n, n] and returns W[n, n].\"\"\"\n        return self._kernel\n\n    @kernel.setter\n    def kernel(self, value: Callable[[torch.Tensor], torch.Tensor]) -&gt; None:\n        if self._kernel != value:\n            _assert(callable(value), \"kernel must be callable\", Slisemap.kernel)\n            self._kernel = value\n            self._loss = None  # invalidate cached loss function\n\n    @property\n    def jit(self) -&gt; bool:\n        \"\"\"Just-In-Time compile the loss function?.\"\"\"\n        return self._jit\n\n    @jit.setter\n    def jit(self, value: bool) -&gt; None:\n        if self._jit != value:\n            self._jit = value\n            self._loss = None  # invalidate cached loss function\n\n    def random_state(self, value: Optional[int]) -&gt; None:\n        \"\"\"Set the seed for the random number generator specific for this object (None reverts to the global `torch` PRNG).\n\n        Deprecated:\n            1.6: Use `Slisemap.escape(random_state=...)` instead.\n        \"\"\"\n        _deprecated(Slisemap.random_state, \"Slisemap.escape(random_state=...)\")\n        if value is None:\n            self._random_state = None\n        else:\n            if self._X.device.type == \"cpu\":\n                self._random_state = torch.random.manual_seed(value)\n            elif self._X.device.type == \"cuda\":\n                gen = torch.cuda.default_generators[self._X.device.index]\n                self._random_state = gen.manual_seed(value)\n            else:\n                _warn(\n                    Slisemap.random_state,\n                    \"Unknown device, setting the global seed insted\",\n                )\n                torch.random.manual_seed(value)\n                self._random_state = None\n\n    random_state = property(fset=random_state, doc=random_state.__doc__)\n\n    @property\n    def tensorargs(self) -&gt; Dict[str, Any]:\n        \"\"\"When creating a new `torch.Tensor` add these keyword arguments to match the `dtype` and `device` of this Slisemap object.\"\"\"\n        return {\"device\": self._X.device, \"dtype\": self._X.dtype}\n\n    def cuda(self, **kwargs: Any) -&gt; None:\n        \"\"\"Move the tensors to CUDA memory (and run the calculations there).\n\n        Note that this resets the random state.\n\n        Keyword Args:\n            **kwargs: Optional arguments to `torch.Tensor.cuda`\n        \"\"\"\n        X = self._X.cuda(**kwargs)\n        self._X = X\n        self._Y = self._Y.cuda(**kwargs)\n        self._Z = self._Z.detach().cuda(**kwargs)\n        self._B = self._B.detach().cuda(**kwargs)\n        self._loss = None  # invalidate cached loss function\n\n    def cpu(self, **kwargs: Any) -&gt; None:\n        \"\"\"Move the tensors to CPU memory (and run the calculations there).\n\n        Note that this resets the random state.\n\n        Keyword Args:\n            **kwargs: Optional arguments to `torch.Tensor.cpu`\n        \"\"\"\n        X = self._X.cpu(**kwargs)\n        self._X = X\n        self._Y = self._Y.cpu(**kwargs)\n        self._Z = self._Z.detach().cpu(**kwargs)\n        self._B = self._B.detach().cpu(**kwargs)\n        self._loss = None  # invalidate cached loss function\n\n    def _get_loss_fn(\n        self, individual: bool = False\n    ) -&gt; Callable[\n        [torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor\n    ]:\n        \"\"\"Return the Slisemap loss function.\n\n        This function JITs and caches the loss function for efficiency.\n\n        Args:\n            individual: Make a loss function for individual losses. Defaults to False.\n\n        Returns:\n            Loss function: `f(X, Y, B, Z) -&gt; loss`.\n        \"\"\"\n        if individual:\n            return make_loss(\n                local_model=self.local_model,\n                local_loss=self.local_loss,\n                distance=self.distance,\n                kernel=self.kernel,\n                radius=self.radius,\n                lasso=self.lasso,\n                ridge=self.ridge,\n                z_norm=0.0,\n                individual=True,\n            )\n        if self._loss is None:\n            # Caching the loss function\n            self._loss = make_loss(\n                local_model=self.local_model,\n                local_loss=self.local_loss,\n                distance=self.distance,\n                kernel=self.kernel,\n                radius=self.radius,\n                lasso=self.lasso,\n                ridge=self.ridge,\n                z_norm=self.z_norm,\n                regularisation=self.regularisation,\n            )\n            # JITting the loss function improves the performance\n            if self._jit:\n                self._loss = torch.jit.trace(\n                    self._loss, (self._X[:1], self._Y[:1], self._B[:1], self._Z[:1])\n                )\n        return self._loss\n\n    def _as_new_X(self, X: Optional[ToTensor] = None) -&gt; torch.Tensor:\n        if X is None:\n            return self._X\n        X = torch.atleast_2d(to_tensor(X, **self.tensorargs)[0])\n        if self._intercept and X.shape[1] == self.m - 1:\n            X = torch.cat((X, torch.ones_like(X[:, :1])), 1)\n        _assert_shape(X, (X.shape[0], self.m), \"X\", Slisemap._as_new_X)\n        return X\n\n    def _as_new_Y(self, Y: Optional[ToTensor] = None, n: int = -1) -&gt; torch.Tensor:\n        if Y is None:\n            return self._Y\n        Y = to_tensor(Y, **self.tensorargs)[0]\n        if len(Y.shape) &lt; 2:\n            Y = torch.reshape(Y, (n, self.o))\n        _assert_shape(Y, (n if n &gt; 0 else Y.shape[0], self.o), \"Y\", Slisemap._as_new_Y)\n        return Y\n\n    def get_Z(\n        self, scale: bool = True, rotate: bool = False, numpy: bool = True\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the Z matrix.\n\n        Args:\n            scale: Scale the returned `Z` to match self.radius. Defaults to True.\n            rotate: Rotate the returned `Z` so that the first dimension is the major axis. Defaults to False.\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n           The `Z` matrix.\n        \"\"\"\n        self._normalise()\n        Z = self._Z * self.radius if scale and self.radius &gt; 0 else self._Z\n        if rotate:\n            Z = Z @ PCA_rotation(Z, center=False)\n        return tonp(Z) if numpy else Z\n\n    def get_B(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the B matrix.\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n           The `B` matrix.\n        \"\"\"\n        return tonp(self._B) if numpy else self._B\n\n    def get_D(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the embedding distance matrix.\n\n        Args:\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n           The `D` matrix.\n        \"\"\"\n        Z = self.get_Z(rotate=False, scale=True, numpy=False)\n        D = self._distance(Z, Z)\n        return tonp(D) if numpy else D\n\n    def get_L(\n        self,\n        X: Optional[ToTensor] = None,\n        Y: Optional[ToTensor] = None,\n        numpy: bool = True,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the loss matrix: [B.shape[0], X.shape[0]].\n\n        Args:\n            X: Optional replacement for the training X. Defaults to None.\n            Y: Optional replacement for the training Y. Defaults to None.\n            numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n        Returns:\n           The `L` matrix.\n        \"\"\"\n        X = self._as_new_X(X)\n        Y = self._as_new_Y(Y, X.shape[0])\n        L = self.local_loss(self.local_model(X, self._B), Y)\n        return tonp(L) if numpy else L\n\n    def get_W(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the weight matrix.\n\n        Args:\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n        Returns:\n           The `W` matrix.\n        \"\"\"\n        W = self.kernel(self.get_D(numpy=False))\n        return tonp(W) if numpy else W\n\n    def get_X(\n        self, numpy: bool = True, intercept: bool = True\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the data matrix.\n\n        Args:\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n            intercept: Include the intercept column (if `self.intercept == True`). Defaults to True.\n\n        Returns:\n           The `X` matrix.\n        \"\"\"\n        X = self._X if intercept or not self._intercept else self._X[:, :-1]\n        return tonp(X) if numpy else X\n\n    def get_Y(\n        self, numpy: bool = True, ravel: bool = False\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"Get the target matrix.\n\n        Args:\n            numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n            ravel: Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.\n\n        Returns:\n           The `Y` matrix.\n        \"\"\"\n        Y = self._Y.ravel() if ravel else self._Y\n        return tonp(Y) if numpy else Y\n\n    def value(\n        self, individual: bool = False, numpy: bool = True\n    ) -&gt; Union[float, np.ndarray, torch.Tensor]:\n        \"\"\"Calculate the loss value.\n\n        Args:\n            individual: Give loss individual loss values for the data points. Defaults to False.\n            numpy: Return the loss as a numpy.ndarray or float instead of a torch.Tensor. Defaults to True.\n\n        Returns:\n            The loss value(s).\n        \"\"\"\n        loss = self._get_loss_fn(individual)\n        loss = loss(X=self._X, Y=self._Y, B=self._B, Z=self._Z)\n        if individual:\n            return tonp(loss) if numpy else loss\n        else:\n            return loss.cpu().item() if numpy else loss\n\n    def entropy(\n        self, aggregate: bool = True, numpy: bool = True\n    ) -&gt; Union[float, np.ndarray, torch.Tensor]:\n        \"\"\"Compute row-wise entropy of the `W` matrix induced by `Z`. **DEPRECATED**.\n\n        Args:\n            aggregate: Aggregate the row-wise entropies into one scalar. Defaults to True.\n            numpy: Return a `numpy.ndarray` or `float` instead of a `torch.Tensor`. Defaults to True.\n\n        Returns:\n            The entropy.\n\n        Deprecated:\n            1.4: Use [slisemap.metrics.entropy][slisemap.metrics.entropy] instead.\n        \"\"\"\n        _deprecated(Slisemap.entropy, \"slisemap.metrics.entropy\")\n        from slisemap.metrics import entropy\n\n        return entropy(self, aggregate, numpy)\n\n    def lbfgs(\n        self,\n        max_iter: int = 500,\n        verbose: bool = False,\n        *,\n        only_B: bool = False,\n        **kwargs: Any,\n    ) -&gt; float:\n        \"\"\"Optimise Slisemap using LBFGS.\n\n        Args:\n            max_iter: Maximum number of LBFGS iterations. Defaults to 500.\n            verbose: Print status messages. Defaults to False.\n\n        Keyword Args:\n            only_B: Only optimise B. Defaults to False.\n            **kwargs: Optional keyword arguments to LBFGS.\n\n        Returns:\n            The loss value.\n        \"\"\"\n        Z = self._Z.detach().clone().requires_grad_(True)\n        B = self._B.detach().clone().requires_grad_(True)\n\n        loss_ = self._get_loss_fn()\n        loss_fn = lambda: loss_(self._X, self._Y, B, Z)  # noqa: E731\n        pre_loss = loss_fn().cpu().detach().item()\n        LBFGS(\n            loss_fn,\n            [B] if only_B else [Z, B],\n            max_iter=max_iter,\n            verbose=verbose,\n            **kwargs,\n        )\n        post_loss = loss_fn().cpu().detach().item()\n\n        if np.isnan(post_loss):\n            _warn(\n                \"An LBFGS optimisation resulted in `nan` (try strengthening the regularisation or reducing the radius)\",\n                Slisemap.lbfgs,\n            )\n            # Some datasets (with logistic local models) are initially numerically unstable.\n            # Just running LBFGS for one iteration seems to avoid those issues.\n            Z = self._Z.detach().requires_grad_(True)\n            B = self._B.detach().requires_grad_(True)\n            LBFGS(\n                loss_fn,\n                [B] if only_B else [Z, B],\n                max_iter=1,\n                verbose=verbose,\n                **kwargs,\n            )\n            post_loss = loss_fn().cpu().detach().item()\n        if post_loss &lt; pre_loss:\n            self._Z = Z.detach()\n            self._B = B.detach()\n            self._normalise()\n            return post_loss\n        else:\n            if verbose:\n                print(\"Slisemap.lbfgs: No improvement found\")\n            return pre_loss\n\n    def escape(\n        self,\n        force_move: bool = True,\n        escape_fn: Callable = escape_neighbourhood,\n        lerp: float = 0.95,\n        noise: float = 0.0,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"Try to escape a local optimum by moving the items (embedding and local model) to the neighbourhoods best suited for them.\n\n        This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.\n\n        Args:\n            force_move: Do not allow the items to pair with themselves. Defaults to True.\n            escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n            lerp: Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.95.\n            noise: Scale of the noise added to the embedding matrix if it looses rank after an escape (recommended for gradient based optimisers). Defaults to 1e-4.\n            random_state: Seed for the random generator if `noise &gt; 0.0`. Defaults to 42.\n        \"\"\"\n        if lerp &lt;= 0.0:\n            _warn(\"Escaping with `lerp &lt;= 0` does nothing!\", Slisemap.escape)\n            return\n        B, Z = escape_fn(\n            X=self._X,\n            Y=self._Y,\n            B=self._B,\n            Z=self._Z,\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            distance=self.distance,\n            kernel=self.kernel,\n            radius=self.radius,\n            force_move=force_move,\n            jit=self.jit,\n        )\n        if lerp &gt;= 1.0:\n            self._B, self._Z = B, Z\n        else:\n            self._B = (1.0 - lerp) * self._B + lerp * B\n            self._Z = (1.0 - lerp) * self._Z + lerp * Z\n        if noise &gt; 0.0:\n            rank = torch.linalg.matrix_rank(self._Z - torch.mean(self._Z, 0, True))\n            if rank.item() &lt; min(*self._Z.shape):\n                generator = torch.Generator(self._Z.device).manual_seed(random_state)\n                self._Z = torch.normal(self._Z, noise, generator=generator)\n        self._normalise()\n\n    def _normalise(self) -&gt; None:\n        \"\"\"Normalise Z.\"\"\"\n        if self.radius &gt; 0:\n            scale = torch.sqrt(torch.sum(self._Z**2) / self.n)\n            if not torch.allclose(scale, torch.ones_like(scale)):\n                self._Z *= 1 / (scale + 1e-8)\n\n    def optimise(\n        self,\n        patience: int = 2,\n        max_escapes: int = 100,\n        max_iter: int = 500,\n        verbose: Literal[0, 1, 2] = 0,\n        only_B: bool = False,\n        escape_kws: Dict[str, object] = {},\n        *,\n        escape_fn: Optional[CallableLike[escape_neighbourhood]] = None,\n        noise: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; float:\n        \"\"\"Optimise Slisemap by alternating between [self.lbfgs()][slisemap.slisemap.Slisemap.lbfgs] and [self.escape()][slisemap.slisemap.Slisemap.escape] until convergence.\n\n        Statistics for the optimisation can be found in `self.metadata[\"optimize_time\"]` and `self.metadata[\"optimize_loss\"]`.\n\n        Args:\n            patience: Number of escapes without improvement before stopping. Defaults to 2.\n            max_escapes: Maximum numbers optimisation rounds. Defaults to 100.\n            max_iter: Maximum number of LBFGS iterations per round. Defaults to 500.\n            verbose: Print status messages (0: no, 1: some, 2: all). Defaults to 0.\n            only_B: Only optimise the local models, not the embedding. Defaults to False.\n            escape_kws: Optional keyword arguments to [self.escape()][slisemap.slisemap.Slisemap.escape]. Defaults to {}.\n\n        Keyword Args:\n            escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n            noise: Scale of the noise added to the embedding matrix if it looses rank after an escape.\n            **kwargs: Optional keyword arguments to Slisemap.lbfgs.\n\n        Returns:\n            The loss value.\n\n        Deprecated:\n            1.6: The `noise` argument, use `escape_kws={\"noise\": noise}` instead.\n            1.6: The `escape_fn` argument, use `escape_kws={\"escape_fn\": escape_fn}` instead.\n        \"\"\"\n        if noise is not None:\n            _deprecated(\n                \"Slisemap.optimise(noise=noise, ...)\",\n                'Slisemap.optimise(escape_kws={\"noise\":noise}, ...)',\n            )\n            escape_kws.setdefault(\"noise\", noise)\n        if escape_fn is not None:\n            _deprecated(\n                \"Slisemap.optimise(escape_fn=escape_fn, ...)\",\n                'Slisemap.optimise(escape_kws={\"escape_fn\":escape_fn}, ...)',\n            )\n            escape_kws.setdefault(\"escape_fn\", escape_fn)\n        loss = np.repeat(np.inf, 2)\n        time = timer()\n        loss[0] = self.lbfgs(\n            max_iter=max_iter,\n            only_B=True,\n            increase_tolerance=not only_B,\n            verbose=verbose &gt; 1,\n            **kwargs,\n        )\n        history = [loss[0]]\n        if verbose:\n            i = 0\n            print(f\"Slisemap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n        if only_B:\n            self.metadata[\"optimize_time\"] = timer() - time\n            self.metadata[\"optimize_loss\"] = history\n            return loss[0]\n        cc = CheckConvergence(patience, max_escapes)\n        while not cc.has_converged(loss, self.copy, verbose=verbose &gt; 1):\n            self.escape(random_state=cc.iter, **escape_kws)\n            loss[1] = self.value()\n            if verbose:\n                print(f\"Slisemap.optimise Escape {i:2d}: {loss[1]:.2f}\")\n            loss[0] = self.lbfgs(\n                max_iter=max_iter,\n                increase_tolerance=True,\n                verbose=verbose &gt; 1,\n                **kwargs,\n            )\n            history.append(loss[1])\n            history.append(loss[0])\n            if verbose:\n                i += 1\n                print(f\"Slisemap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n        self._Z = cc.optimal._Z\n        self._B = cc.optimal._B\n        loss = self.lbfgs(\n            max_iter=max_iter * 2,\n            increase_tolerance=False,\n            verbose=verbose &gt; 1,\n            **kwargs,\n        )\n        history.append(loss)\n        self.metadata[\"optimize_time\"] = timer() - time\n        self.metadata[\"optimize_loss\"] = history\n        if verbose:\n            print(f\"Slisemap.optimise Final    : {loss:.2f}\")\n        return loss\n\n    optimize = optimise\n\n    def fit_new(\n        self,\n        Xnew: ToTensor,\n        ynew: ToTensor,\n        optimise: bool = True,\n        between: bool = True,\n        escape_fn: Callable = escape_neighbourhood,\n        loss: bool = False,\n        verbose: bool = False,\n        numpy: bool = True,\n        **kwargs: Any,\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]],\n        Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]],\n    ]:\n        \"\"\"Generate embedding(s) and model(s) for new data item(s).\n\n        This works as follows:\n            1. Find good initial embedding(s) and local model(s) using the escape_fn.\n            2. Optionally finetune the embedding(s) and model(s) using LBFG.\n\n        Args:\n            Xnew: New data point(s).\n            ynew: New target(s).\n            optimise: Should the embedding and model be optimised (after finding the neighbourhood). Defaults to True.\n            between: If `optimise=True`, should the new points affect each other? Defaults to True.\n            escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n            loss: Return a vector of individual losses for the new items. Defaults to False.\n            verbose: Print status messages. Defaults to False.\n            numpy: Return the results as numpy (True) or pytorch (False) matrices. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Optional keyword arguments to LBFGS.\n\n        Returns:\n            Bnew: Local model coefficients for the new data.\n            Znew: Embedding(s) for the new data.\n            loss: Individual losses if `loss=True`.\n        \"\"\"\n        Xnew = self._as_new_X(Xnew)\n        n = Xnew.shape[0]\n        ynew = self._as_new_Y(ynew, n)\n        if verbose:\n            print(\"Escaping the new data\")\n        Bnew, Znew = escape_fn(\n            X=Xnew,\n            Y=ynew,\n            B=self._B,\n            Z=self._Z,\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            distance=self.distance,\n            kernel=self.kernel,\n            radius=self.radius,\n            force_move=False,\n            Xold=self._X,\n            Yold=self._Y,\n            jit=self.jit,\n        )\n        if verbose:\n            Zrad = torch.sqrt(torch.sum(Znew**2) / n).cpu().detach().item()\n            print(\"  radius(Z_new) =\", Zrad)\n\n        if optimise:\n            if verbose:\n                print(\"Optimising the new data\")\n            lf, set_new = make_marginal_loss(\n                X=self._X,\n                Y=self._Y,\n                B=self._B,\n                Z=self._Z,\n                Xnew=Xnew if between else Xnew[:1],\n                Ynew=ynew if between else ynew[:1],\n                local_model=self.local_model,\n                local_loss=self.local_loss,\n                distance=self.distance,\n                kernel=self.kernel,\n                radius=self.radius,\n                lasso=self.lasso,\n                ridge=self.ridge,\n                jit=self.jit,\n            )\n            if between:\n                Bnew = Bnew.detach().requires_grad_(True)\n                Znew = Znew.detach().requires_grad_(True)\n                LBFGS(lambda: lf(Bnew, Znew), [Bnew, Znew], **kwargs)\n            else:\n                for j in range(n):\n                    set_new(Xnew[None, j], ynew[None, j])\n                    Bi = Bnew[None, j].detach().clone().requires_grad_(True)\n                    Zi = Znew[None, j].detach().clone().requires_grad_(True)\n                    LBFGS(lambda: lf(Bi, Zi), [Bi, Zi], **kwargs)  # noqa: B023\n                    Bnew[j] = Bi.detach()\n                    Znew[j] = Zi.detach()\n            if verbose:\n                Zrad = torch.sqrt(torch.sum(Znew**2) / n).cpu().detach().item()\n                print(\"  radius(Z_new) =\", Zrad)\n        if self.radius &gt; 0:\n            if verbose:\n                print(\"Normalising the solution\")\n            norm = self.radius / (torch.sqrt(torch.sum(self._Z**2) / self.n) + 1e-8)\n            Zout = Znew * norm\n            if verbose:\n                Zrad = torch.sqrt(torch.sum(Zout**2) / n).cpu().detach().item()\n                print(\"  radius(Z_new) =\", Zrad)\n        else:\n            Zout = Znew\n        if loss:\n            if verbose:\n                print(\"Calculating individual losses\")\n            lf = self._get_loss_fn(individual=True)\n            if between:\n                loss = lf(\n                    X=torch.cat((self._X, Xnew), 0),\n                    Y=torch.cat((self._Y, ynew), 0),\n                    B=torch.cat((self._B, Bnew), 0),\n                    Z=torch.cat((self._Z, Znew), 0),\n                )[self.n :]\n            else:\n                if self._jit:\n                    lf = torch.jit.trace(lf, (Xnew[:1], ynew[:1], Bnew[:1], Znew[:1]))\n                loss = torch.zeros(n, **self.tensorargs)\n                for j in range(n):\n                    loss[j] = lf(\n                        X=torch.cat((self._X, Xnew[None, j]), 0),\n                        Y=torch.cat((self._Y, ynew[None, j]), 0),\n                        B=torch.cat((self._B, Bnew[None, j]), 0),\n                        Z=torch.cat((self._Z, Znew[None, j]), 0),\n                    )[-1]\n            if verbose:\n                print(\"  mean(loss) =\", loss.detach().mean().cpu().item())\n            return (tonp(Bnew), tonp(Zout), tonp(loss)) if numpy else (Bnew, Zout, loss)\n        else:\n            return (tonp(Bnew), tonp(Zout)) if numpy else (Bnew, Zout)\n\n    def predict(  # noqa: D417\n        self,\n        X: Optional[ToTensor] = None,\n        B: Optional[ToTensor] = None,\n        Z: Optional[ToTensor] = None,\n        numpy: bool = True,\n        *,\n        Xnew: Optional[ToTensor] = None,\n        Znew: Optional[ToTensor] = None,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"Predict new outcomes when the data and embedding or local model is known.\n\n        If the local models `B` are known they are used.\n        If the embeddings `Z` are known they are used to find new local models.\n        Ohterwise the closest training X gives the `B`.\n\n        Args:\n            X: Data matrix (set to None to use the training data). Defaults to None.\n            B: Coefficient matrix. Defaults to None.\n            Z: Embedding matrix. Defaults to None.\n            numpy: Return the result as a numpy (True) or a pytorch (False) matrix. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Optional keyword arguments to LBFGS.\n\n        Returns:\n            Prediction matrix.\n\n        Deprecated:\n            1.4: Renamed Xnew, Znew to X, Z.\n        \"\"\"\n        if Xnew is not None:\n            X = Xnew\n            _deprecated(\n                \"Parameter 'Xnew' in Slisemap.predict\",\n                \"parameter 'X' in Slisemap.predict\",\n            )\n        if Znew is not None:\n            Z = Znew\n            _deprecated(\n                \"Parameter 'Znew' in Slisemap.predict\",\n                \"parameter 'Z' in Slisemap.predict\",\n            )\n        if X is None:\n            X = self._X\n            if B is None and Z is None:\n                B = self._B\n        else:\n            X = self._as_new_X(X)\n            if B is None and Z is None:\n                D = torch.cdist(X, self._X)\n                B = self._B[D.argmin(1), :]\n        if B is None:\n            Z = torch.atleast_2d(to_tensor(Z, **self.tensorargs)[0])\n            _assert_shape(Z, (X.shape[0], self.d), \"Z\", Slisemap.predict)\n            D = self._distance(Z, self._Z)\n            W = self.kernel(D)\n            B = self._B[torch.argmin(D, 1)].clone().requires_grad_(True)\n            yhat = lambda: (  # noqa: E731\n                torch.sum(W * self.local_loss(self.local_model(self._X, B), self._Y))\n                + self.lasso * torch.sum(torch.abs(B))\n                + self.ridge * torch.sum(B**2)\n            )\n            LBFGS(yhat, [B], **kwargs)\n        else:\n            B = torch.atleast_2d(to_tensor(B, **self.tensorargs)[0])\n            _assert_shape(B, (X.shape[0], self.q), \"B\", Slisemap.predict)\n        yhat = local_predict(X, B, self.local_model)\n        return tonp(yhat) if numpy else yhat\n\n    def copy(self) -&gt; \"Slisemap\":\n        \"\"\"Make a copy of this Slisemap that references as much of the same torch-data as possible.\n\n        Returns:\n            An almost shallow copy of this Slisemap object.\n        \"\"\"\n        other = copy(self)  # Shallow copy!\n        # Deep copy these:\n        other._B = other._B.clone().detach()\n        other._Z = other._Z.clone().detach()\n        return other\n\n    def restore(self) -&gt; None:\n        \"\"\"Reset B and Z to their initial values B0 and Z0.\n\n        Deprecated:\n            1.6: Use `Slisemap.copy` before any optimisation instead.\n        \"\"\"\n        _deprecated(Slisemap.restore, Slisemap.copy)\n        self._Z = self._Z0.clone().detach()\n        self._B = self._B0.clone().detach()\n\n    def save(\n        self,\n        f: Union[str, PathLike, BinaryIO],\n        any_extension: bool = False,\n        compress: Union[bool, int] = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save the Slisemap object to a file.\n\n        This method uses `torch.save` (which uses `pickle` for the non-pytorch properties).\n        This means that lambda-functions are not supported (unless a custom pickle module is used, see `torch.save`).\n\n        Note that the random state is not saved, only the initial seed (if set).\n\n        The default file extension is \".sm\".\n\n        Args:\n            f: Either a Path-like object or a (writable) File-like object.\n            any_extension: Do not check the file extension. Defaults to False.\n            compress: Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.\n\n        Keyword Args:\n            **kwargs: Parameters forwarded to `torch.save`.\n        \"\"\"\n        if not any_extension and isinstance(f, (str, PathLike)):  # noqa: SIM102\n            if not str(f).endswith(\".sm\"):\n                _warn(\n                    \"When saving Slisemap objects, consider using the '.sm' extension for consistency.\",\n                    Slisemap.save,\n                )\n        loss = self._loss\n        prng = self._random_state\n        try:\n            self.metadata.root = None\n            self._B = self._B.detach()\n            self._Z = self._Z.detach()\n            self._loss = None\n            self._random_state = None\n            if isinstance(compress, int) and compress &gt; 0:\n                with lzma.open(f, \"wb\", preset=compress) as f2:\n                    torch.save(self, f2, **kwargs)\n            elif compress:\n                with lzma.open(f, \"wb\") as f2:\n                    torch.save(self, f2, **kwargs)\n            else:\n                torch.save(self, f, **kwargs)\n        finally:\n            self.metadata.root = self\n            self._loss = loss\n            self._random_state = prng\n\n    @classmethod\n    def load(\n        cls,\n        f: Union[str, PathLike, BinaryIO],\n        device: Union[None, str, torch.device] = None,\n        *,\n        map_location: Optional[object] = None,\n        **kwargs: Any,\n    ) -&gt; \"Slisemap\":\n        \"\"\"Load a Slisemap object from a file.\n\n        This function uses `torch.load`, so the tensors are restored to their previous devices.\n        Use `device=\"cpu\"` to avoid assuming that the same device exists.\n        This is useful if the Slisemap object has been trained on a GPU, but the current computer lacks a GPU.\n\n        Note that this is a classmethod, use it with: `Slisemap.load(...)`.\n\n        SAFETY: This function is based on `torch.load` which (by default) uses `pickle`.\n        Do not use `Slisemap.load` on untrusted files, since `pickle` can run arbitrary Python code.\n\n        Args:\n            f: Either a Path-like object or a (readable) File-like object.\n            device: Device to load the tensors to (or the original if None). Defaults to None.\n\n        Keyword Args:\n            map_location: The same as `device` (this is the name used by `torch.load`). Defaults to None.\n            **kwargs: Parameters forwarded to `torch.load`.\n\n        Returns:\n            The loaded Slisemap object.\n        \"\"\"\n        if device is None:\n            device = map_location\n        try:\n            with lzma.open(f, \"rb\") as f2:\n                sm = torch.load(f2, map_location=device, **kwargs)\n        except lzma.LZMAError:\n            sm: Slisemap = torch.load(f, map_location=device, **kwargs)\n        return sm\n\n    def __setstate__(self, data: Any) -&gt; None:\n        # Handling loading of Slisemap objects from older versions\n        if not isinstance(data, dict):\n            data = next(d for d in data if isinstance(d, dict))\n        for k, v in data.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError as e:\n                _warn(e, Slisemap.__setstate__)\n        if isinstance(getattr(self, \"metadata\", {}), Metadata):\n            self.metadata.root = self\n        else:\n            self.metadata = Metadata(self, **getattr(self, \"metadata\", {}))\n        if not hasattr(self, \"_regularisation\"):\n            self._regularisation = ALocalModel.regularisation\n\n    def get_model_clusters(\n        self,\n        clusters: int,\n        B: Optional[np.ndarray] = None,\n        Z: Optional[np.ndarray] = None,\n        random_state: int = 42,\n        **kwargs: Any,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Cluster the local model coefficients using k-means (from scikit-learn).\n\n        This method (with a fixed random seed) is used for plotting Slisemap solutions.\n\n        Args:\n            clusters: Number of clusters.\n            B: B matrix. Defaults to `self.get_B()`.\n            Z: Z matrix. Defaults to `self.get_Z(rotate=True)`.\n            random_state: random_state for the KMeans clustering. Defaults to 42.\n\n        Keyword Args:\n            **kwargs: Additional arguments to `sklearn.cluster.KMeans` or `sklearn.cluster.MiniBatchKMeans` if `self.n &gt;= 1024`.\n\n        Returns:\n            labels: Vector of cluster labels.\n            centres: Matrix of cluster centres.\n        \"\"\"\n        B = B if B is not None else self.get_B()\n        Z = Z if Z is not None else self.get_Z(rotate=True)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # Some sklearn versions warn about changing defaults for KMeans\n            kwargs.setdefault(\"random_state\", random_state)\n            if self.n &gt;= 1024:\n                km = MiniBatchKMeans(clusters, **kwargs).fit(B)\n            else:\n                km = KMeans(clusters, **kwargs).fit(B)\n        ord = np.argsort([Z[km.labels_ == k, 0].mean() for k in range(clusters)])\n        return np.argsort(ord)[km.labels_], km.cluster_centers_[ord]\n\n    def plot(\n        self,\n        title: str = \"\",\n        clusters: Union[None, int, np.ndarray] = None,\n        bars: Union[bool, int, Sequence[str]] = True,\n        jitter: Union[float, np.ndarray] = 0.0,\n        show: bool = True,\n        bar: Union[None, bool, int] = None,\n        *,\n        B: Optional[np.ndarray] = None,\n        Z: Optional[np.ndarray] = None,\n        variables: Optional[Sequence[str]] = None,\n        targets: Union[None, str, Sequence[str]] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[Figure]:\n        \"\"\"Plot the Slisemap solution using seaborn.\n\n        Args:\n            title: Title of the plot. Defaults to \"\".\n            clusters: Can be None (plot individual losses), an int (plot k-means clusters of B), or an array of known cluster id:s. Defaults to None.\n            bars: Plot the local models in a bar plot. Either an int (to only plot the most influential variables), a list of variables, or a bool. Defaults to True.\n            jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n            show: Show the plot. Defaults to True.\n            bar: Alternative spelling for `bars`. Defaults to None.\n\n        Keyword Args:\n            B: Override self.get_B() in the plot. Defaults to None. **DEPRECATED**\n            Z: Override self.get_Z() in the plot. Defaults to None. **DEPRECATED**\n            variables: List of variable names. Defaults to None. **DEPRECATED**\n            targets: Target name(s). Defaults to None. **DEPRECATED**\n            **kwargs: Additional arguments to [plot_solution][slisemap.plot.plot_solution] and `plt.subplots`.\n\n        Returns:\n            `matplotlib.figure.Figure` if `show=False`.\n\n        Deprecated:\n            1.3: Parameter `variables`, use `metadata.set_variables()` instead!\n            1.3: Parameter `targets`, use `metadata.set_targets()` instead!\n            1.3: Parameter `B`.\n            1.3: Parameter `Z`.\n        \"\"\"\n        if bar is not None:\n            bars = bar\n        if Z is None:\n            Z = self.get_Z(rotate=True)\n        else:\n            _deprecated(\"Parameter 'Z' in Slisemap.plot\")\n        if B is None:\n            B = self.get_B()\n        else:\n            _deprecated(\"Parameter 'B' in Slisemap.plot\")\n        dimensions = self.metadata.get_dimensions(long=True)\n        if variables is not None:\n            _deprecated(\n                \"Parameter 'variables' in 'Slisemap.plot'\",\n                \"'Slisemap.metadata.set_variables'\",\n            )\n            coefficients = _expand_variable_names(\n                variables, self.intercept, self.m, targets, self.q\n            )\n        else:\n            coefficients = self.metadata.get_coefficients()\n        if targets is not None:\n            _deprecated(\n                \"Parameter 'targets' in 'Slisemap.plot'\",\n                \"'Slisemap.metadata.set_targets'\",\n            )\n\n        loss = None\n        centers = None\n        if clusters is None:\n            if Z.shape[0] == self._Z.shape[0]:\n                loss = tonp(self.local_loss(self.predict(numpy=False), self._Y))\n        else:\n            if isinstance(clusters, int):\n                clusters, centers = self.get_model_clusters(clusters, B, Z)\n            else:\n                clusters = np.asarray(clusters)\n                centers = np.stack(\n                    [np.mean(B[clusters == c, :], 0) for c in np.unique(clusters)], 0\n                )\n        fig = plot_solution(\n            Z=Z,\n            B=B,\n            loss=loss,\n            clusters=clusters,\n            centers=centers,\n            coefficients=coefficients,\n            dimensions=dimensions,\n            title=title,\n            bars=bars,\n            jitter=jitter,\n            **kwargs,\n        )\n        if show:\n            plt.show()\n        else:\n            return fig\n\n    def plot_position(\n        self,\n        X: Optional[ToTensor] = None,\n        Y: Optional[ToTensor] = None,\n        index: Union[None, int, Sequence[int]] = None,\n        title: str = \"\",\n        jitter: Union[float, np.ndarray] = 0.0,\n        selection: bool = True,\n        legend_inside: bool = True,\n        show: bool = True,\n        *,\n        Z: Optional[np.ndarray] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[sns.FacetGrid]:\n        \"\"\"Plot local losses for alternative locations for the selected item(s).\n\n        Indicate the selected item(s) either via `X` and `Y` or via `index`.\n\n        Args:\n            X: Data matrix for the selected data item(s). Defaults to None.\n            Y: Response matrix for the selected data item(s). Defaults to None.\n            index: Index/indices of the selected data item(s). Defaults to None.\n            title: Title of the plot. Defaults to \"\".\n            jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n            selection: Mark the selected data item(s), if index is given. Defaults to True.\n            legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n            show: Show the plot. Defaults to True.\n\n        Keyword Args:\n            Z: Override `self.get_Z()` in the plot. Defaults to None. **DEPRECATED**\n            **kwargs: Additional arguments to `seaborn.relplot`.\n\n        Returns:\n            `seaborn.FacetGrid` if `show=False`.\n\n        Deprecated:\n            1.3: Parameter `Z`.\n        \"\"\"\n        if Z is not None:\n            _deprecated(\"Parameter 'Z' in Slisemap.plot_position\")\n        else:\n            Z = self.get_Z(rotate=True)\n        if index is None:\n            _assert(\n                X is not None and Y is not None,\n                \"Either index or X and Y must be given\",\n                Slisemap.plot_position,\n            )\n            L = self.get_L(X=X, Y=Y)\n        else:\n            if isinstance(index, int):\n                index = [index]\n            L = self.get_L()[:, index]\n        g = plot_position(\n            Z=Z,\n            L=L,\n            Zs=Z[index, :] if selection and index is not None else None,\n            dimensions=self.metadata.get_dimensions(long=True),\n            title=title,\n            jitter=jitter,\n            legend_inside=legend_inside,\n            **kwargs,\n        )\n        if show:\n            plt.show()\n        else:\n            return g\n\n    def plot_dist(\n        self,\n        title: str = \"\",\n        clusters: Union[None, int, np.ndarray] = None,\n        unscale: bool = True,\n        scatter: bool = False,\n        jitter: float = 0.0,\n        legend_inside: bool = True,\n        show: bool = True,\n        *,\n        X: Optional[np.ndarray] = None,\n        Y: Optional[np.ndarray] = None,\n        B: Optional[np.ndarray] = None,\n        variables: Optional[List[str]] = None,\n        targets: Union[None, str, Sequence[str]] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[sns.FacetGrid]:\n        \"\"\"Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.\n\n        Args:\n            title: Title of the plot. Defaults to \"\".\n            clusters: Number of cluster or vector of cluster labels. Defaults to None.\n            scatter: Use scatterplots instead of density plots (clusters are ignored). Defaults to False.\n            unscale: Unscale `X` and `Y` if scaling metadata has been given (see `Slisemap.metadata.set_scale_X`). Defaults to True.\n            jitter: Add jitter to the scatterplots. Defaults to 0.0.\n            legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n            show: Show the plot. Defaults to True.\n\n        Keyword Args:\n            X: Override self.get_X(). Defaults to None. **DEPRECATED**\n            Y: Override self.get_Y(). Defaults to None. **DEPRECATED**\n            B: Override self.get_B() when finding the clusters (only used if clusters is an int). Defaults to None. **DEPRECATED**\n            variables: List of variable names. Defaults to None. **DEPRECATED**\n            targets: Target name(s). Defaults to None. **DEPRECATED**\n            **kwargs: Additional arguments to `seaborn.relplot` or `seaborn.scatterplot`.\n\n        Returns:\n            `seaborn.FacetGrid` if `show=False`.\n\n        Deprecated:\n            1.3: Parameter `variables`, use `metadata.set_variables()` instead!\n            1.3: Parameter `targets`, use `metadata.set_targets()` instead!\n            1.3: Parameter `X`, use `metadata.set_scale_X()` instead (to automatically unscale)!\n            1.3: Parameter `Y`, use `metadata.set_scale_Y()` instead (to automatically unscale)!\n            1.3: Parameter `B`.\n        \"\"\"\n        if X is None:\n            X = self.get_X(intercept=False)\n        else:\n            _deprecated(\"Parameter 'X' in Slisemap.plot_dist\")\n        if Y is None:\n            Y = self.get_Y()\n        else:\n            _deprecated(\"Parameter 'Y' in Slisemap.plot_dist\")\n            Y = np.reshape(Y, (X.shape[0], -1))\n        if isinstance(X, torch.Tensor):\n            X = tonp(X)\n        if isinstance(Y, torch.Tensor):\n            Y = tonp(Y)\n        if unscale:\n            X = self.metadata.unscale_X(X)\n            Y = self.metadata.unscale_Y(Y)\n        if variables is None:\n            variables = self.metadata.get_variables(intercept=False)\n        else:\n            _deprecated(\n                \"Parameter 'variables' in 'Slisemap.plot_dist'\",\n                \"'Slisemap.metadata.set_variables'\",\n            )\n        if targets is not None:\n            _deprecated(\n                \"Parameter 'targets' in 'Slisemap.plot_dist'\",\n                \"'Slisemap.metadata.set_targets'\",\n            )\n            if isinstance(targets, str):\n                targets = [targets]\n        else:\n            targets = self.metadata.get_targets()\n        if B is not None:\n            _deprecated(\"Parameter 'B' in Slisemap.plot_dist\")\n        if isinstance(clusters, int):\n            clusters, _ = self.get_model_clusters(clusters, B)\n        loss = tonp(self.local_loss(self.predict(numpy=False), self._Y))\n\n        g = plot_dist(\n            X=X,\n            Y=Y,\n            Z=self.get_Z(),\n            loss=loss,\n            variables=self.metadata.get_variables(False),\n            targets=self.metadata.get_targets(),\n            dimensions=self.metadata.get_dimensions(long=True),\n            title=title,\n            clusters=clusters,\n            scatter=scatter,\n            jitter=jitter,\n            legend_inside=legend_inside,\n            **kwargs,\n        )\n        if show:\n            plt.show()\n        else:\n            return g\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.__init__","title":"<code>__init__(X, y, radius=3.5, d=2, lasso=None, ridge=None, z_norm=0.01, intercept=True, local_model=LinearRegression, local_loss=None, coefficients=None, regularisation=None, distance=torch.cdist, kernel=softmax_row_kernel, B0=None, Z0=None, jit=True, random_state=None, dtype=torch.float32, device=None, cuda=None)</code>","text":"<p>Create a Slisemap object.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ToTensor</code> <p>Data matrix.</p> required <code>y</code> <code>ToTensor</code> <p>Target vector or matrix.</p> required <code>radius</code> <code>float</code> <p>The radius of the embedding Z. Defaults to 3.5.</p> <code>3.5</code> <code>d</code> <code>int</code> <p>The number of embedding dimensions. Defaults to 2.</p> <code>2</code> <code>lasso</code> <code>Optional[float]</code> <p>Lasso regularisation coefficient. Defaults to 0.0.</p> <code>None</code> <code>ridge</code> <code>Optional[float]</code> <p>Ridge regularisation coefficient. Defaults to 0.0.</p> <code>None</code> <code>z_norm</code> <code>float</code> <p>Z normalisation regularisation coefficient. Defaults to 0.01.</p> <code>0.01</code> <code>intercept</code> <code>bool</code> <p>Should an intercept term be added to <code>X</code>. Defaults to True.</p> <code>True</code> <code>local_model</code> <code>Union[LocalModelCollection, CallableLike[predict]]</code> <p>Local model prediction function (see slisemap.local_models.identify_local_model). Defaults to LinearRegression.</p> <code>LinearRegression</code> <code>local_loss</code> <code>Optional[CallableLike[loss]]</code> <p>Local model loss function (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>coefficients</code> <code>Union[None, int, CallableLike[coefficients]]</code> <p>The number of local model coefficients (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>regularisation</code> <code>Union[None, CallableLike[regularisation]]</code> <p>Additional regularisation method (see slisemap.local_models.identify_local_model). Defaults to None.</p> <code>None</code> <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Distance function. Defaults to <code>torch.cdist</code> (Euclidean distance).</p> <code>cdist</code> <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel function. Defaults to softmax_row_kernel.</p> <code>softmax_row_kernel</code> <code>B0</code> <code>Optional[ToTensor]</code> <p>Initial value for B (random if None). Defaults to None.</p> <code>None</code> <code>Z0</code> <code>Optional[ToTensor]</code> <p>Initial value for Z (PCA if None). Defaults to None.</p> <code>None</code> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function for increased performance (see <code>torch.jit.trace</code> for caveats). Defaults to True.</p> <code>True</code> <code>random_state</code> <code>Optional[int]</code> <p>Set an explicit seed for the random number generator (i.e. <code>torch.manual_seed</code>). Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Floating type. Defaults to <code>torch.float32</code>.</p> <code>float32</code> <code>device</code> <code>Optional[device]</code> <p>Torch device. Defaults to None.</p> <code>None</code> <code>cuda</code> <code>Optional[bool]</code> <p>Use cuda if available. Defaults to True, if the data is large enough.</p> <code>None</code> Deprecated <p>1.6: Use <code>device</code> instead of <code>cuda</code> to force a specific device. 1.6: The <code>random_state</code> has been moved to the escape function.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def __init__(\n    self,\n    X: ToTensor,\n    y: ToTensor,\n    radius: float = 3.5,\n    d: int = 2,\n    lasso: Optional[float] = None,\n    ridge: Optional[float] = None,\n    z_norm: float = 0.01,\n    intercept: bool = True,\n    local_model: Union[\n        LocalModelCollection, CallableLike[ALocalModel.predict]\n    ] = LinearRegression,\n    local_loss: Optional[CallableLike[ALocalModel.loss]] = None,\n    coefficients: Union[None, int, CallableLike[ALocalModel.coefficients]] = None,\n    regularisation: Union[None, CallableLike[ALocalModel.regularisation]] = None,\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = torch.cdist,\n    kernel: Callable[[torch.Tensor], torch.Tensor] = softmax_row_kernel,\n    B0: Optional[ToTensor] = None,\n    Z0: Optional[ToTensor] = None,\n    jit: bool = True,\n    random_state: Optional[int] = None,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n    cuda: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Create a Slisemap object.\n\n    Args:\n        X: Data matrix.\n        y: Target vector or matrix.\n        radius: The radius of the embedding Z. Defaults to 3.5.\n        d: The number of embedding dimensions. Defaults to 2.\n        lasso: Lasso regularisation coefficient. Defaults to 0.0.\n        ridge: Ridge regularisation coefficient. Defaults to 0.0.\n        z_norm: Z normalisation regularisation coefficient. Defaults to 0.01.\n        intercept: Should an intercept term be added to `X`. Defaults to True.\n        local_model: Local model prediction function (see [slisemap.local_models.identify_local_model][]). Defaults to [LinearRegression][slisemap.local_models.LinearRegression].\n        local_loss: Local model loss function (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        coefficients: The number of local model coefficients (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        regularisation: Additional regularisation method (see [slisemap.local_models.identify_local_model][]). Defaults to None.\n        distance: Distance function. Defaults to `torch.cdist` (Euclidean distance).\n        kernel: Kernel function. Defaults to [softmax_row_kernel][slisemap.utils.softmax_row_kernel].\n        B0: Initial value for B (random if None). Defaults to None.\n        Z0: Initial value for Z (PCA if None). Defaults to None.\n        jit: Just-In-Time compile the loss function for increased performance (see `torch.jit.trace` for caveats). Defaults to True.\n        random_state: Set an explicit seed for the random number generator (i.e. `torch.manual_seed`). Defaults to None.\n        dtype: Floating type. Defaults to `torch.float32`.\n        device: Torch device. Defaults to None.\n        cuda: Use cuda if available. Defaults to True, if the data is large enough.\n\n    Deprecated:\n        1.6: Use `device` instead of `cuda` to force a specific device.\n        1.6: The `random_state` has been moved to the escape function.\n    \"\"\"\n    for s in Slisemap.__slots__:\n        # Initialise all attributes (to avoid attribute errors)\n        setattr(self, s, None)\n    if lasso is None and ridge is None:\n        _warn(\n            \"Consider using regularisation!\\n\"\n            + \"Regularisation is important for handling small neighbourhoods, and also makes the local models more local. \"\n            + \"Lasso (l1) and ridge (l2) regularisation is built-in, via the parameters `lasso` and `ridge`. \"\n            + \"Set `lasso=0` to disable this warning (if no regularisation is really desired).\",\n            Slisemap,\n        )\n    local_model, local_loss, coefficients, regularisation = identify_local_model(\n        local_model, local_loss, coefficients, regularisation\n    )\n    self.lasso = 0.0 if lasso is None else lasso\n    self.ridge = 0.0 if ridge is None else ridge\n    self.kernel = kernel\n    self.distance = distance\n    self.local_model = local_model\n    self.local_loss = local_loss\n    self.regularisation = regularisation\n    self.z_norm = z_norm\n    self.radius = radius\n    self._intercept = intercept\n    self._jit = jit\n    self.metadata: Metadata = Metadata(self)\n\n    if cuda is not None:\n        _deprecated(\"cuda\", \"device\")\n    if device is None:\n        if cuda is None and isinstance(X, torch.Tensor):\n            device = X.device\n        elif cuda is True:\n            device = torch.device(\"cuda\")\n    tensorargs = {\"device\": device, \"dtype\": dtype}\n\n    self._X, X_rows, X_columns = to_tensor(X, **tensorargs)\n    if intercept:\n        self._X = torch.cat((self._X, torch.ones_like(self._X[:, :1])), 1)\n    n, m = self._X.shape\n    self.metadata.set_variables(X_columns, intercept)\n\n    self._Y, Y_rows, Y_columns = to_tensor(y, **tensorargs)\n    self.metadata.set_targets(Y_columns)\n    if len(self._Y.shape) == 1:\n        self._Y = self._Y[:, None]\n    _assert_shape(self._Y, (n, self._Y.shape[1]), \"Y\", Slisemap)\n\n    if random_state is not None:\n        self.random_state = random_state\n\n    if Z0 is None:\n        self._Z0 = self._X @ PCA_rotation(self._X, d)\n        if self._Z0.shape[1] &lt; d:\n            _warn(\n                \"The number of embedding dimensions is larger than the number of data dimensions\",\n                Slisemap,\n            )\n            Z0fill = torch.zeros(size=[n, d - self._Z0.shape[1]], **tensorargs)\n            self._Z0 = torch.cat((self._Z0, Z0fill), 1)\n        Z_rows = None\n    else:\n        self._Z0, Z_rows, Z_columns = to_tensor(Z0, **tensorargs)\n        self.metadata.set_dimensions(Z_columns)\n        _assert_shape(self._Z0, (n, d), \"Z0\", Slisemap)\n    if radius &gt; 0:\n        norm = 1 / (torch.sqrt(torch.sum(self._Z0**2) / self._Z0.shape[0]) + 1e-8)\n        self._Z0 = self._Z0 * norm\n    self._Z = self._Z0.detach().clone()\n\n    if callable(coefficients):\n        coefficients = coefficients(self._X, self._Y)\n    if B0 is None:\n        B0 = global_model(\n            X=self._X,\n            Y=self._Y,\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            coefficients=coefficients,\n            lasso=self.lasso,\n            ridge=self.ridge,\n        ).detach()\n        if not torch.all(torch.isfinite(B0)):\n            _warn(\n                \"Optimising a global model as initialisation resulted in non-finite values. Consider using stronger regularisation (increase `lasso` or `ridge`).\",\n                Slisemap,\n            )\n            B0 = torch.zeros_like(B0)\n        self._B0 = B0.expand((n, coefficients))\n        B_rows = None\n    else:\n        self._B0, B_rows, B_columns = to_tensor(B0, **tensorargs)\n        if self._B0.shape[0] == 1:\n            self._B0 = self._B0.expand((self.n, coefficients))\n        _assert_shape(self._B0, (n, coefficients), \"B0\", Slisemap)\n        self.metadata.set_coefficients(B_columns)\n    self._B = self._B0.clone()\n    self.metadata.set_rows(X_rows, Y_rows, B_rows, Z_rows)\n\n    if (\n        device is None\n        and self.n**2 * self.m * self.o &gt; 1_000_000\n        and torch.cuda.is_available()\n    ):\n        self.cuda()\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>The number of data items.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.m","title":"<code>m: int</code>  <code>property</code>","text":"<p>The number of variables (including potential intercept).</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.o","title":"<code>o: int</code>  <code>property</code>","text":"<p>The number of target variables (i.e. the number of classes).</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.d","title":"<code>d: int</code>  <code>property</code> <code>writable</code>","text":"<p>The number of embedding dimensions.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.q","title":"<code>q: int</code>  <code>property</code>","text":"<p>The number of local model coefficients.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.intercept","title":"<code>intercept: bool</code>  <code>property</code>","text":"<p>Is an intercept column added to the data?.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.radius","title":"<code>radius: float</code>  <code>property</code> <code>writable</code>","text":"<p>The radius of the embedding.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.lasso","title":"<code>lasso: float</code>  <code>property</code> <code>writable</code>","text":"<p>Lasso regularisation strength.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.ridge","title":"<code>ridge: float</code>  <code>property</code> <code>writable</code>","text":"<p>Ridge regularisation strength.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.z_norm","title":"<code>z_norm: float</code>  <code>property</code> <code>writable</code>","text":"<p>Z normalisation regularisation strength.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.local_model","title":"<code>local_model: CallableLike[ALocalModel.predict]</code>  <code>property</code> <code>writable</code>","text":"<p>Local model prediction function. Takes in X[n, m] and B[n, q], and returns Ytilde[n, n, o].</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.local_loss","title":"<code>local_loss: CallableLike[ALocalModel.loss]</code>  <code>property</code> <code>writable</code>","text":"<p>Local model loss function. Takes in Ytilde[n, n, o] and Y[n, o] and returns L[n, n].</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.regularisation","title":"<code>regularisation: CallableLike[ALocalModel.regularisation]</code>  <code>property</code> <code>writable</code>","text":"<p>Regularisation function. Takes in X, Y, Bp, Z, and Ytilde and returns an additional loss scalar.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.distance","title":"<code>distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>  <code>property</code> <code>writable</code>","text":"<p>Distance function. Takes in Z[n1, d] and Z[n2, d], and returns D[n1, n2].</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.kernel","title":"<code>kernel: Callable[[torch.Tensor], torch.Tensor]</code>  <code>property</code> <code>writable</code>","text":"<p>Kernel function. Takes in D[n, n] and returns W[n, n].</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.jit","title":"<code>jit: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Just-In-Time compile the loss function?.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.random_state","title":"<code>random_state: None = property(fset=random_state, doc=random_state.__doc__)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the seed for the random number generator specific for this object (None reverts to the global <code>torch</code> PRNG).</p> Deprecated <p>1.6: Use <code>Slisemap.escape(random_state=...)</code> instead.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.tensorargs","title":"<code>tensorargs: Dict[str, Any]</code>  <code>property</code>","text":"<p>When creating a new <code>torch.Tensor</code> add these keyword arguments to match the <code>dtype</code> and <code>device</code> of this Slisemap object.</p>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.cuda","title":"<code>cuda(**kwargs)</code>","text":"<p>Move the tensors to CUDA memory (and run the calculations there).</p> <p>Note that this resets the random state.</p> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional arguments to <code>torch.Tensor.cuda</code></p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def cuda(self, **kwargs: Any) -&gt; None:\n    \"\"\"Move the tensors to CUDA memory (and run the calculations there).\n\n    Note that this resets the random state.\n\n    Keyword Args:\n        **kwargs: Optional arguments to `torch.Tensor.cuda`\n    \"\"\"\n    X = self._X.cuda(**kwargs)\n    self._X = X\n    self._Y = self._Y.cuda(**kwargs)\n    self._Z = self._Z.detach().cuda(**kwargs)\n    self._B = self._B.detach().cuda(**kwargs)\n    self._loss = None  # invalidate cached loss function\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.cpu","title":"<code>cpu(**kwargs)</code>","text":"<p>Move the tensors to CPU memory (and run the calculations there).</p> <p>Note that this resets the random state.</p> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional arguments to <code>torch.Tensor.cpu</code></p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def cpu(self, **kwargs: Any) -&gt; None:\n    \"\"\"Move the tensors to CPU memory (and run the calculations there).\n\n    Note that this resets the random state.\n\n    Keyword Args:\n        **kwargs: Optional arguments to `torch.Tensor.cpu`\n    \"\"\"\n    X = self._X.cpu(**kwargs)\n    self._X = X\n    self._Y = self._Y.cpu(**kwargs)\n    self._Z = self._Z.detach().cpu(**kwargs)\n    self._B = self._B.detach().cpu(**kwargs)\n    self._loss = None  # invalidate cached loss function\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_Z","title":"<code>get_Z(scale=True, rotate=False, numpy=True)</code>","text":"<p>Get the Z matrix.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>bool</code> <p>Scale the returned <code>Z</code> to match self.radius. Defaults to True.</p> <code>True</code> <code>rotate</code> <code>bool</code> <p>Rotate the returned <code>Z</code> so that the first dimension is the major axis. Defaults to False.</p> <code>False</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>Z</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_Z(\n    self, scale: bool = True, rotate: bool = False, numpy: bool = True\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the Z matrix.\n\n    Args:\n        scale: Scale the returned `Z` to match self.radius. Defaults to True.\n        rotate: Rotate the returned `Z` so that the first dimension is the major axis. Defaults to False.\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n       The `Z` matrix.\n    \"\"\"\n    self._normalise()\n    Z = self._Z * self.radius if scale and self.radius &gt; 0 else self._Z\n    if rotate:\n        Z = Z @ PCA_rotation(Z, center=False)\n    return tonp(Z) if numpy else Z\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_B","title":"<code>get_B(numpy=True)</code>","text":"<p>Get the B matrix.</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>B</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_B(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the B matrix.\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n       The `B` matrix.\n    \"\"\"\n    return tonp(self._B) if numpy else self._B\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_D","title":"<code>get_D(numpy=True)</code>","text":"<p>Get the embedding distance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>D</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_D(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the embedding distance matrix.\n\n    Args:\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n       The `D` matrix.\n    \"\"\"\n    Z = self.get_Z(rotate=False, scale=True, numpy=False)\n    D = self._distance(Z, Z)\n    return tonp(D) if numpy else D\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_L","title":"<code>get_L(X=None, Y=None, numpy=True)</code>","text":"<p>Get the loss matrix: [B.shape[0], X.shape[0]].</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ToTensor]</code> <p>Optional replacement for the training X. Defaults to None.</p> <code>None</code> <code>Y</code> <code>Optional[ToTensor]</code> <p>Optional replacement for the training Y. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>L</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_L(\n    self,\n    X: Optional[ToTensor] = None,\n    Y: Optional[ToTensor] = None,\n    numpy: bool = True,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the loss matrix: [B.shape[0], X.shape[0]].\n\n    Args:\n        X: Optional replacement for the training X. Defaults to None.\n        Y: Optional replacement for the training Y. Defaults to None.\n        numpy: Return the matrix as a numpy (True) or pytorch (False) matrix. Defaults to True.\n\n    Returns:\n       The `L` matrix.\n    \"\"\"\n    X = self._as_new_X(X)\n    Y = self._as_new_Y(Y, X.shape[0])\n    L = self.local_loss(self.local_model(X, self._B), Y)\n    return tonp(L) if numpy else L\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_W","title":"<code>get_W(numpy=True)</code>","text":"<p>Get the weight matrix.</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>W</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_W(self, numpy: bool = True) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the weight matrix.\n\n    Args:\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n\n    Returns:\n       The `W` matrix.\n    \"\"\"\n    W = self.kernel(self.get_D(numpy=False))\n    return tonp(W) if numpy else W\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_X","title":"<code>get_X(numpy=True, intercept=True)</code>","text":"<p>Get the data matrix.</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <code>intercept</code> <code>bool</code> <p>Include the intercept column (if <code>self.intercept == True</code>). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>X</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_X(\n    self, numpy: bool = True, intercept: bool = True\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the data matrix.\n\n    Args:\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n        intercept: Include the intercept column (if `self.intercept == True`). Defaults to True.\n\n    Returns:\n       The `X` matrix.\n    \"\"\"\n    X = self._X if intercept or not self._intercept else self._X[:, :-1]\n    return tonp(X) if numpy else X\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_Y","title":"<code>get_Y(numpy=True, ravel=False)</code>","text":"<p>Get the target matrix.</p> <p>Parameters:</p> Name Type Description Default <code>numpy</code> <code>bool</code> <p>Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <code>ravel</code> <code>bool</code> <p>Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>The <code>Y</code> matrix.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_Y(\n    self, numpy: bool = True, ravel: bool = False\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"Get the target matrix.\n\n    Args:\n        numpy: Return the matrix as a numpy.ndarray instead of a torch.Tensor. Defaults to True.\n        ravel: Remove the second dimension if it is singular (i.e. turn it into a vector). Defaults to False.\n\n    Returns:\n       The `Y` matrix.\n    \"\"\"\n    Y = self._Y.ravel() if ravel else self._Y\n    return tonp(Y) if numpy else Y\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.value","title":"<code>value(individual=False, numpy=True)</code>","text":"<p>Calculate the loss value.</p> <p>Parameters:</p> Name Type Description Default <code>individual</code> <code>bool</code> <p>Give loss individual loss values for the data points. Defaults to False.</p> <code>False</code> <code>numpy</code> <code>bool</code> <p>Return the loss as a numpy.ndarray or float instead of a torch.Tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[float, ndarray, Tensor]</code> <p>The loss value(s).</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def value(\n    self, individual: bool = False, numpy: bool = True\n) -&gt; Union[float, np.ndarray, torch.Tensor]:\n    \"\"\"Calculate the loss value.\n\n    Args:\n        individual: Give loss individual loss values for the data points. Defaults to False.\n        numpy: Return the loss as a numpy.ndarray or float instead of a torch.Tensor. Defaults to True.\n\n    Returns:\n        The loss value(s).\n    \"\"\"\n    loss = self._get_loss_fn(individual)\n    loss = loss(X=self._X, Y=self._Y, B=self._B, Z=self._Z)\n    if individual:\n        return tonp(loss) if numpy else loss\n    else:\n        return loss.cpu().item() if numpy else loss\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.entropy","title":"<code>entropy(aggregate=True, numpy=True)</code>","text":"<p>Compute row-wise entropy of the <code>W</code> matrix induced by <code>Z</code>. DEPRECATED.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate</code> <code>bool</code> <p>Aggregate the row-wise entropies into one scalar. Defaults to True.</p> <code>True</code> <code>numpy</code> <code>bool</code> <p>Return a <code>numpy.ndarray</code> or <code>float</code> instead of a <code>torch.Tensor</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[float, ndarray, Tensor]</code> <p>The entropy.</p> Deprecated <p>1.4: Use slisemap.metrics.entropy instead.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def entropy(\n    self, aggregate: bool = True, numpy: bool = True\n) -&gt; Union[float, np.ndarray, torch.Tensor]:\n    \"\"\"Compute row-wise entropy of the `W` matrix induced by `Z`. **DEPRECATED**.\n\n    Args:\n        aggregate: Aggregate the row-wise entropies into one scalar. Defaults to True.\n        numpy: Return a `numpy.ndarray` or `float` instead of a `torch.Tensor`. Defaults to True.\n\n    Returns:\n        The entropy.\n\n    Deprecated:\n        1.4: Use [slisemap.metrics.entropy][slisemap.metrics.entropy] instead.\n    \"\"\"\n    _deprecated(Slisemap.entropy, \"slisemap.metrics.entropy\")\n    from slisemap.metrics import entropy\n\n    return entropy(self, aggregate, numpy)\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.lbfgs","title":"<code>lbfgs(max_iter=500, verbose=False, *, only_B=False, **kwargs)</code>","text":"<p>Optimise Slisemap using LBFGS.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <code>int</code> <p>Maximum number of LBFGS iterations. Defaults to 500.</p> <code>500</code> <code>verbose</code> <code>bool</code> <p>Print status messages. Defaults to False.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>only_B</code> <code>bool</code> <p>Only optimise B. Defaults to False.</p> <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to LBFGS.</p> <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def lbfgs(\n    self,\n    max_iter: int = 500,\n    verbose: bool = False,\n    *,\n    only_B: bool = False,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Optimise Slisemap using LBFGS.\n\n    Args:\n        max_iter: Maximum number of LBFGS iterations. Defaults to 500.\n        verbose: Print status messages. Defaults to False.\n\n    Keyword Args:\n        only_B: Only optimise B. Defaults to False.\n        **kwargs: Optional keyword arguments to LBFGS.\n\n    Returns:\n        The loss value.\n    \"\"\"\n    Z = self._Z.detach().clone().requires_grad_(True)\n    B = self._B.detach().clone().requires_grad_(True)\n\n    loss_ = self._get_loss_fn()\n    loss_fn = lambda: loss_(self._X, self._Y, B, Z)  # noqa: E731\n    pre_loss = loss_fn().cpu().detach().item()\n    LBFGS(\n        loss_fn,\n        [B] if only_B else [Z, B],\n        max_iter=max_iter,\n        verbose=verbose,\n        **kwargs,\n    )\n    post_loss = loss_fn().cpu().detach().item()\n\n    if np.isnan(post_loss):\n        _warn(\n            \"An LBFGS optimisation resulted in `nan` (try strengthening the regularisation or reducing the radius)\",\n            Slisemap.lbfgs,\n        )\n        # Some datasets (with logistic local models) are initially numerically unstable.\n        # Just running LBFGS for one iteration seems to avoid those issues.\n        Z = self._Z.detach().requires_grad_(True)\n        B = self._B.detach().requires_grad_(True)\n        LBFGS(\n            loss_fn,\n            [B] if only_B else [Z, B],\n            max_iter=1,\n            verbose=verbose,\n            **kwargs,\n        )\n        post_loss = loss_fn().cpu().detach().item()\n    if post_loss &lt; pre_loss:\n        self._Z = Z.detach()\n        self._B = B.detach()\n        self._normalise()\n        return post_loss\n    else:\n        if verbose:\n            print(\"Slisemap.lbfgs: No improvement found\")\n        return pre_loss\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.escape","title":"<code>escape(force_move=True, escape_fn=escape_neighbourhood, lerp=0.95, noise=0.0, random_state=42)</code>","text":"<p>Try to escape a local optimum by moving the items (embedding and local model) to the neighbourhoods best suited for them.</p> <p>This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.</p> <p>Parameters:</p> Name Type Description Default <code>force_move</code> <code>bool</code> <p>Do not allow the items to pair with themselves. Defaults to True.</p> <code>True</code> <code>escape_fn</code> <code>Callable</code> <p>Escape function (see slisemap.escape). Defaults to escape_neighbourhood.</p> <code>escape_neighbourhood</code> <code>lerp</code> <code>float</code> <p>Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.95.</p> <code>0.95</code> <code>noise</code> <code>float</code> <p>Scale of the noise added to the embedding matrix if it looses rank after an escape (recommended for gradient based optimisers). Defaults to 1e-4.</p> <code>0.0</code> <code>random_state</code> <code>int</code> <p>Seed for the random generator if <code>noise &gt; 0.0</code>. Defaults to 42.</p> <code>42</code> Source code in <code>slisemap/slisemap.py</code> <pre><code>def escape(\n    self,\n    force_move: bool = True,\n    escape_fn: Callable = escape_neighbourhood,\n    lerp: float = 0.95,\n    noise: float = 0.0,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"Try to escape a local optimum by moving the items (embedding and local model) to the neighbourhoods best suited for them.\n\n    This is done by finding another item (in the optimal neighbourhood) and copying its values for Z and B.\n\n    Args:\n        force_move: Do not allow the items to pair with themselves. Defaults to True.\n        escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n        lerp: Linear interpolation between the old (0.0) and the new (1.0) embedding position. Defaults to 0.95.\n        noise: Scale of the noise added to the embedding matrix if it looses rank after an escape (recommended for gradient based optimisers). Defaults to 1e-4.\n        random_state: Seed for the random generator if `noise &gt; 0.0`. Defaults to 42.\n    \"\"\"\n    if lerp &lt;= 0.0:\n        _warn(\"Escaping with `lerp &lt;= 0` does nothing!\", Slisemap.escape)\n        return\n    B, Z = escape_fn(\n        X=self._X,\n        Y=self._Y,\n        B=self._B,\n        Z=self._Z,\n        local_model=self.local_model,\n        local_loss=self.local_loss,\n        distance=self.distance,\n        kernel=self.kernel,\n        radius=self.radius,\n        force_move=force_move,\n        jit=self.jit,\n    )\n    if lerp &gt;= 1.0:\n        self._B, self._Z = B, Z\n    else:\n        self._B = (1.0 - lerp) * self._B + lerp * B\n        self._Z = (1.0 - lerp) * self._Z + lerp * Z\n    if noise &gt; 0.0:\n        rank = torch.linalg.matrix_rank(self._Z - torch.mean(self._Z, 0, True))\n        if rank.item() &lt; min(*self._Z.shape):\n            generator = torch.Generator(self._Z.device).manual_seed(random_state)\n            self._Z = torch.normal(self._Z, noise, generator=generator)\n    self._normalise()\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.optimise","title":"<code>optimise(patience=2, max_escapes=100, max_iter=500, verbose=0, only_B=False, escape_kws={}, *, escape_fn=None, noise=None, **kwargs)</code>","text":"<p>Optimise Slisemap by alternating between self.lbfgs() and self.escape() until convergence.</p> <p>Statistics for the optimisation can be found in <code>self.metadata[\"optimize_time\"]</code> and <code>self.metadata[\"optimize_loss\"]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of escapes without improvement before stopping. Defaults to 2.</p> <code>2</code> <code>max_escapes</code> <code>int</code> <p>Maximum numbers optimisation rounds. Defaults to 100.</p> <code>100</code> <code>max_iter</code> <code>int</code> <p>Maximum number of LBFGS iterations per round. Defaults to 500.</p> <code>500</code> <code>verbose</code> <code>Literal[0, 1, 2]</code> <p>Print status messages (0: no, 1: some, 2: all). Defaults to 0.</p> <code>0</code> <code>only_B</code> <code>bool</code> <p>Only optimise the local models, not the embedding. Defaults to False.</p> <code>False</code> <code>escape_kws</code> <code>Dict[str, object]</code> <p>Optional keyword arguments to self.escape(). Defaults to {}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>escape_fn</code> <code>Optional[CallableLike[escape_neighbourhood]]</code> <p>Escape function (see slisemap.escape). Defaults to escape_neighbourhood.</p> <code>noise</code> <code>Optional[float]</code> <p>Scale of the noise added to the embedding matrix if it looses rank after an escape.</p> <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to Slisemap.lbfgs.</p> <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Deprecated <p>1.6: The <code>noise</code> argument, use <code>escape_kws={\"noise\": noise}</code> instead. 1.6: The <code>escape_fn</code> argument, use <code>escape_kws={\"escape_fn\": escape_fn}</code> instead.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def optimise(\n    self,\n    patience: int = 2,\n    max_escapes: int = 100,\n    max_iter: int = 500,\n    verbose: Literal[0, 1, 2] = 0,\n    only_B: bool = False,\n    escape_kws: Dict[str, object] = {},\n    *,\n    escape_fn: Optional[CallableLike[escape_neighbourhood]] = None,\n    noise: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Optimise Slisemap by alternating between [self.lbfgs()][slisemap.slisemap.Slisemap.lbfgs] and [self.escape()][slisemap.slisemap.Slisemap.escape] until convergence.\n\n    Statistics for the optimisation can be found in `self.metadata[\"optimize_time\"]` and `self.metadata[\"optimize_loss\"]`.\n\n    Args:\n        patience: Number of escapes without improvement before stopping. Defaults to 2.\n        max_escapes: Maximum numbers optimisation rounds. Defaults to 100.\n        max_iter: Maximum number of LBFGS iterations per round. Defaults to 500.\n        verbose: Print status messages (0: no, 1: some, 2: all). Defaults to 0.\n        only_B: Only optimise the local models, not the embedding. Defaults to False.\n        escape_kws: Optional keyword arguments to [self.escape()][slisemap.slisemap.Slisemap.escape]. Defaults to {}.\n\n    Keyword Args:\n        escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n        noise: Scale of the noise added to the embedding matrix if it looses rank after an escape.\n        **kwargs: Optional keyword arguments to Slisemap.lbfgs.\n\n    Returns:\n        The loss value.\n\n    Deprecated:\n        1.6: The `noise` argument, use `escape_kws={\"noise\": noise}` instead.\n        1.6: The `escape_fn` argument, use `escape_kws={\"escape_fn\": escape_fn}` instead.\n    \"\"\"\n    if noise is not None:\n        _deprecated(\n            \"Slisemap.optimise(noise=noise, ...)\",\n            'Slisemap.optimise(escape_kws={\"noise\":noise}, ...)',\n        )\n        escape_kws.setdefault(\"noise\", noise)\n    if escape_fn is not None:\n        _deprecated(\n            \"Slisemap.optimise(escape_fn=escape_fn, ...)\",\n            'Slisemap.optimise(escape_kws={\"escape_fn\":escape_fn}, ...)',\n        )\n        escape_kws.setdefault(\"escape_fn\", escape_fn)\n    loss = np.repeat(np.inf, 2)\n    time = timer()\n    loss[0] = self.lbfgs(\n        max_iter=max_iter,\n        only_B=True,\n        increase_tolerance=not only_B,\n        verbose=verbose &gt; 1,\n        **kwargs,\n    )\n    history = [loss[0]]\n    if verbose:\n        i = 0\n        print(f\"Slisemap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n    if only_B:\n        self.metadata[\"optimize_time\"] = timer() - time\n        self.metadata[\"optimize_loss\"] = history\n        return loss[0]\n    cc = CheckConvergence(patience, max_escapes)\n    while not cc.has_converged(loss, self.copy, verbose=verbose &gt; 1):\n        self.escape(random_state=cc.iter, **escape_kws)\n        loss[1] = self.value()\n        if verbose:\n            print(f\"Slisemap.optimise Escape {i:2d}: {loss[1]:.2f}\")\n        loss[0] = self.lbfgs(\n            max_iter=max_iter,\n            increase_tolerance=True,\n            verbose=verbose &gt; 1,\n            **kwargs,\n        )\n        history.append(loss[1])\n        history.append(loss[0])\n        if verbose:\n            i += 1\n            print(f\"Slisemap.optimise LBFGS  {i:2d}: {loss[0]:.2f}\")\n    self._Z = cc.optimal._Z\n    self._B = cc.optimal._B\n    loss = self.lbfgs(\n        max_iter=max_iter * 2,\n        increase_tolerance=False,\n        verbose=verbose &gt; 1,\n        **kwargs,\n    )\n    history.append(loss)\n    self.metadata[\"optimize_time\"] = timer() - time\n    self.metadata[\"optimize_loss\"] = history\n    if verbose:\n        print(f\"Slisemap.optimise Final    : {loss:.2f}\")\n    return loss\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.fit_new","title":"<code>fit_new(Xnew, ynew, optimise=True, between=True, escape_fn=escape_neighbourhood, loss=False, verbose=False, numpy=True, **kwargs)</code>","text":"<p>Generate embedding(s) and model(s) for new data item(s).</p> This works as follows <ol> <li>Find good initial embedding(s) and local model(s) using the escape_fn.</li> <li>Optionally finetune the embedding(s) and model(s) using LBFG.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>Xnew</code> <code>ToTensor</code> <p>New data point(s).</p> required <code>ynew</code> <code>ToTensor</code> <p>New target(s).</p> required <code>optimise</code> <code>bool</code> <p>Should the embedding and model be optimised (after finding the neighbourhood). Defaults to True.</p> <code>True</code> <code>between</code> <code>bool</code> <p>If <code>optimise=True</code>, should the new points affect each other? Defaults to True.</p> <code>True</code> <code>escape_fn</code> <code>Callable</code> <p>Escape function (see slisemap.escape). Defaults to escape_neighbourhood.</p> <code>escape_neighbourhood</code> <code>loss</code> <code>bool</code> <p>Return a vector of individual losses for the new items. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print status messages. Defaults to False.</p> <code>False</code> <code>numpy</code> <code>bool</code> <p>Return the results as numpy (True) or pytorch (False) matrices. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to LBFGS.</p> <p>Returns:</p> Name Type Description <code>Bnew</code> <code>Union[Tuple[ndarray, ndarray, Optional[ndarray]], Tuple[Tensor, Tensor, Optional[Tensor]]]</code> <p>Local model coefficients for the new data.</p> <code>Znew</code> <code>Union[Tuple[ndarray, ndarray, Optional[ndarray]], Tuple[Tensor, Tensor, Optional[Tensor]]]</code> <p>Embedding(s) for the new data.</p> <code>loss</code> <code>Union[Tuple[ndarray, ndarray, Optional[ndarray]], Tuple[Tensor, Tensor, Optional[Tensor]]]</code> <p>Individual losses if <code>loss=True</code>.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def fit_new(\n    self,\n    Xnew: ToTensor,\n    ynew: ToTensor,\n    optimise: bool = True,\n    between: bool = True,\n    escape_fn: Callable = escape_neighbourhood,\n    loss: bool = False,\n    verbose: bool = False,\n    numpy: bool = True,\n    **kwargs: Any,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]],\n    Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]],\n]:\n    \"\"\"Generate embedding(s) and model(s) for new data item(s).\n\n    This works as follows:\n        1. Find good initial embedding(s) and local model(s) using the escape_fn.\n        2. Optionally finetune the embedding(s) and model(s) using LBFG.\n\n    Args:\n        Xnew: New data point(s).\n        ynew: New target(s).\n        optimise: Should the embedding and model be optimised (after finding the neighbourhood). Defaults to True.\n        between: If `optimise=True`, should the new points affect each other? Defaults to True.\n        escape_fn: Escape function (see [slisemap.escape][]). Defaults to [escape_neighbourhood][slisemap.escape.escape_neighbourhood].\n        loss: Return a vector of individual losses for the new items. Defaults to False.\n        verbose: Print status messages. Defaults to False.\n        numpy: Return the results as numpy (True) or pytorch (False) matrices. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to LBFGS.\n\n    Returns:\n        Bnew: Local model coefficients for the new data.\n        Znew: Embedding(s) for the new data.\n        loss: Individual losses if `loss=True`.\n    \"\"\"\n    Xnew = self._as_new_X(Xnew)\n    n = Xnew.shape[0]\n    ynew = self._as_new_Y(ynew, n)\n    if verbose:\n        print(\"Escaping the new data\")\n    Bnew, Znew = escape_fn(\n        X=Xnew,\n        Y=ynew,\n        B=self._B,\n        Z=self._Z,\n        local_model=self.local_model,\n        local_loss=self.local_loss,\n        distance=self.distance,\n        kernel=self.kernel,\n        radius=self.radius,\n        force_move=False,\n        Xold=self._X,\n        Yold=self._Y,\n        jit=self.jit,\n    )\n    if verbose:\n        Zrad = torch.sqrt(torch.sum(Znew**2) / n).cpu().detach().item()\n        print(\"  radius(Z_new) =\", Zrad)\n\n    if optimise:\n        if verbose:\n            print(\"Optimising the new data\")\n        lf, set_new = make_marginal_loss(\n            X=self._X,\n            Y=self._Y,\n            B=self._B,\n            Z=self._Z,\n            Xnew=Xnew if between else Xnew[:1],\n            Ynew=ynew if between else ynew[:1],\n            local_model=self.local_model,\n            local_loss=self.local_loss,\n            distance=self.distance,\n            kernel=self.kernel,\n            radius=self.radius,\n            lasso=self.lasso,\n            ridge=self.ridge,\n            jit=self.jit,\n        )\n        if between:\n            Bnew = Bnew.detach().requires_grad_(True)\n            Znew = Znew.detach().requires_grad_(True)\n            LBFGS(lambda: lf(Bnew, Znew), [Bnew, Znew], **kwargs)\n        else:\n            for j in range(n):\n                set_new(Xnew[None, j], ynew[None, j])\n                Bi = Bnew[None, j].detach().clone().requires_grad_(True)\n                Zi = Znew[None, j].detach().clone().requires_grad_(True)\n                LBFGS(lambda: lf(Bi, Zi), [Bi, Zi], **kwargs)  # noqa: B023\n                Bnew[j] = Bi.detach()\n                Znew[j] = Zi.detach()\n        if verbose:\n            Zrad = torch.sqrt(torch.sum(Znew**2) / n).cpu().detach().item()\n            print(\"  radius(Z_new) =\", Zrad)\n    if self.radius &gt; 0:\n        if verbose:\n            print(\"Normalising the solution\")\n        norm = self.radius / (torch.sqrt(torch.sum(self._Z**2) / self.n) + 1e-8)\n        Zout = Znew * norm\n        if verbose:\n            Zrad = torch.sqrt(torch.sum(Zout**2) / n).cpu().detach().item()\n            print(\"  radius(Z_new) =\", Zrad)\n    else:\n        Zout = Znew\n    if loss:\n        if verbose:\n            print(\"Calculating individual losses\")\n        lf = self._get_loss_fn(individual=True)\n        if between:\n            loss = lf(\n                X=torch.cat((self._X, Xnew), 0),\n                Y=torch.cat((self._Y, ynew), 0),\n                B=torch.cat((self._B, Bnew), 0),\n                Z=torch.cat((self._Z, Znew), 0),\n            )[self.n :]\n        else:\n            if self._jit:\n                lf = torch.jit.trace(lf, (Xnew[:1], ynew[:1], Bnew[:1], Znew[:1]))\n            loss = torch.zeros(n, **self.tensorargs)\n            for j in range(n):\n                loss[j] = lf(\n                    X=torch.cat((self._X, Xnew[None, j]), 0),\n                    Y=torch.cat((self._Y, ynew[None, j]), 0),\n                    B=torch.cat((self._B, Bnew[None, j]), 0),\n                    Z=torch.cat((self._Z, Znew[None, j]), 0),\n                )[-1]\n        if verbose:\n            print(\"  mean(loss) =\", loss.detach().mean().cpu().item())\n        return (tonp(Bnew), tonp(Zout), tonp(loss)) if numpy else (Bnew, Zout, loss)\n    else:\n        return (tonp(Bnew), tonp(Zout)) if numpy else (Bnew, Zout)\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.predict","title":"<code>predict(X=None, B=None, Z=None, numpy=True, *, Xnew=None, Znew=None, **kwargs)</code>","text":"<p>Predict new outcomes when the data and embedding or local model is known.</p> <p>If the local models <code>B</code> are known they are used. If the embeddings <code>Z</code> are known they are used to find new local models. Ohterwise the closest training X gives the <code>B</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ToTensor]</code> <p>Data matrix (set to None to use the training data). Defaults to None.</p> <code>None</code> <code>B</code> <code>Optional[ToTensor]</code> <p>Coefficient matrix. Defaults to None.</p> <code>None</code> <code>Z</code> <code>Optional[ToTensor]</code> <p>Embedding matrix. Defaults to None.</p> <code>None</code> <code>numpy</code> <code>bool</code> <p>Return the result as a numpy (True) or a pytorch (False) matrix. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to LBFGS.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Prediction matrix.</p> Deprecated <p>1.4: Renamed Xnew, Znew to X, Z.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def predict(  # noqa: D417\n    self,\n    X: Optional[ToTensor] = None,\n    B: Optional[ToTensor] = None,\n    Z: Optional[ToTensor] = None,\n    numpy: bool = True,\n    *,\n    Xnew: Optional[ToTensor] = None,\n    Znew: Optional[ToTensor] = None,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Predict new outcomes when the data and embedding or local model is known.\n\n    If the local models `B` are known they are used.\n    If the embeddings `Z` are known they are used to find new local models.\n    Ohterwise the closest training X gives the `B`.\n\n    Args:\n        X: Data matrix (set to None to use the training data). Defaults to None.\n        B: Coefficient matrix. Defaults to None.\n        Z: Embedding matrix. Defaults to None.\n        numpy: Return the result as a numpy (True) or a pytorch (False) matrix. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to LBFGS.\n\n    Returns:\n        Prediction matrix.\n\n    Deprecated:\n        1.4: Renamed Xnew, Znew to X, Z.\n    \"\"\"\n    if Xnew is not None:\n        X = Xnew\n        _deprecated(\n            \"Parameter 'Xnew' in Slisemap.predict\",\n            \"parameter 'X' in Slisemap.predict\",\n        )\n    if Znew is not None:\n        Z = Znew\n        _deprecated(\n            \"Parameter 'Znew' in Slisemap.predict\",\n            \"parameter 'Z' in Slisemap.predict\",\n        )\n    if X is None:\n        X = self._X\n        if B is None and Z is None:\n            B = self._B\n    else:\n        X = self._as_new_X(X)\n        if B is None and Z is None:\n            D = torch.cdist(X, self._X)\n            B = self._B[D.argmin(1), :]\n    if B is None:\n        Z = torch.atleast_2d(to_tensor(Z, **self.tensorargs)[0])\n        _assert_shape(Z, (X.shape[0], self.d), \"Z\", Slisemap.predict)\n        D = self._distance(Z, self._Z)\n        W = self.kernel(D)\n        B = self._B[torch.argmin(D, 1)].clone().requires_grad_(True)\n        yhat = lambda: (  # noqa: E731\n            torch.sum(W * self.local_loss(self.local_model(self._X, B), self._Y))\n            + self.lasso * torch.sum(torch.abs(B))\n            + self.ridge * torch.sum(B**2)\n        )\n        LBFGS(yhat, [B], **kwargs)\n    else:\n        B = torch.atleast_2d(to_tensor(B, **self.tensorargs)[0])\n        _assert_shape(B, (X.shape[0], self.q), \"B\", Slisemap.predict)\n    yhat = local_predict(X, B, self.local_model)\n    return tonp(yhat) if numpy else yhat\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.copy","title":"<code>copy()</code>","text":"<p>Make a copy of this Slisemap that references as much of the same torch-data as possible.</p> <p>Returns:</p> Type Description <code>Slisemap</code> <p>An almost shallow copy of this Slisemap object.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def copy(self) -&gt; \"Slisemap\":\n    \"\"\"Make a copy of this Slisemap that references as much of the same torch-data as possible.\n\n    Returns:\n        An almost shallow copy of this Slisemap object.\n    \"\"\"\n    other = copy(self)  # Shallow copy!\n    # Deep copy these:\n    other._B = other._B.clone().detach()\n    other._Z = other._Z.clone().detach()\n    return other\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.restore","title":"<code>restore()</code>","text":"<p>Reset B and Z to their initial values B0 and Z0.</p> Deprecated <p>1.6: Use <code>Slisemap.copy</code> before any optimisation instead.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def restore(self) -&gt; None:\n    \"\"\"Reset B and Z to their initial values B0 and Z0.\n\n    Deprecated:\n        1.6: Use `Slisemap.copy` before any optimisation instead.\n    \"\"\"\n    _deprecated(Slisemap.restore, Slisemap.copy)\n    self._Z = self._Z0.clone().detach()\n    self._B = self._B0.clone().detach()\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.save","title":"<code>save(f, any_extension=False, compress=True, **kwargs)</code>","text":"<p>Save the Slisemap object to a file.</p> <p>This method uses <code>torch.save</code> (which uses <code>pickle</code> for the non-pytorch properties). This means that lambda-functions are not supported (unless a custom pickle module is used, see <code>torch.save</code>).</p> <p>Note that the random state is not saved, only the initial seed (if set).</p> <p>The default file extension is \".sm\".</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Union[str, PathLike, BinaryIO]</code> <p>Either a Path-like object or a (writable) File-like object.</p> required <code>any_extension</code> <code>bool</code> <p>Do not check the file extension. Defaults to False.</p> <code>False</code> <code>compress</code> <code>Union[bool, int]</code> <p>Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Parameters forwarded to <code>torch.save</code>.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def save(\n    self,\n    f: Union[str, PathLike, BinaryIO],\n    any_extension: bool = False,\n    compress: Union[bool, int] = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the Slisemap object to a file.\n\n    This method uses `torch.save` (which uses `pickle` for the non-pytorch properties).\n    This means that lambda-functions are not supported (unless a custom pickle module is used, see `torch.save`).\n\n    Note that the random state is not saved, only the initial seed (if set).\n\n    The default file extension is \".sm\".\n\n    Args:\n        f: Either a Path-like object or a (writable) File-like object.\n        any_extension: Do not check the file extension. Defaults to False.\n        compress: Compress the file with LZMA. Either a bool or a compression preset [0, 9]. Defaults to True.\n\n    Keyword Args:\n        **kwargs: Parameters forwarded to `torch.save`.\n    \"\"\"\n    if not any_extension and isinstance(f, (str, PathLike)):  # noqa: SIM102\n        if not str(f).endswith(\".sm\"):\n            _warn(\n                \"When saving Slisemap objects, consider using the '.sm' extension for consistency.\",\n                Slisemap.save,\n            )\n    loss = self._loss\n    prng = self._random_state\n    try:\n        self.metadata.root = None\n        self._B = self._B.detach()\n        self._Z = self._Z.detach()\n        self._loss = None\n        self._random_state = None\n        if isinstance(compress, int) and compress &gt; 0:\n            with lzma.open(f, \"wb\", preset=compress) as f2:\n                torch.save(self, f2, **kwargs)\n        elif compress:\n            with lzma.open(f, \"wb\") as f2:\n                torch.save(self, f2, **kwargs)\n        else:\n            torch.save(self, f, **kwargs)\n    finally:\n        self.metadata.root = self\n        self._loss = loss\n        self._random_state = prng\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.load","title":"<code>load(f, device=None, *, map_location=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load a Slisemap object from a file.</p> <p>This function uses <code>torch.load</code>, so the tensors are restored to their previous devices. Use <code>device=\"cpu\"</code> to avoid assuming that the same device exists. This is useful if the Slisemap object has been trained on a GPU, but the current computer lacks a GPU.</p> <p>Note that this is a classmethod, use it with: <code>Slisemap.load(...)</code>.</p> <p>SAFETY: This function is based on <code>torch.load</code> which (by default) uses <code>pickle</code>. Do not use <code>Slisemap.load</code> on untrusted files, since <code>pickle</code> can run arbitrary Python code.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Union[str, PathLike, BinaryIO]</code> <p>Either a Path-like object or a (readable) File-like object.</p> required <code>device</code> <code>Union[None, str, device]</code> <p>Device to load the tensors to (or the original if None). Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>map_location</code> <code>Optional[object]</code> <p>The same as <code>device</code> (this is the name used by <code>torch.load</code>). Defaults to None.</p> <code>**kwargs</code> <code>Any</code> <p>Parameters forwarded to <code>torch.load</code>.</p> <p>Returns:</p> Type Description <code>Slisemap</code> <p>The loaded Slisemap object.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    f: Union[str, PathLike, BinaryIO],\n    device: Union[None, str, torch.device] = None,\n    *,\n    map_location: Optional[object] = None,\n    **kwargs: Any,\n) -&gt; \"Slisemap\":\n    \"\"\"Load a Slisemap object from a file.\n\n    This function uses `torch.load`, so the tensors are restored to their previous devices.\n    Use `device=\"cpu\"` to avoid assuming that the same device exists.\n    This is useful if the Slisemap object has been trained on a GPU, but the current computer lacks a GPU.\n\n    Note that this is a classmethod, use it with: `Slisemap.load(...)`.\n\n    SAFETY: This function is based on `torch.load` which (by default) uses `pickle`.\n    Do not use `Slisemap.load` on untrusted files, since `pickle` can run arbitrary Python code.\n\n    Args:\n        f: Either a Path-like object or a (readable) File-like object.\n        device: Device to load the tensors to (or the original if None). Defaults to None.\n\n    Keyword Args:\n        map_location: The same as `device` (this is the name used by `torch.load`). Defaults to None.\n        **kwargs: Parameters forwarded to `torch.load`.\n\n    Returns:\n        The loaded Slisemap object.\n    \"\"\"\n    if device is None:\n        device = map_location\n    try:\n        with lzma.open(f, \"rb\") as f2:\n            sm = torch.load(f2, map_location=device, **kwargs)\n    except lzma.LZMAError:\n        sm: Slisemap = torch.load(f, map_location=device, **kwargs)\n    return sm\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.get_model_clusters","title":"<code>get_model_clusters(clusters, B=None, Z=None, random_state=42, **kwargs)</code>","text":"<p>Cluster the local model coefficients using k-means (from scikit-learn).</p> <p>This method (with a fixed random seed) is used for plotting Slisemap solutions.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>int</code> <p>Number of clusters.</p> required <code>B</code> <code>Optional[ndarray]</code> <p>B matrix. Defaults to <code>self.get_B()</code>.</p> <code>None</code> <code>Z</code> <code>Optional[ndarray]</code> <p>Z matrix. Defaults to <code>self.get_Z(rotate=True)</code>.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>random_state for the KMeans clustering. Defaults to 42.</p> <code>42</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>sklearn.cluster.KMeans</code> or <code>sklearn.cluster.MiniBatchKMeans</code> if <code>self.n &gt;= 1024</code>.</p> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray</code> <p>Vector of cluster labels.</p> <code>centres</code> <code>ndarray</code> <p>Matrix of cluster centres.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def get_model_clusters(\n    self,\n    clusters: int,\n    B: Optional[np.ndarray] = None,\n    Z: Optional[np.ndarray] = None,\n    random_state: int = 42,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Cluster the local model coefficients using k-means (from scikit-learn).\n\n    This method (with a fixed random seed) is used for plotting Slisemap solutions.\n\n    Args:\n        clusters: Number of clusters.\n        B: B matrix. Defaults to `self.get_B()`.\n        Z: Z matrix. Defaults to `self.get_Z(rotate=True)`.\n        random_state: random_state for the KMeans clustering. Defaults to 42.\n\n    Keyword Args:\n        **kwargs: Additional arguments to `sklearn.cluster.KMeans` or `sklearn.cluster.MiniBatchKMeans` if `self.n &gt;= 1024`.\n\n    Returns:\n        labels: Vector of cluster labels.\n        centres: Matrix of cluster centres.\n    \"\"\"\n    B = B if B is not None else self.get_B()\n    Z = Z if Z is not None else self.get_Z(rotate=True)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", FutureWarning)\n        # Some sklearn versions warn about changing defaults for KMeans\n        kwargs.setdefault(\"random_state\", random_state)\n        if self.n &gt;= 1024:\n            km = MiniBatchKMeans(clusters, **kwargs).fit(B)\n        else:\n            km = KMeans(clusters, **kwargs).fit(B)\n    ord = np.argsort([Z[km.labels_ == k, 0].mean() for k in range(clusters)])\n    return np.argsort(ord)[km.labels_], km.cluster_centers_[ord]\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.plot","title":"<code>plot(title='', clusters=None, bars=True, jitter=0.0, show=True, bar=None, *, B=None, Z=None, variables=None, targets=None, **kwargs)</code>","text":"<p>Plot the Slisemap solution using seaborn.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>clusters</code> <code>Union[None, int, ndarray]</code> <p>Can be None (plot individual losses), an int (plot k-means clusters of B), or an array of known cluster id:s. Defaults to None.</p> <code>None</code> <code>bars</code> <code>Union[bool, int, Sequence[str]]</code> <p>Plot the local models in a bar plot. Either an int (to only plot the most influential variables), a list of variables, or a bool. Defaults to True.</p> <code>True</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.</p> <code>0.0</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <code>bar</code> <code>Union[None, bool, int]</code> <p>Alternative spelling for <code>bars</code>. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>B</code> <code>Optional[ndarray]</code> <p>Override self.get_B() in the plot. Defaults to None. DEPRECATED</p> <code>Z</code> <code>Optional[ndarray]</code> <p>Override self.get_Z() in the plot. Defaults to None. DEPRECATED</p> <code>variables</code> <code>Optional[Sequence[str]]</code> <p>List of variable names. Defaults to None. DEPRECATED</p> <code>targets</code> <code>Union[None, str, Sequence[str]]</code> <p>Target name(s). Defaults to None. DEPRECATED</p> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to plot_solution and <code>plt.subplots</code>.</p> <p>Returns:</p> Type Description <code>Optional[Figure]</code> <p><code>matplotlib.figure.Figure</code> if <code>show=False</code>.</p> Deprecated <p>1.3: Parameter <code>variables</code>, use <code>metadata.set_variables()</code> instead! 1.3: Parameter <code>targets</code>, use <code>metadata.set_targets()</code> instead! 1.3: Parameter <code>B</code>. 1.3: Parameter <code>Z</code>.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def plot(\n    self,\n    title: str = \"\",\n    clusters: Union[None, int, np.ndarray] = None,\n    bars: Union[bool, int, Sequence[str]] = True,\n    jitter: Union[float, np.ndarray] = 0.0,\n    show: bool = True,\n    bar: Union[None, bool, int] = None,\n    *,\n    B: Optional[np.ndarray] = None,\n    Z: Optional[np.ndarray] = None,\n    variables: Optional[Sequence[str]] = None,\n    targets: Union[None, str, Sequence[str]] = None,\n    **kwargs: Any,\n) -&gt; Optional[Figure]:\n    \"\"\"Plot the Slisemap solution using seaborn.\n\n    Args:\n        title: Title of the plot. Defaults to \"\".\n        clusters: Can be None (plot individual losses), an int (plot k-means clusters of B), or an array of known cluster id:s. Defaults to None.\n        bars: Plot the local models in a bar plot. Either an int (to only plot the most influential variables), a list of variables, or a bool. Defaults to True.\n        jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n        show: Show the plot. Defaults to True.\n        bar: Alternative spelling for `bars`. Defaults to None.\n\n    Keyword Args:\n        B: Override self.get_B() in the plot. Defaults to None. **DEPRECATED**\n        Z: Override self.get_Z() in the plot. Defaults to None. **DEPRECATED**\n        variables: List of variable names. Defaults to None. **DEPRECATED**\n        targets: Target name(s). Defaults to None. **DEPRECATED**\n        **kwargs: Additional arguments to [plot_solution][slisemap.plot.plot_solution] and `plt.subplots`.\n\n    Returns:\n        `matplotlib.figure.Figure` if `show=False`.\n\n    Deprecated:\n        1.3: Parameter `variables`, use `metadata.set_variables()` instead!\n        1.3: Parameter `targets`, use `metadata.set_targets()` instead!\n        1.3: Parameter `B`.\n        1.3: Parameter `Z`.\n    \"\"\"\n    if bar is not None:\n        bars = bar\n    if Z is None:\n        Z = self.get_Z(rotate=True)\n    else:\n        _deprecated(\"Parameter 'Z' in Slisemap.plot\")\n    if B is None:\n        B = self.get_B()\n    else:\n        _deprecated(\"Parameter 'B' in Slisemap.plot\")\n    dimensions = self.metadata.get_dimensions(long=True)\n    if variables is not None:\n        _deprecated(\n            \"Parameter 'variables' in 'Slisemap.plot'\",\n            \"'Slisemap.metadata.set_variables'\",\n        )\n        coefficients = _expand_variable_names(\n            variables, self.intercept, self.m, targets, self.q\n        )\n    else:\n        coefficients = self.metadata.get_coefficients()\n    if targets is not None:\n        _deprecated(\n            \"Parameter 'targets' in 'Slisemap.plot'\",\n            \"'Slisemap.metadata.set_targets'\",\n        )\n\n    loss = None\n    centers = None\n    if clusters is None:\n        if Z.shape[0] == self._Z.shape[0]:\n            loss = tonp(self.local_loss(self.predict(numpy=False), self._Y))\n    else:\n        if isinstance(clusters, int):\n            clusters, centers = self.get_model_clusters(clusters, B, Z)\n        else:\n            clusters = np.asarray(clusters)\n            centers = np.stack(\n                [np.mean(B[clusters == c, :], 0) for c in np.unique(clusters)], 0\n            )\n    fig = plot_solution(\n        Z=Z,\n        B=B,\n        loss=loss,\n        clusters=clusters,\n        centers=centers,\n        coefficients=coefficients,\n        dimensions=dimensions,\n        title=title,\n        bars=bars,\n        jitter=jitter,\n        **kwargs,\n    )\n    if show:\n        plt.show()\n    else:\n        return fig\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.plot_position","title":"<code>plot_position(X=None, Y=None, index=None, title='', jitter=0.0, selection=True, legend_inside=True, show=True, *, Z=None, **kwargs)</code>","text":"<p>Plot local losses for alternative locations for the selected item(s).</p> <p>Indicate the selected item(s) either via <code>X</code> and <code>Y</code> or via <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ToTensor]</code> <p>Data matrix for the selected data item(s). Defaults to None.</p> <code>None</code> <code>Y</code> <code>Optional[ToTensor]</code> <p>Response matrix for the selected data item(s). Defaults to None.</p> <code>None</code> <code>index</code> <code>Union[None, int, Sequence[int]]</code> <p>Index/indices of the selected data item(s). Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>jitter</code> <code>Union[float, ndarray]</code> <p>Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.</p> <code>0.0</code> <code>selection</code> <code>bool</code> <p>Mark the selected data item(s), if index is given. Defaults to True.</p> <code>True</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside the grid (if there is an empty cell). Defaults to True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>Z</code> <code>Optional[ndarray]</code> <p>Override <code>self.get_Z()</code> in the plot. Defaults to None. DEPRECATED</p> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code>.</p> <p>Returns:</p> Type Description <code>Optional[FacetGrid]</code> <p><code>seaborn.FacetGrid</code> if <code>show=False</code>.</p> Deprecated <p>1.3: Parameter <code>Z</code>.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def plot_position(\n    self,\n    X: Optional[ToTensor] = None,\n    Y: Optional[ToTensor] = None,\n    index: Union[None, int, Sequence[int]] = None,\n    title: str = \"\",\n    jitter: Union[float, np.ndarray] = 0.0,\n    selection: bool = True,\n    legend_inside: bool = True,\n    show: bool = True,\n    *,\n    Z: Optional[np.ndarray] = None,\n    **kwargs: Any,\n) -&gt; Optional[sns.FacetGrid]:\n    \"\"\"Plot local losses for alternative locations for the selected item(s).\n\n    Indicate the selected item(s) either via `X` and `Y` or via `index`.\n\n    Args:\n        X: Data matrix for the selected data item(s). Defaults to None.\n        Y: Response matrix for the selected data item(s). Defaults to None.\n        index: Index/indices of the selected data item(s). Defaults to None.\n        title: Title of the plot. Defaults to \"\".\n        jitter: Add random (normal) noise to the embedding, or a matrix with pre-generated noise matching Z. Defaults to 0.0.\n        selection: Mark the selected data item(s), if index is given. Defaults to True.\n        legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n        show: Show the plot. Defaults to True.\n\n    Keyword Args:\n        Z: Override `self.get_Z()` in the plot. Defaults to None. **DEPRECATED**\n        **kwargs: Additional arguments to `seaborn.relplot`.\n\n    Returns:\n        `seaborn.FacetGrid` if `show=False`.\n\n    Deprecated:\n        1.3: Parameter `Z`.\n    \"\"\"\n    if Z is not None:\n        _deprecated(\"Parameter 'Z' in Slisemap.plot_position\")\n    else:\n        Z = self.get_Z(rotate=True)\n    if index is None:\n        _assert(\n            X is not None and Y is not None,\n            \"Either index or X and Y must be given\",\n            Slisemap.plot_position,\n        )\n        L = self.get_L(X=X, Y=Y)\n    else:\n        if isinstance(index, int):\n            index = [index]\n        L = self.get_L()[:, index]\n    g = plot_position(\n        Z=Z,\n        L=L,\n        Zs=Z[index, :] if selection and index is not None else None,\n        dimensions=self.metadata.get_dimensions(long=True),\n        title=title,\n        jitter=jitter,\n        legend_inside=legend_inside,\n        **kwargs,\n    )\n    if show:\n        plt.show()\n    else:\n        return g\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.Slisemap.plot_dist","title":"<code>plot_dist(title='', clusters=None, unscale=True, scatter=False, jitter=0.0, legend_inside=True, show=True, *, X=None, Y=None, B=None, variables=None, targets=None, **kwargs)</code>","text":"<p>Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"\".</p> <code>''</code> <code>clusters</code> <code>Union[None, int, ndarray]</code> <p>Number of cluster or vector of cluster labels. Defaults to None.</p> <code>None</code> <code>scatter</code> <code>bool</code> <p>Use scatterplots instead of density plots (clusters are ignored). Defaults to False.</p> <code>False</code> <code>unscale</code> <code>bool</code> <p>Unscale <code>X</code> and <code>Y</code> if scaling metadata has been given (see <code>Slisemap.metadata.set_scale_X</code>). Defaults to True.</p> <code>True</code> <code>jitter</code> <code>float</code> <p>Add jitter to the scatterplots. Defaults to 0.0.</p> <code>0.0</code> <code>legend_inside</code> <code>bool</code> <p>Move the legend inside the grid (if there is an empty cell). Defaults to True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Show the plot. Defaults to True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>X</code> <code>Optional[ndarray]</code> <p>Override self.get_X(). Defaults to None. DEPRECATED</p> <code>Y</code> <code>Optional[ndarray]</code> <p>Override self.get_Y(). Defaults to None. DEPRECATED</p> <code>B</code> <code>Optional[ndarray]</code> <p>Override self.get_B() when finding the clusters (only used if clusters is an int). Defaults to None. DEPRECATED</p> <code>variables</code> <code>Optional[List[str]]</code> <p>List of variable names. Defaults to None. DEPRECATED</p> <code>targets</code> <code>Union[None, str, Sequence[str]]</code> <p>Target name(s). Defaults to None. DEPRECATED</p> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to <code>seaborn.relplot</code> or <code>seaborn.scatterplot</code>.</p> <p>Returns:</p> Type Description <code>Optional[FacetGrid]</code> <p><code>seaborn.FacetGrid</code> if <code>show=False</code>.</p> Deprecated <p>1.3: Parameter <code>variables</code>, use <code>metadata.set_variables()</code> instead! 1.3: Parameter <code>targets</code>, use <code>metadata.set_targets()</code> instead! 1.3: Parameter <code>X</code>, use <code>metadata.set_scale_X()</code> instead (to automatically unscale)! 1.3: Parameter <code>Y</code>, use <code>metadata.set_scale_Y()</code> instead (to automatically unscale)! 1.3: Parameter <code>B</code>.</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def plot_dist(\n    self,\n    title: str = \"\",\n    clusters: Union[None, int, np.ndarray] = None,\n    unscale: bool = True,\n    scatter: bool = False,\n    jitter: float = 0.0,\n    legend_inside: bool = True,\n    show: bool = True,\n    *,\n    X: Optional[np.ndarray] = None,\n    Y: Optional[np.ndarray] = None,\n    B: Optional[np.ndarray] = None,\n    variables: Optional[List[str]] = None,\n    targets: Union[None, str, Sequence[str]] = None,\n    **kwargs: Any,\n) -&gt; Optional[sns.FacetGrid]:\n    \"\"\"Plot the distribution of the variables, either as density plots (with clusters) or as scatterplots.\n\n    Args:\n        title: Title of the plot. Defaults to \"\".\n        clusters: Number of cluster or vector of cluster labels. Defaults to None.\n        scatter: Use scatterplots instead of density plots (clusters are ignored). Defaults to False.\n        unscale: Unscale `X` and `Y` if scaling metadata has been given (see `Slisemap.metadata.set_scale_X`). Defaults to True.\n        jitter: Add jitter to the scatterplots. Defaults to 0.0.\n        legend_inside: Move the legend inside the grid (if there is an empty cell). Defaults to True.\n        show: Show the plot. Defaults to True.\n\n    Keyword Args:\n        X: Override self.get_X(). Defaults to None. **DEPRECATED**\n        Y: Override self.get_Y(). Defaults to None. **DEPRECATED**\n        B: Override self.get_B() when finding the clusters (only used if clusters is an int). Defaults to None. **DEPRECATED**\n        variables: List of variable names. Defaults to None. **DEPRECATED**\n        targets: Target name(s). Defaults to None. **DEPRECATED**\n        **kwargs: Additional arguments to `seaborn.relplot` or `seaborn.scatterplot`.\n\n    Returns:\n        `seaborn.FacetGrid` if `show=False`.\n\n    Deprecated:\n        1.3: Parameter `variables`, use `metadata.set_variables()` instead!\n        1.3: Parameter `targets`, use `metadata.set_targets()` instead!\n        1.3: Parameter `X`, use `metadata.set_scale_X()` instead (to automatically unscale)!\n        1.3: Parameter `Y`, use `metadata.set_scale_Y()` instead (to automatically unscale)!\n        1.3: Parameter `B`.\n    \"\"\"\n    if X is None:\n        X = self.get_X(intercept=False)\n    else:\n        _deprecated(\"Parameter 'X' in Slisemap.plot_dist\")\n    if Y is None:\n        Y = self.get_Y()\n    else:\n        _deprecated(\"Parameter 'Y' in Slisemap.plot_dist\")\n        Y = np.reshape(Y, (X.shape[0], -1))\n    if isinstance(X, torch.Tensor):\n        X = tonp(X)\n    if isinstance(Y, torch.Tensor):\n        Y = tonp(Y)\n    if unscale:\n        X = self.metadata.unscale_X(X)\n        Y = self.metadata.unscale_Y(Y)\n    if variables is None:\n        variables = self.metadata.get_variables(intercept=False)\n    else:\n        _deprecated(\n            \"Parameter 'variables' in 'Slisemap.plot_dist'\",\n            \"'Slisemap.metadata.set_variables'\",\n        )\n    if targets is not None:\n        _deprecated(\n            \"Parameter 'targets' in 'Slisemap.plot_dist'\",\n            \"'Slisemap.metadata.set_targets'\",\n        )\n        if isinstance(targets, str):\n            targets = [targets]\n    else:\n        targets = self.metadata.get_targets()\n    if B is not None:\n        _deprecated(\"Parameter 'B' in Slisemap.plot_dist\")\n    if isinstance(clusters, int):\n        clusters, _ = self.get_model_clusters(clusters, B)\n    loss = tonp(self.local_loss(self.predict(numpy=False), self._Y))\n\n    g = plot_dist(\n        X=X,\n        Y=Y,\n        Z=self.get_Z(),\n        loss=loss,\n        variables=self.metadata.get_variables(False),\n        targets=self.metadata.get_targets(),\n        dimensions=self.metadata.get_dimensions(long=True),\n        title=title,\n        clusters=clusters,\n        scatter=scatter,\n        jitter=jitter,\n        legend_inside=legend_inside,\n        **kwargs,\n    )\n    if show:\n        plt.show()\n    else:\n        return g\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.make_loss","title":"<code>make_loss(local_model, local_loss, distance=torch.cdist, kernel=softmax_row_kernel, radius=3.5, lasso=0.0, ridge=0.0, z_norm=1.0, individual=False, regularisation=None)</code>","text":"<p>Create a loss function for Slisemap to optimise.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function. Defaults to <code>torch.cdist</code> (Euclidean distance).</p> <code>cdist</code> <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances, Defaults to <code>softmax_kernel</code>.</p> <code>softmax_row_kernel</code> <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>lasso</code> <code>float</code> <p>Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.</p> <code>0.0</code> <code>ridge</code> <code>float</code> <p>Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.</p> <code>0.0</code> <code>z_norm</code> <code>float</code> <p>Z normalisation regularisation coefficient ($\\lambda_{norm} * (sum(Z^2)-n)^2$). Defaults to 1.0.</p> <code>1.0</code> <code>individual</code> <code>bool</code> <p>Return individual (row-wise) losses. Defaults to False.</p> <code>False</code> <code>regularisation</code> <code>Optional[Callable]</code> <p>Additional loss function. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Tensor, Tensor, Tensor, Tensor], Tensor]</code> <p>Loss function for SLISEMAP</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def make_loss(\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = torch.cdist,\n    kernel: Callable[[torch.Tensor], torch.Tensor] = softmax_row_kernel,\n    radius: float = 3.5,\n    lasso: float = 0.0,\n    ridge: float = 0.0,\n    z_norm: float = 1.0,\n    individual: bool = False,\n    regularisation: Optional[Callable] = None,\n) -&gt; Callable[[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n    r\"\"\"Create a loss function for Slisemap to optimise.\n\n    Args:\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function. Defaults to `torch.cdist` (Euclidean distance).\n        kernel: Kernel for embedding distances, Defaults to `softmax_kernel`.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        lasso: Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.\n        ridge: Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.\n        z_norm: Z normalisation regularisation coefficient ($\\\\lambda_{norm} * (sum(Z^2)-n)^2$). Defaults to 1.0.\n        individual: Return individual (row-wise) losses. Defaults to False.\n        regularisation: Additional loss function. Defaults to None.\n\n    Returns:\n        Loss function for SLISEMAP\n    \"\"\"\n    dim = 1 if individual else ()\n    if individual and z_norm &gt; 0:\n        _warn(\n            \"The Z normalisation is added to every individual loss if z_norm &gt; 0\",\n            make_loss,\n        )\n\n    def loss_fn(\n        X: torch.Tensor,\n        Y: torch.Tensor,\n        B: torch.Tensor,\n        Z: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Slisemap loss function.\n\n        Args:\n            X: Data matrix [n, m].\n            Y: Target matrix [n, k].\n            B: Local models [n, p].\n            Z: Embedding matrix [n, d].\n\n        Returns:\n            The loss value.\n        \"\"\"\n        if radius &gt; 0:\n            Zss = torch.sum(Z**2)\n            Z = Z * (radius / (torch.sqrt(Zss / Z.shape[0]) + 1e-8))\n        D = distance(Z, Z)\n        Ytilde = local_model(X, B)\n        L = local_loss(Ytilde, Y)\n        loss = torch.sum(kernel(D) * L, dim=dim)\n        if lasso &gt; 0:\n            loss += lasso * torch.sum(B.abs(), dim=dim)\n        if ridge &gt; 0:\n            loss += ridge * torch.sum(B**2, dim=dim)\n        if z_norm &gt; 0 and radius &gt; 0:\n            loss += z_norm * (Zss - Z.shape[0]) ** 2\n        if regularisation is not None:\n            loss += regularisation(X, Y, B, Z, Ytilde)\n        return loss\n\n    return loss_fn\n</code></pre>"},{"location":"slisemap.slisemap/#slisemap.slisemap.make_marginal_loss","title":"<code>make_marginal_loss(X, Y, B, Z, Xnew, Ynew, local_model, local_loss, distance=torch.cdist, kernel=softmax_row_kernel, radius=3.5, lasso=0.0, ridge=0.0, jit=True)</code>","text":"<p>Create a loss for adding new points with Slisemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The existing data matrix [n_old, m].</p> required <code>Y</code> <code>Tensor</code> <p>The existing target matrix [n_old, k].</p> required <code>B</code> <code>Tensor</code> <p>The fitted models [n_old, p].</p> required <code>Z</code> <code>Tensor</code> <p>The fitted embedding [n_old, d].</p> required <code>Xnew</code> <code>Tensor</code> <p>The new data matrix [n_new, m].</p> required <code>Ynew</code> <code>Tensor</code> <p>The new target matrix [n_new, k].</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the local models.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Loss function for the local models.</p> required <code>distance</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Embedding distance function. Defaults to <code>torch.cdist</code> (Euclidean distance).</p> <code>cdist</code> <code>kernel</code> <code>Callable[[Tensor], Tensor]</code> <p>Kernel for embedding distances, Defaults to <code>softmax_kernel</code>.</p> <code>softmax_row_kernel</code> <code>radius</code> <code>float</code> <p>For enforcing the radius of Z. Defaults to 3.5.</p> <code>3.5</code> <code>lasso</code> <code>float</code> <p>Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.</p> <code>0.0</code> <code>ridge</code> <code>float</code> <p>Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.</p> <code>0.0</code> <code>jit</code> <code>bool</code> <p>Just-In-Time compile the loss function. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>A marginal loss function that takes Bnew [n_new, p] and Znew [n_new, d].</p> <code>set_new</code> <code>Callable[[Tensor, Tensor], None]</code> <p>A function for changing the Xnew [n_new, m] and Ynew [n_new, k].</p> Source code in <code>slisemap/slisemap.py</code> <pre><code>def make_marginal_loss(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    B: torch.Tensor,\n    Z: torch.Tensor,\n    Xnew: torch.Tensor,\n    Ynew: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    distance: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = torch.cdist,\n    kernel: Callable[[torch.Tensor], torch.Tensor] = softmax_row_kernel,\n    radius: float = 3.5,\n    lasso: float = 0.0,\n    ridge: float = 0.0,\n    jit: bool = True,\n) -&gt; Tuple[\n    Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    Callable[[torch.Tensor, torch.Tensor], None],\n]:\n    r\"\"\"Create a loss for adding new points with Slisemap.\n\n    Args:\n        X: The existing data matrix [n_old, m].\n        Y: The existing target matrix [n_old, k].\n        B: The fitted models [n_old, p].\n        Z: The fitted embedding [n_old, d].\n        Xnew: The new data matrix [n_new, m].\n        Ynew: The new target matrix [n_new, k].\n        local_model: Prediction function for the local models.\n        local_loss: Loss function for the local models.\n        distance: Embedding distance function. Defaults to `torch.cdist` (Euclidean distance).\n        kernel: Kernel for embedding distances, Defaults to `softmax_kernel`.\n        radius: For enforcing the radius of Z. Defaults to 3.5.\n        lasso: Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.\n        ridge: Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.\n        jit: Just-In-Time compile the loss function. Defaults to True.\n\n    Returns:\n        loss: A marginal loss function that takes Bnew [n_new, p] and Znew [n_new, d].\n        set_new: A function for changing the Xnew [n_new, m] and Ynew [n_new, k].\n    \"\"\"\n    Xcomb = torch.cat((X, Xnew), 0)\n    Ycomb = torch.cat((Y, Ynew), 0)\n    Nold = X.shape[0]\n    L0 = local_loss(local_model(Xcomb, B), Ycomb)  # Nold x Ncomb\n    D0 = distance(Z, Z)  # Nold x Nold\n\n    def set_new(Xnew: torch.Tensor, Ynew: torch.Tensor) -&gt; None:\n        \"\"\"Set the Xnew and Ynew for the generated marginal Slisemap loss function.\n\n        Args:\n            Xnew: New data matrix [n_new, m].\n            Ynew: New target matrix [n_new, k].\n        \"\"\"\n        nonlocal Xcomb, Ycomb, L0\n        Xcomb[Nold:] = Xnew\n        Ycomb[Nold:] = Ynew\n        L0[:, Nold:] = local_loss(local_model(Xnew, B), Ynew)\n\n    if radius &gt; 0:\n        Zss0 = torch.sum(Z**2)\n\n    def loss(Bnew: torch.Tensor, Znew: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Marginal Slisemap loss.\n\n        Args:\n            Bnew: New local models [n_new, p].\n            Znew: New embedding matrix [n_new, d].\n\n        Returns:\n            The marginal loss value.\n        \"\"\"\n        L1 = local_loss(local_model(Xcomb, Bnew), Ycomb)  # Nnew x Ncomb\n        L = torch.cat((L0, L1), 0)  # Ncomb x Ncomb\n\n        D1 = distance(Znew, Z)  # Nnew x Nold\n        D2 = distance(Znew, Znew)  # Nnew x Nnew\n        D3 = D1.transpose(0, 1)\n        D = torch.cat(\n            (torch.cat((D0, D1), 0), torch.cat((D3, D2), 0)), 1\n        )  # Ncomb x Ncomb\n        if radius &gt; 0:\n            Zss = Zss0 + torch.sum(Znew**2)\n            Ncomb = Z.shape[0] + Znew.shape[0]\n            norm = radius / (torch.sqrt(Zss / Ncomb) + 1e-8)\n            D = D * norm\n\n        kD = kernel(D)\n        a = torch.sum(kD * L)\n        if lasso &gt; 0:\n            a += lasso * torch.sum(Bnew.abs())\n        if ridge &gt; 0:\n            a += ridge * torch.sum(Bnew**2)\n        return a\n\n    if jit:\n        Nnew = Xnew.shape[0]\n        loss = torch.jit.trace(loss, (B[:1].expand(Nnew, -1), Z[:1].expand(Nnew, -1)))\n    return loss, set_new\n</code></pre>"},{"location":"slisemap.tuning/","title":"slisemap.tuning","text":""},{"location":"slisemap.tuning/#slisemap.tuning","title":"<code>slisemap.tuning</code>","text":"<p>Find optimal hyper-parameters for Slisemap and Slipmap.</p>"},{"location":"slisemap.tuning/#slisemap.tuning.hyperparameter_tune","title":"<code>hyperparameter_tune(method, X, y, X_test, y_test, lasso=(0.001, 10.0), ridge=(0.0001, 1.0), radius=(1.5, 4.0), *args, model=True, n_calls=15, verbose=False, random_state=42, predict_kws={}, optim_kws={}, gp_kws={}, **kwargs)</code>","text":"<p>Tune the <code>lasso</code>, <code>ridge</code>, and <code>radius</code> hyperparameters using Bayesian optimisation.</p> <p>The search space is configured through the <code>lasso</code>/<code>ridge</code>/<code>radius</code> arguments as follows:     - float: Skip the tuning of that hyperparameter.     - tuple: tune the parameters limited to the space of <code>(lowerbound, upperbound)</code>.</p> <p>This function selects a candidate set of hyperparameters using <code>skopt.gp_minimize</code>. For a given set of hyperparameters, a Slisemap/Slipmap model is trained on <code>X</code> and <code>y</code>. Then the solution is evaluated using <code>X_test</code> and <code>y_test</code>. This procedure is repeated for <code>n_calls</code> iterations before the best result is returned.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Union[Type[Slisemap], Type[Slipmap]]</code> <p>Method to tune, either <code>Slisemap</code> or <code>Slipmap</code>.</p> required <code>X</code> <code>ToTensor</code> <p>Data matrix.</p> required <code>y</code> <code>ToTensor</code> <p>target matrix.</p> required <code>X_test</code> <code>ToTensor</code> <p>New data for evaluation.</p> required <code>y_test</code> <code>ToTensor</code> <p>New data for evaluation.</p> required <code>lasso</code> <code>Union[float, Tuple[float, float]]</code> <p>Limits for the <code>lasso</code> parameter. Defaults to (0.001, 10.0).</p> <code>(0.001, 10.0)</code> <code>ridge</code> <code>Union[float, Tuple[float, float]]</code> <p>Limits for the <code>ridge</code> parameter. Defaults to (0.0001, 1.0).</p> <code>(0.0001, 1.0)</code> <code>radius</code> <code>Union[float, Tuple[float, float]]</code> <p>Limits for the <code>radius</code> parameter. Defaults to (1.5, 4.0).</p> <code>(1.5, 4.0)</code> <code>*args</code> <code>Any</code> <p>Arguments forwarded to <code>method</code>.</p> <code>()</code> <p>Other Parameters:</p> Name Type Description <code>model</code> <code>bool</code> <p>Return a trained model instead of a dictionary with tuned parameters. Defaults to True.</p> <code>n_calls</code> <code>int</code> <p>Number of parameter evaluations. Defaults to 15.</p> <code>verbose</code> <code>bool</code> <p>Print status messages. Defaults to False.</p> <code>random_state</code> <code>int</code> <p>Random seed. Defaults to 42.</p> <code>predict_kws</code> <code>Dict[str, object]</code> <p>Keyword arguments forwarded to <code>sm.predict</code>.</p> <code>optim_kws</code> <code>Dict[str, object]</code> <p>Keyword arguments forwarded to <code>sm.optimise</code>.</p> <code>gp_kws</code> <code>Dict[str, object]</code> <p>Keyword arguments forwarded to <code>skopt.gp_minimize</code>.</p> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>method</code>.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>scikit-optimize</code> is not installed.</p> <p>Returns:</p> Type Description <code>Union[Slisemap, Slipmap, Dict[str, float]]</code> <p>Dictionary with hyperparameter values or a Slisemap/Slipmap model trained on those (see the <code>model</code> argument).</p> Source code in <code>slisemap/tuning.py</code> <pre><code>def hyperparameter_tune(\n    method: Union[Type[Slisemap], Type[Slipmap]],\n    X: ToTensor,\n    y: ToTensor,\n    X_test: ToTensor,\n    y_test: ToTensor,\n    lasso: Union[float, Tuple[float, float]] = (0.001, 10.0),\n    ridge: Union[float, Tuple[float, float]] = (0.0001, 1.0),\n    radius: Union[float, Tuple[float, float]] = (1.5, 4.0),\n    *args: Any,\n    model: bool = True,\n    n_calls: int = 15,\n    verbose: bool = False,\n    random_state: int = 42,\n    predict_kws: Dict[str, object] = {},\n    optim_kws: Dict[str, object] = {},\n    gp_kws: Dict[str, object] = {},\n    **kwargs: Any,\n) -&gt; Union[Slisemap, Slipmap, Dict[str, float]]:\n    \"\"\"Tune the `lasso`, `ridge`, and `radius` hyperparameters using Bayesian optimisation.\n\n    The search space is configured through the `lasso`/`ridge`/`radius` arguments as follows:\n        - float: Skip the tuning of that hyperparameter.\n        - tuple: tune the parameters limited to the space of `(lowerbound, upperbound)`.\n\n    This function selects a candidate set of hyperparameters using `skopt.gp_minimize`.\n    For a given set of hyperparameters, a Slisemap/Slipmap model is trained on `X` and `y`.\n    Then the solution is evaluated using `X_test` and `y_test`.\n    This procedure is repeated for `n_calls` iterations before the best result is returned.\n\n    Args:\n        method: Method to tune, either `Slisemap` or `Slipmap`.\n        X: Data matrix.\n        y: target matrix.\n        X_test: New data for evaluation.\n        y_test: New data for evaluation.\n        lasso: Limits for the `lasso` parameter. Defaults to (0.001, 10.0).\n        ridge: Limits for the `ridge` parameter. Defaults to (0.0001, 1.0).\n        radius: Limits for the `radius` parameter. Defaults to (1.5, 4.0).\n        *args: Arguments forwarded to `method`.\n\n    Keyword Args:\n        model: Return a trained model instead of a dictionary with tuned parameters. Defaults to True.\n        n_calls: Number of parameter evaluations. Defaults to 15.\n        verbose: Print status messages. Defaults to False.\n        random_state: Random seed. Defaults to 42.\n        predict_kws: Keyword arguments forwarded to `sm.predict`.\n        optim_kws: Keyword arguments forwarded to `sm.optimise`.\n        gp_kws: Keyword arguments forwarded to `skopt.gp_minimize`.\n        **kwargs: Keyword arguments forwarded to `method`.\n\n    Raises:\n        ImportError: If `scikit-optimize` is not installed.\n\n    Returns:\n        Dictionary with hyperparameter values or a Slisemap/Slipmap model trained on those (see the `model` argument).\n    \"\"\"\n    space = []\n    params = {}\n\n    def make_space(grid, name, prior):  # noqa: ANN001, ANN202\n        if isinstance(grid, (float, int)):\n            params[name] = grid\n        else:\n            _assert(\n                len(grid) == 2,\n                f\"Wrong size `len({name}) = {len(grid)} != 2`\",\n                hyperparameter_tune,\n            )\n            space.append(skopt.space.Real(*grid, prior=prior, name=name))\n            params[name] = (grid[0] * grid[1]) ** 0.5\n\n    make_space(lasso, \"lasso\", \"log-uniform\")\n    make_space(ridge, \"ridge\", \"log-uniform\")\n    make_space(radius, \"radius\", \"uniform\")\n    if len(space) == 0:\n        _warn(\"No hyperparameters to tune\", hyperparameter_tune)\n        if model:\n            sm = method(X, y, radius=radius, lasso=lasso, ridge=ridge, *args, **kwargs)  # noqa: B026\n            sm.optimise(**optim_kws)\n            return sm\n        else:\n            return params\n\n    if model:\n        best_loss = np.inf\n        best_sm = None\n\n    @skopt.utils.use_named_args(space)\n    @lru_cache\n    def objective(\n        lasso: float = params[\"lasso\"],\n        ridge: float = params[\"ridge\"],\n        radius: float = params[\"radius\"],\n    ) -&gt; float:\n        sm = method(X, y, radius=radius, lasso=lasso, ridge=ridge, *args, **kwargs)  # noqa: B026\n        sm.optimise(**optim_kws)\n        Xt = sm._as_new_X(X_test)\n        Yt = sm._as_new_Y(y_test, Xt.shape[0])\n        P = sm.predict(Xt, **predict_kws, numpy=False)\n        loss = sm.local_loss(Yt, P).mean().cpu().item()\n        if verbose:\n            print(\n                f\"Loss with { {'lasso': lasso, 'ridge': ridge, 'radius': radius} }: {loss}\"\n            )\n        if model:\n            nonlocal best_loss, best_sm\n            if loss &lt; best_loss:\n                best_sm = sm\n                best_loss = loss\n        del sm\n        return loss\n\n    res = skopt.gp_minimize(\n        objective,\n        space,\n        n_initial_points=min(10, max(3, (n_calls - 1) // 3 + 1)),\n        n_calls=n_calls,\n        random_state=random_state,\n        **gp_kws,\n    )\n    for s, v in zip(space, res.x):\n        params[s.name] = v\n    if verbose:\n        print(\"Final parameter values:\", params)\n\n    if model:\n        return best_sm\n    else:\n        return params\n</code></pre>"},{"location":"slisemap.tuning/#slisemap.tuning.optimise_with_test","title":"<code>optimise_with_test(sm, X_test, y_test, lasso_grid=3.0, ridge_grid=3.0, radius_grid=1.1, search_size=6, test=accuracy, patience=2, max_escapes=100, verbose=0, escape_kws={}, *, max_iterations=None, **kwargs)</code>","text":"<p>Optimise a Slisemap or Slipmap object using test data to tune the regularisation.</p> How this works <ul> <li>The procedure is very similar to Slisemap.optimise, which alternates between LBFGS optimisation and an \"escape\" heuristic until convergence.</li> <li>The hyperoptimisation tuning adds an additional step after each call to LBFGS where a small local search is performed to tune the hyperparameters.</li> <li>The convergence criteria is also changed to use the test data (see the <code>test</code> parameter).</li> <li>This should be faster than the usual \"outer-loop\" hyperperameter optimisation, but the local search dynamics might be less exhaustive.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Union[Slisemap, Slipmap]</code> <p>Slisemap or Slipmap object.</p> required <code>X_test</code> <code>Union[ndarray, Tensor]</code> <p>Data matrix for the test set.</p> required <code>y_test</code> <code>Union[ndarray, Tensor]</code> <p>Target matrix/vector for the test set.</p> required <code>lasso_grid</code> <code>float</code> <p>The extent of the local search for the lasso parameter <code>(lasso/lasso_grid, lasso*lasso_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 3.0.</p> <code>3.0</code> <code>ridge_grid</code> <code>float</code> <p>The extent of the local search for the ridge parameter <code>(ridge/ridge_grid, ridge*ridge_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 3.0.</p> <code>3.0</code> <code>radius_grid</code> <code>float</code> <p>The extent of the local search for the radius parameter <code>(radius/radius_grid, radius*radius_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 1.5.</p> <code>1.1</code> <code>search_size</code> <code>int</code> <p>The number of evaluations in the local random search. Defaults to 6.</p> <code>6</code> <code>test</code> <code>Callable[[Slisemap, Tensor, Tensor], float]</code> <p>Test to measure the performance of different hyperparameter values. Defaults to accuracy.</p> <code>accuracy</code> <code>patience</code> <code>int</code> <p>Number of optimisation rounds without improvement before stopping. Defaults to 2.</p> <code>2</code> <code>max_escapes</code> <code>int</code> <p>Maximum numbers optimisation rounds. Defaults to 100.</p> <code>100</code> <code>verbose</code> <code>Literal[0, 1, 2, 3]</code> <p>Print status messages. Defaults to 0.</p> <code>0</code> <code>escape_kws</code> <code>Dict[str, Any]</code> <p>Keyword arguments forwarded to sm.escape. Defaults to {}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to sm.lbfgs.</p> <p>Returns:</p> Type Description <code>Union[Slisemap, Slipmap]</code> <p>Optimised Slisemap or Slipmap object. This is not the same object as the input!</p> Deprecated <p>1.6: <code>max_iterations</code> renamed to <code>max_escapes</code></p> Source code in <code>slisemap/tuning.py</code> <pre><code>def optimise_with_test(  # noqa: D417\n    sm: Union[Slisemap, Slipmap],\n    X_test: Union[np.ndarray, torch.Tensor],\n    y_test: Union[np.ndarray, torch.Tensor],\n    lasso_grid: float = 3.0,\n    ridge_grid: float = 3.0,\n    radius_grid: float = 1.1,\n    search_size: int = 6,\n    test: Callable[[Slisemap, torch.Tensor, torch.Tensor], float] = accuracy,\n    patience: int = 2,\n    max_escapes: int = 100,\n    verbose: Literal[0, 1, 2, 3] = 0,\n    escape_kws: Dict[str, Any] = {},\n    *,\n    max_iterations: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Union[Slisemap, Slipmap]:\n    \"\"\"Optimise a Slisemap or Slipmap object using test data to tune the regularisation.\n\n    How this works:\n        - The procedure is very similar to [Slisemap.optimise][slisemap.slisemap.Slisemap.optimise], which alternates between [LBFGS][slisemap.slisemap.Slisemap.lbfgs] optimisation and an [\"escape\" heuristic][slisemap.slisemap.Slisemap.escape] until convergence.\n        - The hyperoptimisation tuning adds an additional step after each call to [LBFGS][slisemap.slisemap.Slisemap.lbfgs] where a small local search is performed to tune the hyperparameters.\n        - The convergence criteria is also changed to use the test data (see the `test` parameter).\n        - This should be faster than the usual \"outer-loop\" hyperperameter optimisation, but the local search dynamics might be less exhaustive.\n\n    Args:\n        sm: Slisemap or Slipmap object.\n        X_test: Data matrix for the test set.\n        y_test: Target matrix/vector for the test set.\n        lasso_grid: The extent of the local search for the lasso parameter `(lasso/lasso_grid, lasso*lasso_grid)`. Set to zero to disable the hyperparameter search. Defaults to 3.0.\n        ridge_grid: The extent of the local search for the ridge parameter `(ridge/ridge_grid, ridge*ridge_grid)`. Set to zero to disable the hyperparameter search. Defaults to 3.0.\n        radius_grid: The extent of the local search for the radius parameter `(radius/radius_grid, radius*radius_grid)`. Set to zero to disable the hyperparameter search. Defaults to 1.5.\n        search_size: The number of evaluations in the local random search. Defaults to 6.\n        test: Test to measure the performance of different hyperparameter values. Defaults to [accuracy][slisemap.metrics.accuracy].\n        patience: Number of optimisation rounds without improvement before stopping. Defaults to 2.\n        max_escapes: Maximum numbers optimisation rounds. Defaults to 100.\n        verbose: Print status messages. Defaults to 0.\n        escape_kws: Keyword arguments forwarded to [sm.escape][slisemap.slisemap.Slisemap.escape]. Defaults to {}.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to [sm.lbfgs][slisemap.slisemap.Slisemap.lbfgs].\n\n    Returns:\n        Optimised Slisemap or Slipmap object. This is not the same object as the input!\n\n    Deprecated:\n        1.6: `max_iterations` renamed to `max_escapes`\n    \"\"\"\n    if max_iterations is not None:\n        _deprecated(\n            optimise_with_cv.max_iterations,\n            optimise_with_cv.max_escapes,\n        )\n        max_escapes = max_iterations\n    kwargs, hs_kws = _hyper_init(\n        optimise_with_test,\n        sm=sm,\n        kwargs=kwargs,\n        lasso_grid=lasso_grid,\n        ridge_grid=ridge_grid,\n        radius_grid=radius_grid,\n        search_size=search_size,\n    )\n    if hs_kws is None:\n        kwargs[\"increase_tolerance\"] = False\n        sm.optimise(verbose=verbose, escape_kws=escape_kws, **kwargs)\n        return sm\n\n    X_test = sm._as_new_X(X_test)\n    y_test = sm._as_new_Y(y_test, X_test.shape[0])\n    if verbose:\n        _hyper_verbose(optimise_with_test, sm, 0, test(sm, X_test, y_test))\n\n    # Initial optimisation with: _hyper_select -&gt; escape -&gt; lbfgs\n    sm.lbfgs(only_B=True, verbose=verbose &gt; 2, **kwargs)\n    sm, ev = _hyper_tune(sm, X_test, y_test, test, **hs_kws, **kwargs)\n    cc = CheckConvergence(patience, max_escapes)\n    while not cc.has_converged(ev, sm.copy, verbose=verbose &gt; 1):\n        sm.escape(**escape_kws)\n        sm.lbfgs(verbose=verbose &gt; 2, **kwargs)\n        sm, ev = _hyper_tune(sm, X_test, y_test, test, **hs_kws, **kwargs)\n        if verbose:\n            _hyper_verbose(optimise_with_test, sm, cc.iter, ev)\n\n    # Secondary optimisation with: lbfgs -&gt; _hyper_select\n    sm, ev = cc.optimal, cc.best\n    kwargs[\"increase_tolerance\"] = False\n    cc.patience = min(patience, 1)\n    cc.counter = 0.0\n    while not cc.has_converged(ev, sm.copy, verbose=verbose &gt; 1):\n        sm.lbfgs(verbose=verbose &gt; 2, **kwargs)\n        sm, ev = _hyper_tune(sm, X_test, y_test, test, **hs_kws, **kwargs)\n        if verbose:\n            _hyper_verbose(optimise_with_test, sm, cc.iter, ev)\n\n    return cc.optimal\n</code></pre>"},{"location":"slisemap.tuning/#slisemap.tuning.optimise_with_cv","title":"<code>optimise_with_cv(sm, k=5, lasso_grid=3.0, ridge_grid=3.0, radius_grid=1.1, search_size=6, lerp=0.3, test=accuracy, patience=2, max_escapes=100, verbose=0, escape_kws={}, *, max_iterations=None, **kwargs)</code>","text":"<p>Optimise a Slisemap or Slipmap object using cross validation to tune the regularisation.</p> How this works <ul> <li>The data is split into k folds for cross validation.</li> <li>Then a procedure like optimise_with_test is used.</li> <li>After every hyperparameter tuning the regularisation coefficients are smoothed across the folds (see the <code>lerp</code> parameter).</li> <li>Finally, when the cross validation has converged the solution is transferred to the complete data for one final optimisation.</li> <li>Note that this is significantly slower than just training on Slisemap solution.</li> <li>However, this should be faster than the usual \"outer-loop\" hyperperameter optimisation (but the local search dynamics might be less exhaustive).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Union[Slisemap, Slipmap]</code> <p>Slisemap or Slipmap object.</p> required <code>k</code> <code>int</code> <p>Number of folds for the cross validation. Defaults to 5.</p> <code>5</code> <code>lasso_grid</code> <code>float</code> <p>The extent of the local search for the lasso parameter <code>(lasso/lasso_grid, lasso*lasso_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 3.0.</p> <code>3.0</code> <code>ridge_grid</code> <code>float</code> <p>The extent of the local search for the ridge parameter <code>(ridge/ridge_grid, ridge*ridge_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 3.0.</p> <code>3.0</code> <code>radius_grid</code> <code>float</code> <p>The extent of the local search for the radius parameter <code>(radius/radius_grid, radius*radius_grid)</code>. Set to zero to disable the hyperparameter search. Defaults to 1.5.</p> <code>1.1</code> <code>search_size</code> <code>int</code> <p>The number of evaluations in the local random search. Defaults to 6.</p> <code>6</code> <code>lerp</code> <code>float</code> <p>Smooth regularisation coefficients across folds (linearly interpolating towards the mean coefficients). Defaults to 0.3.</p> <code>0.3</code> <code>test</code> <code>Callable[[Slisemap, Tensor, Tensor], float]</code> <p>Test to measure the performance of different hyperparameter values. Defaults to accuracy.</p> <code>accuracy</code> <code>patience</code> <code>int</code> <p>Number of optimisation rounds without improvement before stopping. Defaults to 1.</p> <code>2</code> <code>max_escapes</code> <code>int</code> <p>Maximum numbers optimisation rounds. Defaults to 100.</p> <code>100</code> <code>verbose</code> <code>Literal[0, 1, 2, 3]</code> <p>Print status messages. Defaults to 0.</p> <code>0</code> <code>escape_kws</code> <code>Dict[str, Any]</code> <p>Keyword arguments forwarded to sm.escape. Defaults to {}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to sm.lbfgs.</p> <p>Returns:</p> Type Description <code>Union[Slisemap, Slipmap]</code> <p>Optimised Slisemap or Slipmap object.</p> Deprecated <p>1.6: <code>max_iterations</code> renamed to <code>max_escapes</code></p> Source code in <code>slisemap/tuning.py</code> <pre><code>def optimise_with_cv(  # noqa: D417\n    sm: Union[Slisemap, Slipmap],\n    k: int = 5,\n    lasso_grid: float = 3.0,\n    ridge_grid: float = 3.0,\n    radius_grid: float = 1.1,\n    search_size: int = 6,\n    lerp: float = 0.3,\n    test: Callable[[Slisemap, torch.Tensor, torch.Tensor], float] = accuracy,\n    patience: int = 2,\n    max_escapes: int = 100,\n    verbose: Literal[0, 1, 2, 3] = 0,\n    escape_kws: Dict[str, Any] = {},\n    *,\n    max_iterations: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Union[Slisemap, Slipmap]:\n    \"\"\"Optimise a Slisemap or Slipmap object using cross validation to tune the regularisation.\n\n    How this works:\n        - The data is split into k folds for cross validation.\n        - Then a procedure like [optimise_with_test][slisemap.tuning.optimise_with_test] is used.\n        - After every hyperparameter tuning the regularisation coefficients are smoothed across the folds (see the `lerp` parameter).\n        - Finally, when the cross validation has converged the solution is transferred to the complete data for one final optimisation.\n        - Note that this is significantly slower than just training on Slisemap solution.\n        - However, this should be faster than the usual \"outer-loop\" hyperperameter optimisation (but the local search dynamics might be less exhaustive).\n\n    Args:\n        sm: Slisemap or Slipmap object.\n        k: Number of folds for the cross validation. Defaults to 5.\n        lasso_grid: The extent of the local search for the lasso parameter `(lasso/lasso_grid, lasso*lasso_grid)`. Set to zero to disable the hyperparameter search. Defaults to 3.0.\n        ridge_grid: The extent of the local search for the ridge parameter `(ridge/ridge_grid, ridge*ridge_grid)`. Set to zero to disable the hyperparameter search. Defaults to 3.0.\n        radius_grid: The extent of the local search for the radius parameter `(radius/radius_grid, radius*radius_grid)`. Set to zero to disable the hyperparameter search. Defaults to 1.5.\n        search_size: The number of evaluations in the local random search. Defaults to 6.\n        lerp: Smooth regularisation coefficients across folds (linearly interpolating towards the mean coefficients). Defaults to 0.3.\n        test: Test to measure the performance of different hyperparameter values. Defaults to [accuracy][slisemap.metrics.accuracy].\n        patience: Number of optimisation rounds without improvement before stopping. Defaults to 1.\n        max_escapes: Maximum numbers optimisation rounds. Defaults to 100.\n        verbose: Print status messages. Defaults to 0.\n        escape_kws: Keyword arguments forwarded to [sm.escape][slisemap.slisemap.Slisemap.escape]. Defaults to {}.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to [sm.lbfgs][slisemap.slisemap.Slisemap.lbfgs].\n\n    Returns:\n        Optimised Slisemap or Slipmap object.\n\n    Deprecated:\n        1.6: `max_iterations` renamed to `max_escapes`\n    \"\"\"\n    if max_iterations is not None:\n        _deprecated(\n            optimise_with_cv.max_iterations,\n            optimise_with_cv.max_escapes,\n        )\n        max_escapes = max_iterations\n    kwargs, hs_kws = _hyper_init(\n        optimise_with_cv,\n        sm=sm,\n        kwargs=kwargs,\n        lasso_grid=lasso_grid,\n        ridge_grid=ridge_grid,\n        radius_grid=radius_grid,\n        search_size=search_size,\n    )\n    if hs_kws is None:\n        kwargs[\"increase_tolerance\"] = False\n        sm.optimise(verbose=verbose, escape_kws=escape_kws, **kwargs)\n        return sm\n\n    # Create k folds\n    fold_size = (sm.n - 1) // k + 1\n    folds = torch.tile(torch.arange(k, **sm.tensorargs), (fold_size,))[: sm.n]\n    sms = []\n    tests = []\n    for i in range(k):\n        X_test = sm._X[folds == i, ...]\n        y_test = sm._Y[folds == i, ...]\n        tests.append((X_test, y_test))\n        sm2 = sm.copy()\n        sm2._X = sm._X[folds != i, ...].clone()\n        sm2._Y = sm._Y[folds != i, ...].clone()\n        if isinstance(sm, Slisemap):\n            sm2._B = sm._B[folds != i, ...].clone()\n        sm2._Z = sm._Z[folds != i, ...].clone()\n        sm2.lbfgs(only_B=True, verbose=verbose &gt; 2, **kwargs)\n        sms.append(sm2)\n\n    # Helper functions\n    def hyper() -&gt; List[float]:\n        nonlocal sms\n        losses = []\n        for i, (X_test, y_test) in enumerate(tests):\n            sms[i], loss = _hyper_tune(sms[i], X_test, y_test, test, **hs_kws, **kwargs)\n            losses.append(loss)\n        return [np.mean(losses), *losses]\n\n    def optim() -&gt; List[float]:\n        lasso = np.mean([sm2.lasso for sm2 in sms])\n        ridge = np.mean([sm2.ridge for sm2 in sms])\n        radius = np.mean([sm2.radius for sm2 in sms])\n        for sm2 in sms:\n            sm2.lasso = sm2.lasso * (1 - lerp) + lasso * lerp\n            sm2.ridge = sm2.ridge * (1 - lerp) + ridge * lerp\n            sm2.radius = sm2.radius * (1 - lerp) + radius * lerp\n            sm2.escape(**escape_kws)\n            sm2.lbfgs(verbose=verbose &gt; 2, **kwargs)\n        return hyper()\n\n    # Optimise the cross validation folds with hyperparameter tuning\n    cc = CheckConvergence(patience, max_escapes)\n    loss = hyper()\n    if verbose:\n        _hyper_verbose(optimise_with_cv, sms, 0, loss[1:])\n    while not cc.has_converged(loss, lambda: [sm2.copy() for sm2 in sms], verbose &gt; 1):\n        loss = optim()\n        if verbose:\n            _hyper_verbose(optimise_with_cv, sms, cc.iter, loss[1:])\n\n    # Apply the tuned parameters on the complete model\n    loss = [test(sm2, sm._X, sm._Y) for sm2 in cc.optimal]\n    opt = np.argmin(loss)\n    sm._Z[folds != opt, ...] = cc.optimal[opt]._Z\n    if isinstance(sm, Slisemap):\n        sm._B[folds != opt, ...] = cc.optimal[opt]._B\n    sm.lasso = np.mean([sm2.lasso for sm2 in cc.optimal])\n    sm.ridge = np.mean([sm2.ridge for sm2 in cc.optimal])\n    sm.radius = np.mean([sm2.radius for sm2 in cc.optimal])\n\n    # Optimise the complete model\n    kwargs[\"increase_tolerance\"] = False\n    sm.lbfgs(verbose=verbose &gt; 2, **kwargs)\n    if verbose:\n        _hyper_verbose(optimise_with_cv, sm, \"Final\", None)\n\n    return sm\n</code></pre>"},{"location":"slisemap.tuning/#slisemap.tuning.optimise","title":"<code>optimise(sm, X_test=None, y_test=None, **kwargs)</code>","text":"<p>Optimise a Slisemap or Slipmap object with hyperparameter tuning.</p> <p>This can either be done using a test set or cross validation. The choice of method is based on whether <code>X_test</code> and <code>y_test</code> is given.</p> <p>Parameters:</p> Name Type Description Default <code>sm</code> <code>Union[Slisemap, Slipmap]</code> <p>Slisemap or Slipmap object.</p> required <code>X_test</code> <code>Union[None, ndarray, Tensor]</code> <p>Data matrix for the test set. Defaults to None.</p> <code>None</code> <code>y_test</code> <code>Union[None, ndarray, Tensor]</code> <p>Target matrix/vector for the test set. Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments to slisemap.tuning.optimise_with_test or slisemap.tuning.optimise_with_cv.</p> <p>Returns:</p> Type Description <code>Union[Slisemap, Slipmap]</code> <p>Optimised Slisemap or Slipmap object. This is not the same object as the input!</p> Deprecated <p>1.6: Use the uncerlying function directly instead</p> Source code in <code>slisemap/tuning.py</code> <pre><code>def optimise(\n    sm: Union[Slisemap, Slipmap],\n    X_test: Union[None, np.ndarray, torch.Tensor] = None,\n    y_test: Union[None, np.ndarray, torch.Tensor] = None,\n    **kwargs: Any,\n) -&gt; Union[Slisemap, Slipmap]:\n    \"\"\"Optimise a Slisemap or Slipmap object with hyperparameter tuning.\n\n    This can either be done using a [test set][slisemap.tuning.optimise_with_test] or [cross validation][slisemap.tuning.optimise_with_cv].\n    The choice of method is based on whether `X_test` and `y_test` is given.\n\n    Args:\n        sm: Slisemap or Slipmap object.\n        X_test: Data matrix for the test set. Defaults to None.\n        y_test: Target matrix/vector for the test set. Defaults to None.\n\n    Keyword Args:\n        **kwargs: Optional keyword arguments to [slisemap.tuning.optimise_with_test][] or [slisemap.tuning.optimise_with_cv][].\n\n    Returns:\n        Optimised Slisemap or Slipmap object. This is not the same object as the input!\n\n    Deprecated:\n        1.6: Use the uncerlying function directly instead\n    \"\"\"\n    if X_test is None or y_test is None:\n        _deprecated(optimise, optimise_with_cv)\n        return optimise_with_cv(sm, **kwargs)\n    else:\n        _deprecated(optimise, optimise_with_test)\n        return optimise_with_test(sm, X_test, y_test, **kwargs)\n</code></pre>"},{"location":"slisemap.utils/","title":"slisemap.utils","text":""},{"location":"slisemap.utils/#slisemap.utils","title":"<code>slisemap.utils</code>","text":"<p>Module that contains various useful functions.</p>"},{"location":"slisemap.utils/#slisemap.utils.softmax_row_kernel","title":"<code>softmax_row_kernel(D)</code>","text":"<p>Kernel function that applies softmax on the rows.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Weight matrix.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def softmax_row_kernel(D: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Kernel function that applies softmax on the rows.\n\n    Args:\n        D: Distance matrix.\n\n    Returns:\n        Weight matrix.\n    \"\"\"\n    return torch.softmax(-D, 1)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.softmax_column_kernel","title":"<code>softmax_column_kernel(D)</code>","text":"<p>Kernel function that applies softmax on the columns.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>Tensor</code> <p>Distance matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Weight matrix.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def softmax_column_kernel(D: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Kernel function that applies softmax on the columns.\n\n    Args:\n        D: Distance matrix.\n\n    Returns:\n        Weight matrix.\n    \"\"\"\n    return torch.softmax(-D, 0)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.squared_distance","title":"<code>squared_distance(A, B)</code>","text":"<p>Distance function that returns the squared euclidean distances.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>The first matrix [n1, d].</p> required <code>B</code> <code>Tensor</code> <p>The second matrix [n2, d].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Distance matrix [n1, n2].</p> Source code in <code>slisemap/utils.py</code> <pre><code>def squared_distance(A: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Distance function that returns the squared euclidean distances.\n\n    Args:\n        A: The first matrix [n1, d].\n        B: The second matrix [n2, d].\n\n    Returns:\n        Distance matrix [n1, n2].\n    \"\"\"\n    return torch.sum((A[:, None, ...] - B[None, ...]) ** 2, -1)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.SlisemapException","title":"<code>SlisemapException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom Exception type (for filtering).</p> Source code in <code>slisemap/utils.py</code> <pre><code>class SlisemapException(Exception):  # noqa: N818\n    \"\"\"Custom Exception type (for filtering).\"\"\"\n\n    pass\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.SlisemapWarning","title":"<code>SlisemapWarning</code>","text":"<p>             Bases: <code>Warning</code></p> <p>Custom Warning type (for filtering).</p> Source code in <code>slisemap/utils.py</code> <pre><code>class SlisemapWarning(Warning):\n    \"\"\"Custom Warning type (for filtering).\"\"\"\n\n    pass\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.CallableLike","title":"<code>CallableLike</code>","text":"<p>             Bases: <code>Generic[_F]</code></p> <p>Type annotation for functions matching the signature of a given function.</p> Source code in <code>slisemap/utils.py</code> <pre><code>class CallableLike(Generic[_F]):\n    \"\"\"Type annotation for functions matching the signature of a given function.\"\"\"\n\n    @staticmethod\n    def __class_getitem__(fn: _F) -&gt; _F:\n        return fn\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.tonp","title":"<code>tonp(x)</code>","text":"<p>Convert a <code>torch.Tensor</code> to a <code>numpy.ndarray</code>.</p> <p>If <code>x</code> is not a <code>torch.Tensor</code> then <code>np.asarray</code> is used instead.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, object]</code> <p>Input <code>torch.Tensor</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Output <code>numpy.ndarray</code>.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def tonp(x: Union[torch.Tensor, object]) -&gt; np.ndarray:\n    \"\"\"Convert a `torch.Tensor` to a `numpy.ndarray`.\n\n    If `x` is not a `torch.Tensor` then `np.asarray` is used instead.\n\n    Args:\n        x: Input `torch.Tensor`.\n\n    Returns:\n        Output `numpy.ndarray`.\n    \"\"\"\n    if isinstance(x, torch.Tensor):\n        return x.cpu().detach().numpy()\n    else:\n        return np.asarray(x)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.CheckConvergence","title":"<code>CheckConvergence</code>","text":"<p>An object that tries to estimate when an optimisation has converged.</p> <p>Use it for, e.g., escape+optimisation cycles in Slisemap.</p> Source code in <code>slisemap/utils.py</code> <pre><code>class CheckConvergence:\n    \"\"\"An object that tries to estimate when an optimisation has converged.\n\n    Use it for, e.g., escape+optimisation cycles in Slisemap.\n    \"\"\"\n\n    __slots__ = {\n        \"current\": \"Current loss value.\",\n        \"best\": \"Best loss value, so far.\",\n        \"counter\": \"Number of steps since the best loss value.\",\n        \"patience\": \"Number of steps allowed without improvement.\",\n        \"optimal\": \"Cache for storing the state that produced the best loss value.\",\n        \"max_iter\": \"The maximum number of iterations.\",\n        \"iter\": \"The current number of iterations.\",\n        \"rel\": \"Minimum relative error for convergence check\",\n    }\n\n    def __init__(\n        self, patience: float = 3, max_iter: int = 1 &lt;&lt; 20, rel: float = 1e-4\n    ) -&gt; None:\n        \"\"\"Create a `CheckConvergence` object.\n\n        Args:\n            patience: How long should the optimisation continue without improvement. Defaults to 3.\n            max_iter: The maximum number of iterations. Defaults to `2**20`.\n            rel: Minimum relative error change that is considered an improvement. Defaults to `1e-4`.\n        \"\"\"\n        self.current = np.inf\n        self.best = np.asarray(np.inf)\n        self.counter = 0.0\n        self.patience = patience\n        self.optimal = None\n        self.max_iter = max_iter\n        self.iter = 0\n        self.rel = rel\n\n    def has_converged(\n        self,\n        loss: Union[float, Sequence[float], np.ndarray],\n        store: Optional[Callable[[], Any]] = None,\n        verbose: bool = False,\n    ) -&gt; bool:\n        \"\"\"Check if the optimisation has converged.\n\n        If more than one loss value is provided, then only the first one is checked when storing the `optimal_state`.\n        The other losses are only used for checking convergence.\n\n        Args:\n            loss: The latest loss value(s).\n            store: Function that returns the current state for storing in `self.optimal_state`. Defaults to None.\n            verbose: Pring debug messages. Defaults to False.\n\n        Returns:\n            True if the optimisation has converged.\n        \"\"\"\n        self.iter += 1\n        loss = np.asarray(loss)\n        if np.any(np.isnan(loss)):\n            _warn(\"Loss is `nan`\", CheckConvergence.has_converged)\n            return True\n        if np.any(loss + np.abs(loss) * self.rel &lt; self.best):\n            self.counter = 0.0  # Reset the counter if a new best\n            if store is not None and loss.item(0) &lt; self.best.item(0):\n                self.optimal = store()\n            self.best = np.minimum(loss, self.best)\n        else:\n            # Increase the counter if no improvement\n            self.counter += np.mean(self.current &lt;= loss)\n        self.current = loss\n        if verbose:\n            print(\n                f\"CheckConvergence: patience={self.patience-self.counter:g}/{self.patience:g}   iter={self.iter}/{self.max_iter}\"\n            )\n        return self.counter &gt;= self.patience or self.iter &gt;= self.max_iter\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.CheckConvergence.__init__","title":"<code>__init__(patience=3, max_iter=1 &lt;&lt; 20, rel=0.0001)</code>","text":"<p>Create a <code>CheckConvergence</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>float</code> <p>How long should the optimisation continue without improvement. Defaults to 3.</p> <code>3</code> <code>max_iter</code> <code>int</code> <p>The maximum number of iterations. Defaults to <code>2**20</code>.</p> <code>1 &lt;&lt; 20</code> <code>rel</code> <code>float</code> <p>Minimum relative error change that is considered an improvement. Defaults to <code>1e-4</code>.</p> <code>0.0001</code> Source code in <code>slisemap/utils.py</code> <pre><code>def __init__(\n    self, patience: float = 3, max_iter: int = 1 &lt;&lt; 20, rel: float = 1e-4\n) -&gt; None:\n    \"\"\"Create a `CheckConvergence` object.\n\n    Args:\n        patience: How long should the optimisation continue without improvement. Defaults to 3.\n        max_iter: The maximum number of iterations. Defaults to `2**20`.\n        rel: Minimum relative error change that is considered an improvement. Defaults to `1e-4`.\n    \"\"\"\n    self.current = np.inf\n    self.best = np.asarray(np.inf)\n    self.counter = 0.0\n    self.patience = patience\n    self.optimal = None\n    self.max_iter = max_iter\n    self.iter = 0\n    self.rel = rel\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.CheckConvergence.has_converged","title":"<code>has_converged(loss, store=None, verbose=False)</code>","text":"<p>Check if the optimisation has converged.</p> <p>If more than one loss value is provided, then only the first one is checked when storing the <code>optimal_state</code>. The other losses are only used for checking convergence.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Union[float, Sequence[float], ndarray]</code> <p>The latest loss value(s).</p> required <code>store</code> <code>Optional[Callable[[], Any]]</code> <p>Function that returns the current state for storing in <code>self.optimal_state</code>. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Pring debug messages. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the optimisation has converged.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def has_converged(\n    self,\n    loss: Union[float, Sequence[float], np.ndarray],\n    store: Optional[Callable[[], Any]] = None,\n    verbose: bool = False,\n) -&gt; bool:\n    \"\"\"Check if the optimisation has converged.\n\n    If more than one loss value is provided, then only the first one is checked when storing the `optimal_state`.\n    The other losses are only used for checking convergence.\n\n    Args:\n        loss: The latest loss value(s).\n        store: Function that returns the current state for storing in `self.optimal_state`. Defaults to None.\n        verbose: Pring debug messages. Defaults to False.\n\n    Returns:\n        True if the optimisation has converged.\n    \"\"\"\n    self.iter += 1\n    loss = np.asarray(loss)\n    if np.any(np.isnan(loss)):\n        _warn(\"Loss is `nan`\", CheckConvergence.has_converged)\n        return True\n    if np.any(loss + np.abs(loss) * self.rel &lt; self.best):\n        self.counter = 0.0  # Reset the counter if a new best\n        if store is not None and loss.item(0) &lt; self.best.item(0):\n            self.optimal = store()\n        self.best = np.minimum(loss, self.best)\n    else:\n        # Increase the counter if no improvement\n        self.counter += np.mean(self.current &lt;= loss)\n    self.current = loss\n    if verbose:\n        print(\n            f\"CheckConvergence: patience={self.patience-self.counter:g}/{self.patience:g}   iter={self.iter}/{self.max_iter}\"\n        )\n    return self.counter &gt;= self.patience or self.iter &gt;= self.max_iter\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.LBFGS","title":"<code>LBFGS(loss_fn, variables, max_iter=500, max_eval=None, line_search_fn='strong_wolfe', time_limit=None, increase_tolerance=False, verbose=False, **kwargs)</code>","text":"<p>Optimise a function using LBFGS.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable[[], Tensor]</code> <p>Function that returns a value to be minimised.</p> required <code>variables</code> <code>List[Tensor]</code> <p>List of variables to optimise (must have <code>requires_grad=True</code>).</p> required <code>max_iter</code> <code>int</code> <p>Maximum number of LBFGS iterations. Defaults to 500.</p> <code>500</code> <code>max_eval</code> <code>Optional[int]</code> <p>Maximum number of function evaluations. Defaults to <code>1.25 * max_iter</code>.</p> <code>None</code> <code>line_search_fn</code> <code>Optional[str]</code> <p>Line search method (None or \"strong_wolfe\"). Defaults to \"strong_wolfe\".</p> <code>'strong_wolfe'</code> <code>time_limit</code> <code>Optional[float]</code> <p>Optional time limit for the optimisation (in seconds). Defaults to None.</p> <code>None</code> <code>increase_tolerance</code> <code>bool</code> <p>Increase the tolerances for convergence checking. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print status messages. Defaults to False.</p> <code>False</code> <p>Other Parameters:</p> Name Type Description <code>**kwargs</code> <code>Any</code> <p>Arguments forwarded to <code>torch.optim.LBFGS</code>.</p> <p>Returns:</p> Type Description <code>LBFGS</code> <p>The LBFGS optimiser.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def LBFGS(\n    loss_fn: Callable[[], torch.Tensor],\n    variables: List[torch.Tensor],\n    max_iter: int = 500,\n    max_eval: Optional[int] = None,\n    line_search_fn: Optional[str] = \"strong_wolfe\",\n    time_limit: Optional[float] = None,\n    increase_tolerance: bool = False,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; torch.optim.LBFGS:\n    \"\"\"Optimise a function using [LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS).\n\n    Args:\n        loss_fn: Function that returns a value to be minimised.\n        variables: List of variables to optimise (must have `requires_grad=True`).\n        max_iter: Maximum number of LBFGS iterations. Defaults to 500.\n        max_eval: Maximum number of function evaluations. Defaults to `1.25 * max_iter`.\n        line_search_fn: Line search method (None or \"strong_wolfe\"). Defaults to \"strong_wolfe\".\n        time_limit: Optional time limit for the optimisation (in seconds). Defaults to None.\n        increase_tolerance: Increase the tolerances for convergence checking. Defaults to False.\n        verbose: Print status messages. Defaults to False.\n\n    Keyword Args:\n        **kwargs: Arguments forwarded to [`torch.optim.LBFGS`](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html).\n\n    Returns:\n        The LBFGS optimiser.\n    \"\"\"\n    if increase_tolerance:\n        kwargs[\"tolerance_grad\"] = 100 * kwargs.get(\"tolerance_grad\", 1e-7)\n        kwargs[\"tolerance_change\"] = 100 * kwargs.get(\"tolerance_change\", 1e-9)\n    optimiser = torch.optim.LBFGS(\n        variables,\n        max_iter=max_iter if time_limit is None else 20,\n        max_eval=max_eval,\n        line_search_fn=line_search_fn,\n        **kwargs,\n    )\n\n    def closure() -&gt; torch.Tensor:\n        optimiser.zero_grad()\n        loss = loss_fn()\n        loss.backward()\n        return loss\n\n    if time_limit is None:\n        loss = optimiser.step(closure)\n    else:\n        start = timer()\n        prev_evals = 0\n        for _ in range((max_iter - 1) // 20 + 1):\n            loss = optimiser.step(closure)\n            if not torch.all(torch.isfinite(loss)).cpu().detach().item():\n                break\n            if timer() - start &gt; time_limit:\n                if verbose:\n                    print(\"LBFGS: Time limit exceeded!\")\n                break\n            tot_evals = optimiser.state_dict()[\"state\"][0][\"func_evals\"]\n            if prev_evals + 1 == tot_evals:\n                break  # LBFGS has converged if it returns after one evaluation\n            prev_evals = tot_evals\n            if max_eval is not None:\n                if tot_evals &gt;= max_eval:\n                    break  # Number of evaluations exceeded max_eval\n                optimiser.param_groups[0][\"max_eval\"] -= tot_evals\n            # The number of steps is limited by ceiling(max_iter/20) with 20 iterations per step\n\n    if verbose:\n        iters = optimiser.state_dict()[\"state\"][0][\"n_iter\"]\n        evals = optimiser.state_dict()[\"state\"][0][\"func_evals\"]\n        loss = loss.mean().cpu().detach().item()\n        if not np.isfinite(loss):\n            print(\"LBFGS: Loss is not finite {}!\")\n        elif iters &gt;= max_iter:\n            print(\"LBFGS: Maximum number of iterations exceeded!\")\n        elif max_eval is not None and evals &gt;= max_eval:\n            print(\"LBFGS: Maximum number of evaluations exceeded!\")\n        else:\n            print(f\"LBFGS: Converged in {iters} iterations\")\n\n    return optimiser\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.PCA_rotation","title":"<code>PCA_rotation(X, components=-1, center=True, full=True, niter=10)</code>","text":"<p>Calculate the rotation matrix from PCA.</p> <p>If the PCA fails (e.g. if original matrix is not full rank) then this shows a warning instead of throwing an error (returns a dummy rotation).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The original matrix.</p> required <code>components</code> <code>int</code> <p>The maximum number of components in the embedding. Defaults to <code>min(*X.shape)</code>.</p> <code>-1</code> <code>center</code> <code>bool</code> <p>Center the matrix before calculating the PCA.</p> <code>True</code> <code>full</code> <code>bool</code> <p>Use a full SVD for the PCA (slower). Defaults to True.</p> <code>True</code> <code>niter</code> <code>int</code> <p>The number of iterations when a randomised approach is used. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Rotation matrix that turns the original matrix into the embedded space.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def PCA_rotation(\n    X: torch.Tensor,\n    components: int = -1,\n    center: bool = True,\n    full: bool = True,\n    niter: int = 10,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the rotation matrix from PCA.\n\n    If the PCA fails (e.g. if original matrix is not full rank) then this shows a warning instead of throwing an error (returns a dummy rotation).\n\n    Args:\n        X: The original matrix.\n        components: The maximum number of components in the embedding. Defaults to `min(*X.shape)`.\n        center: Center the matrix before calculating the PCA.\n        full: Use a full SVD for the PCA (slower). Defaults to True.\n        niter: The number of iterations when a randomised approach is used. Defaults to 10.\n\n    Returns:\n        Rotation matrix that turns the original matrix into the embedded space.\n    \"\"\"\n    try:\n        components = min(*X.shape, components) if components &gt; 0 else min(*X.shape)\n        if full:\n            if center:\n                X = X - X.mean(dim=(-2,), keepdim=True)\n            return torch.linalg.svd(X, full_matrices=False)[2].T[:, :components]\n        else:\n            return torch.pca_lowrank(X, components, center=center, niter=niter)[2]\n    except Exception:\n        _warn(\"Could not perform PCA\", PCA_rotation)\n        z = torch.zeros((X.shape[1], components), dtype=X.dtype, device=X.device)\n        z.fill_diagonal_(1.0, True)\n        return z\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.global_model","title":"<code>global_model(X, Y, local_model, local_loss, coefficients=None, lasso=0.0, ridge=0.0)</code>","text":"<p>Find coefficients for a global model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Data matrix.</p> required <code>Y</code> <code>Tensor</code> <p>Target matrix.</p> required <code>local_model</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Prediction function for the model.</p> required <code>local_loss</code> <code>Callable[[Tensor, Tensor, Tensor], Tensor]</code> <p>Loss function for the model.</p> required <code>coefficients</code> <code>Optional[int]</code> <p>Number of coefficients. Defaults to X.shape[1].</p> <code>None</code> <code>lasso</code> <code>float</code> <p>Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.</p> <code>0.0</code> <code>ridge</code> <code>float</code> <p>Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Global model coefficients.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def global_model(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    local_model: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    local_loss: Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor],\n    coefficients: Optional[int] = None,\n    lasso: float = 0.0,\n    ridge: float = 0.0,\n) -&gt; torch.Tensor:\n    r\"\"\"Find coefficients for a global model.\n\n    Args:\n        X: Data matrix.\n        Y: Target matrix.\n        local_model: Prediction function for the model.\n        local_loss: Loss function for the model.\n        coefficients: Number of coefficients. Defaults to X.shape[1].\n        lasso: Lasso-regularisation coefficient for B ($\\lambda_{lasso} * ||B||_1$). Defaults to 0.0.\n        ridge: Ridge-regularisation coefficient for B ($\\lambda_{ridge} * ||B||_2$). Defaults to 0.0.\n\n    Returns:\n        Global model coefficients.\n    \"\"\"\n    shape = (1, X.shape[1] * Y.shape[1] if coefficients is None else coefficients)\n    B = torch.zeros(shape, dtype=X.dtype, device=X.device).requires_grad_(True)\n\n    def loss() -&gt; torch.Tensor:\n        loss = local_loss(local_model(X, B), Y).mean()\n        if lasso &gt; 0:\n            loss += lasso * torch.sum(B.abs())\n        if ridge &gt; 0:\n            loss += ridge * torch.sum(B**2)\n        return loss\n\n    LBFGS(loss, [B])\n    return B.detach()\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.dict_array","title":"<code>dict_array(dict)</code>","text":"<p>Turn a dictionary of various values to a dictionary of numpy arrays with equal length inplace.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>The same dictionary where the values are numpy arrays with equal length.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def dict_array(dict: Dict[str, Any]) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Turn a dictionary of various values to a dictionary of numpy arrays with equal length inplace.\n\n    Args:\n        dict: Dictionary.\n\n    Returns:\n        The same dictionary where the values are numpy arrays with equal length.\n    \"\"\"\n    n = 1\n    for k, v in dict.items():\n        v = np.asarray(v).ravel()\n        dict[k] = v\n        n = max(n, len(v))\n    for k, v in dict.items():\n        if len(v) == 1:\n            dict[k] = np.repeat(v, n)\n        elif len(v) != n:\n            _warn(f\"Uneven lengths in dictionary ({k}: {len(v)} != {n})\", dict_array)\n    return dict\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.dict_append","title":"<code>dict_append(df, d)</code>","text":"<p>Append a dictionary of values to a dictionary of numpy arrays (see <code>dict_array</code>) inplace.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dict[str, ndarray]</code> <p>Dictionary of numpy arrays.</p> required <code>d</code> <code>Dict[str, Any]</code> <p>Dictionary to append.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>The same dictionary as <code>df</code> with the values from <code>d</code> appended.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def dict_append(df: Dict[str, np.ndarray], d: Dict[str, Any]) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Append a dictionary of values to a dictionary of numpy arrays (see `dict_array`) inplace.\n\n    Args:\n        df: Dictionary of numpy arrays.\n        d: Dictionary to append.\n\n    Returns:\n        The same dictionary as `df` with the values from `d` appended.\n    \"\"\"\n    d = dict_array(d)\n    for k in df:\n        df[k] = np.concatenate((df[k], d[k]), 0)\n    return df\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.dict_concat","title":"<code>dict_concat(dicts)</code>","text":"<p>Combine multiple dictionaries into one by concatenating the values.</p> <p>Calls <code>dict_array</code> to pre-process the dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>dicts</code> <code>Union[Sequence[Dict[str, Any]], Iterator[Dict[str, Any]]]</code> <p>Sequence or Generator with dictionaries (all must have the same keys).</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Combined dictionary.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def dict_concat(\n    dicts: Union[Sequence[Dict[str, Any]], Iterator[Dict[str, Any]]],\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Combine multiple dictionaries into one by concatenating the values.\n\n    Calls `dict_array` to pre-process the dictionaries.\n\n    Args:\n        dicts: Sequence or Generator with dictionaries (all must have the same keys).\n\n    Returns:\n        Combined dictionary.\n    \"\"\"\n    if isinstance(dicts, Sequence):\n        dicts = (d for d in dicts)\n    df = dict_array(next(dicts))\n    for d in dicts:\n        dict_append(df, d)\n    return df\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.ToTensor","title":"<code>ToTensor = Union[float, np.ndarray, torch.Tensor, 'pandas.DataFrame', Dict[str, Sequence[float]], Sequence[float]]</code>  <code>module-attribute</code>","text":"<p>Type annotations for objects that can be turned into a <code>torch.Tensor</code> with the to_tensor function.</p>"},{"location":"slisemap.utils/#slisemap.utils.to_tensor","title":"<code>to_tensor(input, **tensorargs)</code>","text":"<p>Convert the input into a <code>torch.Tensor</code> (via <code>numpy.ndarray</code> if necessary).</p> <p>This function wrapps <code>torch.as_tensor</code> (and <code>numpy.asarray</code>) and tries to extract row and column names. This function can handle arbitrary objects (such as <code>pandas.DataFrame</code>) if they implement <code>.to_numpy()</code> and, optionally, <code>.index</code> and <code>.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>ToTensor</code> <p>input data</p> required <p>Keyword Args:     **tensorargs: additional arguments to <code>torch.as_tensor</code></p> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>output tensor</p> <code>rows</code> <code>Optional[Sequence[object]]</code> <p>row names or <code>None</code></p> <code>columns</code> <code>Optional[Sequence[object]]</code> <p>column names or <code>None</code></p> Source code in <code>slisemap/utils.py</code> <pre><code>def to_tensor(\n    input: ToTensor, **tensorargs: object\n) -&gt; Tuple[torch.Tensor, Optional[Sequence[object]], Optional[Sequence[object]]]:\n    \"\"\"Convert the input into a `torch.Tensor` (via `numpy.ndarray` if necessary).\n\n    This function wrapps `torch.as_tensor` (and `numpy.asarray`) and tries to extract row and column names.\n    This function can handle arbitrary objects (such as `pandas.DataFrame`) if they implement `.to_numpy()` and, optionally, `.index` and `.columns`.\n\n    Args:\n        input: input data\n    Keyword Args:\n        **tensorargs: additional arguments to `torch.as_tensor`\n\n    Returns:\n        output: output tensor\n        rows: row names or `None`\n        columns: column names or `None`\n    \"\"\"\n    if isinstance(input, dict):\n        output = torch.as_tensor(np.asarray(tuple(input.values())).T, **tensorargs)\n        return output, None, list(input.keys())\n    elif isinstance(input, (np.ndarray, torch.Tensor)):\n        return (torch.as_tensor(input, **tensorargs), None, None)\n    else:\n        # Check if X is similar to a Pandas DataFrame\n        try:\n            output = torch.as_tensor(input.to_numpy(), **tensorargs)\n        except (AttributeError, TypeError):\n            try:\n                output = torch.as_tensor(input.numpy(), **tensorargs)\n            except (AttributeError, TypeError):\n                try:\n                    output = torch.as_tensor(input, **tensorargs)\n                except (TypeError, RuntimeError):\n                    output = torch.as_tensor(np.asarray(input), **tensorargs)\n        try:\n            columns = input.columns if len(input.columns) == output.shape[1] else None\n        except (AttributeError, TypeError):\n            columns = None\n        try:\n            rows = input.index if len(input.index) == output.shape[0] else None\n        except (AttributeError, TypeError):\n            rows = None\n        return output, rows, columns\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata","title":"<code>Metadata</code>","text":"<p>             Bases: <code>dict</code></p> <p>Metadata for Slisemap objects.</p> <p>Primarily row names, column names, and scaling information about the matrices (these are used when plotting). But other arbitrary information can also be stored in this dictionary (The main Slisemap class has predefined \"slots\").</p> Source code in <code>slisemap/utils.py</code> <pre><code>class Metadata(dict):\n    \"\"\"Metadata for Slisemap objects.\n\n    Primarily row names, column names, and scaling information about the matrices (these are used when plotting).\n    But other arbitrary information can also be stored in this dictionary (The main Slisemap class has predefined \"slots\").\n    \"\"\"\n\n    def __init__(self, root: \"Slisemap\", **kwargs: Any) -&gt; None:  # noqa: F821\n        \"\"\"Create a Metadata dictionary.\"\"\"\n        super().__init__(**kwargs)\n        self.root = root\n\n    def set_rows(self, *rows: Optional[Sequence[object]]) -&gt; None:\n        \"\"\"Set the row names with checks to avoid saving ranges.\n\n        Args:\n            *rows: row names\n        \"\"\"\n        for row in rows:\n            if row is not None:\n                try:\n                    # Check if row is `range(0, self.root.n, 1)`-like (duck typing)\n                    if row.start == 0 and row.step == 1 and row.stop == self.root.n:\n                        continue\n                except AttributeError:\n                    pass\n                _assert(\n                    len(row) == self.root.n,\n                    f\"Wrong number of row names {len(row)} != {self.root.n}\",\n                    Metadata.set_rows,\n                )\n                if all(i == j for i, j in enumerate(row)):\n                    continue\n                self[\"rows\"] = list(row)\n                break\n\n    def set_variables(\n        self,\n        variables: Optional[Sequence[Any]] = None,\n        add_intercept: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Set the variable names with checks.\n\n        Args:\n            variables: variable names\n            add_intercept: add \"Intercept\" to the variable names. Defaults to `self.root.intercept`,\n        \"\"\"\n        if add_intercept is None:\n            add_intercept = self.root.intercept\n        if variables is not None:\n            variables = list(variables)\n            if add_intercept:\n                variables.append(\"Intercept\")\n            _assert(\n                len(variables) == self.root.m,\n                f\"Wrong number of variables {len(variables)} != {self.root.m} ({variables})\",\n                Metadata.set_variables,\n            )\n            self[\"variables\"] = variables\n\n    def set_targets(self, targets: Union[None, str, Sequence[Any]] = None) -&gt; None:\n        \"\"\"Set the target names with checks.\n\n        Args:\n            targets: target names\n        \"\"\"\n        if targets is not None:\n            targets = [targets] if isinstance(targets, str) else list(targets)\n            _assert(\n                len(targets) == self.root.o,\n                f\"Wrong number of targets {len(targets)} != {self.root.o}\",\n                Metadata.set_targets,\n            )\n            self[\"targets\"] = targets\n\n    def set_coefficients(self, coefficients: Optional[Sequence[Any]] = None) -&gt; None:\n        \"\"\"Set the coefficient names with checks.\n\n        Args:\n            coefficients: coefficient names\n        \"\"\"\n        if coefficients is not None:\n            _assert(\n                len(coefficients) == self.root.q,\n                f\"Wrong number of targets {len(coefficients)} != {self.root.q}\",\n                Metadata.set_coefficients,\n            )\n            self[\"coefficients\"] = list(coefficients)\n\n    def set_dimensions(self, dimensions: Optional[Sequence[Any]] = None) -&gt; None:\n        \"\"\"Set the dimension names with checks.\n\n        Args:\n            dimensions: dimension names\n        \"\"\"\n        if dimensions is not None:\n            _assert(\n                len(dimensions) == self.root.d,\n                f\"Wrong number of targets {len(dimensions)} != {self.root.d}\",\n                Metadata.set_dimensions,\n            )\n            self[\"dimensions\"] = list(dimensions)\n\n    def get_coefficients(self, fallback: bool = True) -&gt; Optional[List[str]]:\n        \"\"\"Get a list of coefficient names.\n\n        Args:\n            fallback: If metadata for coefficients is missing, return a new list instead of None. Defaults to True.\n\n        Returns:\n            list of coefficient names\n        \"\"\"\n        if \"coefficients\" in self:\n            return self[\"coefficients\"]\n        if \"variables\" in self:\n            if self.root.m == self.root.q:\n                return self[\"variables\"]\n            if \"targets\" in self and self.root.m * self.root.o &gt;= self.root.q:\n                return [\n                    f\"{t}: {v}\" for t in self[\"targets\"] for v in self[\"variables\"]\n                ][: self.root.q]\n        if fallback:\n            return [f\"B_{i}\" for i in range(self.root.q)]\n        else:\n            return None\n\n    def get_targets(self, fallback: bool = True) -&gt; Optional[List[str]]:\n        \"\"\"Get a list of target names.\n\n        Args:\n            fallback: If metadata for targets is missing, return a new list instead of None. Defaults to True.\n\n        Returns:\n            list of target names\n        \"\"\"\n        if \"targets\" in self:\n            return self[\"targets\"]\n        elif fallback:\n            return [f\"Y_{i}\" for i in range(self.root.o)] if self.root.o &gt; 1 else [\"Y\"]\n        else:\n            return None\n\n    def get_variables(\n        self, intercept: bool = True, fallback: bool = True\n    ) -&gt; Optional[List[str]]:\n        \"\"\"Get a list of variable names.\n\n        Args:\n            intercept: include the intercept in the list. Defaults to True.\n            fallback: If metadata for variables is missing, return a new list instead of None. Defaults to True.\n\n\n        Returns:\n            list of variable names\n        \"\"\"\n        if \"variables\" in self:\n            if self.root.intercept and not intercept:\n                return self[\"variables\"][:-1]\n            else:\n                return self[\"variables\"]\n        elif fallback:\n            if self.root.intercept:\n                if not intercept:\n                    return [f\"X_{i}\" for i in range(self.root.m - 1)]\n                else:\n                    return [f\"X_{i}\" for i in range(self.root.m - 1)] + [\"X_Intercept\"]\n            else:\n                return [f\"X_{i}\" for i in range(self.root.m)]\n        else:\n            return None\n\n    def get_dimensions(\n        self, fallback: bool = True, long: bool = False\n    ) -&gt; Optional[List[str]]:\n        \"\"\"Get a list of dimension names.\n\n        Args:\n            fallback: If metadata for dimensions is missing, return a new list instead of None. Defaults to True.\n            long: Use \"SLISEMAP 1\",... as fallback instead of \"Z_0\",...\n\n        Returns:\n            list of dimension names\n        \"\"\"\n        if \"dimensions\" in self:\n            return self[\"dimensions\"]\n        elif fallback:\n            if long:\n                cls = \"Slisemap\" if self.root is None else type(self.root).__name__\n                return [f\"{cls} {i+1}\" for i in range(self.root.d)]\n            else:\n                return [f\"Z_{i}\" for i in range(self.root.d)]\n        else:\n            return None\n\n    def get_rows(self, fallback: bool = True) -&gt; Optional[Sequence[Any]]:\n        \"\"\"Get a list of row names.\n\n        Args:\n            fallback: If metadata for rows is missing, return a range instead of None. Defaults to True.\n\n        Returns:\n            list (or range) of row names\n        \"\"\"\n        if \"rows\" in self:\n            return self[\"rows\"]\n        elif fallback:\n            return range(self.root.n)\n        else:\n            return None\n\n    def set_scale_X(\n        self,\n        center: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n        scale: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n    ) -&gt; None:\n        \"\"\"Set scaling information with checks.\n\n        Use if `X` has been scaled before being input to Slisemap.\n        Assuming the scaling can be converted to the form `X = (X_unscaled - center) / scale`.\n        This allows some plots to (temporarily) revert the scaling (for more intuitive units).\n\n        Args:\n            center: The constant offset of `X`. Defaults to None.\n            scale: The scaling factor of `X`. Defaults to None.\n        \"\"\"\n        if center is not None:\n            center = tonp(center).ravel()\n            assert center.size == self.root.m - self.root.intercept\n            self[\"X_center\"] = center\n        if scale is not None:\n            scale = tonp(scale).ravel()\n            assert scale.size == self.root.m - self.root.intercept\n            self[\"X_scale\"] = scale\n\n    def set_scale_Y(\n        self,\n        center: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n        scale: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n    ) -&gt; None:\n        \"\"\"Set scaling information with checks.\n\n        Use if `Y` has been scaled before being input to Slisemap.\n        Assuming the scaling can be converted to the form `Y = (Y_unscaled - center) / scale`.\n        This allows some plots to (temporarily) revert the scaling (for more intuitive units).\n\n        Args:\n            center: The constant offset of `Y`. Defaults to None.\n            scale: The scaling factor of `Y`. Defaults to None.\n        \"\"\"\n        if center is not None:\n            center = tonp(center).ravel()\n            assert center.size == self.root.o\n            self[\"Y_center\"] = center\n        if scale is not None:\n            scale = tonp(scale).ravel()\n            assert scale.size == self.root.o\n            self[\"Y_scale\"] = scale\n\n    def unscale_X(self, X: Optional[np.ndarray] = None) -&gt; np.ndarray:\n        \"\"\"Unscale X if the scaling information has been given (see `set_scale_X`).\n\n        Args:\n            X: The data matrix X (or `self.root.get_X(intercept=False)` if None).\n\n        Returns:\n            Possibly scaled X.\n        \"\"\"\n        if X is None:\n            X = self.root.get_X(intercept=False)\n        if \"X_scale\" in self:\n            X = X * self[\"X_scale\"][None, :]\n        if \"X_center\" in self:\n            X = X + self[\"X_center\"][None, :]\n        return X\n\n    def unscale_Y(self, Y: Optional[np.ndarray] = None) -&gt; np.ndarray:\n        \"\"\"Unscale Y if the scaling information has been given (see `set_scale_Y`).\n\n        Args:\n            Y: The response matrix Y (or `self.root.get_Y()` if None).\n\n        Returns:\n            Possibly scaled Y.\n        \"\"\"\n        if Y is None:\n            Y = self.root.get_Y()\n        if \"Y_scale\" in self:\n            Y = Y * self[\"Y_scale\"][None, :]\n        if \"Y_center\" in self:\n            Y = Y + self[\"Y_center\"][None, :]\n        return Y\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.__init__","title":"<code>__init__(root, **kwargs)</code>","text":"<p>Create a Metadata dictionary.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def __init__(self, root: \"Slisemap\", **kwargs: Any) -&gt; None:  # noqa: F821\n    \"\"\"Create a Metadata dictionary.\"\"\"\n    super().__init__(**kwargs)\n    self.root = root\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_rows","title":"<code>set_rows(*rows)</code>","text":"<p>Set the row names with checks to avoid saving ranges.</p> <p>Parameters:</p> Name Type Description Default <code>*rows</code> <code>Optional[Sequence[object]]</code> <p>row names</p> <code>()</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_rows(self, *rows: Optional[Sequence[object]]) -&gt; None:\n    \"\"\"Set the row names with checks to avoid saving ranges.\n\n    Args:\n        *rows: row names\n    \"\"\"\n    for row in rows:\n        if row is not None:\n            try:\n                # Check if row is `range(0, self.root.n, 1)`-like (duck typing)\n                if row.start == 0 and row.step == 1 and row.stop == self.root.n:\n                    continue\n            except AttributeError:\n                pass\n            _assert(\n                len(row) == self.root.n,\n                f\"Wrong number of row names {len(row)} != {self.root.n}\",\n                Metadata.set_rows,\n            )\n            if all(i == j for i, j in enumerate(row)):\n                continue\n            self[\"rows\"] = list(row)\n            break\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_variables","title":"<code>set_variables(variables=None, add_intercept=None)</code>","text":"<p>Set the variable names with checks.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>Optional[Sequence[Any]]</code> <p>variable names</p> <code>None</code> <code>add_intercept</code> <code>Optional[bool]</code> <p>add \"Intercept\" to the variable names. Defaults to <code>self.root.intercept</code>,</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_variables(\n    self,\n    variables: Optional[Sequence[Any]] = None,\n    add_intercept: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Set the variable names with checks.\n\n    Args:\n        variables: variable names\n        add_intercept: add \"Intercept\" to the variable names. Defaults to `self.root.intercept`,\n    \"\"\"\n    if add_intercept is None:\n        add_intercept = self.root.intercept\n    if variables is not None:\n        variables = list(variables)\n        if add_intercept:\n            variables.append(\"Intercept\")\n        _assert(\n            len(variables) == self.root.m,\n            f\"Wrong number of variables {len(variables)} != {self.root.m} ({variables})\",\n            Metadata.set_variables,\n        )\n        self[\"variables\"] = variables\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_targets","title":"<code>set_targets(targets=None)</code>","text":"<p>Set the target names with checks.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>Union[None, str, Sequence[Any]]</code> <p>target names</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_targets(self, targets: Union[None, str, Sequence[Any]] = None) -&gt; None:\n    \"\"\"Set the target names with checks.\n\n    Args:\n        targets: target names\n    \"\"\"\n    if targets is not None:\n        targets = [targets] if isinstance(targets, str) else list(targets)\n        _assert(\n            len(targets) == self.root.o,\n            f\"Wrong number of targets {len(targets)} != {self.root.o}\",\n            Metadata.set_targets,\n        )\n        self[\"targets\"] = targets\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_coefficients","title":"<code>set_coefficients(coefficients=None)</code>","text":"<p>Set the coefficient names with checks.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>Optional[Sequence[Any]]</code> <p>coefficient names</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_coefficients(self, coefficients: Optional[Sequence[Any]] = None) -&gt; None:\n    \"\"\"Set the coefficient names with checks.\n\n    Args:\n        coefficients: coefficient names\n    \"\"\"\n    if coefficients is not None:\n        _assert(\n            len(coefficients) == self.root.q,\n            f\"Wrong number of targets {len(coefficients)} != {self.root.q}\",\n            Metadata.set_coefficients,\n        )\n        self[\"coefficients\"] = list(coefficients)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_dimensions","title":"<code>set_dimensions(dimensions=None)</code>","text":"<p>Set the dimension names with checks.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>Optional[Sequence[Any]]</code> <p>dimension names</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_dimensions(self, dimensions: Optional[Sequence[Any]] = None) -&gt; None:\n    \"\"\"Set the dimension names with checks.\n\n    Args:\n        dimensions: dimension names\n    \"\"\"\n    if dimensions is not None:\n        _assert(\n            len(dimensions) == self.root.d,\n            f\"Wrong number of targets {len(dimensions)} != {self.root.d}\",\n            Metadata.set_dimensions,\n        )\n        self[\"dimensions\"] = list(dimensions)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.get_coefficients","title":"<code>get_coefficients(fallback=True)</code>","text":"<p>Get a list of coefficient names.</p> <p>Parameters:</p> Name Type Description Default <code>fallback</code> <code>bool</code> <p>If metadata for coefficients is missing, return a new list instead of None. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list of coefficient names</p> Source code in <code>slisemap/utils.py</code> <pre><code>def get_coefficients(self, fallback: bool = True) -&gt; Optional[List[str]]:\n    \"\"\"Get a list of coefficient names.\n\n    Args:\n        fallback: If metadata for coefficients is missing, return a new list instead of None. Defaults to True.\n\n    Returns:\n        list of coefficient names\n    \"\"\"\n    if \"coefficients\" in self:\n        return self[\"coefficients\"]\n    if \"variables\" in self:\n        if self.root.m == self.root.q:\n            return self[\"variables\"]\n        if \"targets\" in self and self.root.m * self.root.o &gt;= self.root.q:\n            return [\n                f\"{t}: {v}\" for t in self[\"targets\"] for v in self[\"variables\"]\n            ][: self.root.q]\n    if fallback:\n        return [f\"B_{i}\" for i in range(self.root.q)]\n    else:\n        return None\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.get_targets","title":"<code>get_targets(fallback=True)</code>","text":"<p>Get a list of target names.</p> <p>Parameters:</p> Name Type Description Default <code>fallback</code> <code>bool</code> <p>If metadata for targets is missing, return a new list instead of None. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list of target names</p> Source code in <code>slisemap/utils.py</code> <pre><code>def get_targets(self, fallback: bool = True) -&gt; Optional[List[str]]:\n    \"\"\"Get a list of target names.\n\n    Args:\n        fallback: If metadata for targets is missing, return a new list instead of None. Defaults to True.\n\n    Returns:\n        list of target names\n    \"\"\"\n    if \"targets\" in self:\n        return self[\"targets\"]\n    elif fallback:\n        return [f\"Y_{i}\" for i in range(self.root.o)] if self.root.o &gt; 1 else [\"Y\"]\n    else:\n        return None\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.get_variables","title":"<code>get_variables(intercept=True, fallback=True)</code>","text":"<p>Get a list of variable names.</p> <p>Parameters:</p> Name Type Description Default <code>intercept</code> <code>bool</code> <p>include the intercept in the list. Defaults to True.</p> <code>True</code> <code>fallback</code> <code>bool</code> <p>If metadata for variables is missing, return a new list instead of None. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list of variable names</p> Source code in <code>slisemap/utils.py</code> <pre><code>def get_variables(\n    self, intercept: bool = True, fallback: bool = True\n) -&gt; Optional[List[str]]:\n    \"\"\"Get a list of variable names.\n\n    Args:\n        intercept: include the intercept in the list. Defaults to True.\n        fallback: If metadata for variables is missing, return a new list instead of None. Defaults to True.\n\n\n    Returns:\n        list of variable names\n    \"\"\"\n    if \"variables\" in self:\n        if self.root.intercept and not intercept:\n            return self[\"variables\"][:-1]\n        else:\n            return self[\"variables\"]\n    elif fallback:\n        if self.root.intercept:\n            if not intercept:\n                return [f\"X_{i}\" for i in range(self.root.m - 1)]\n            else:\n                return [f\"X_{i}\" for i in range(self.root.m - 1)] + [\"X_Intercept\"]\n        else:\n            return [f\"X_{i}\" for i in range(self.root.m)]\n    else:\n        return None\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.get_dimensions","title":"<code>get_dimensions(fallback=True, long=False)</code>","text":"<p>Get a list of dimension names.</p> <p>Parameters:</p> Name Type Description Default <code>fallback</code> <code>bool</code> <p>If metadata for dimensions is missing, return a new list instead of None. Defaults to True.</p> <code>True</code> <code>long</code> <code>bool</code> <p>Use \"SLISEMAP 1\",... as fallback instead of \"Z_0\",...</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list of dimension names</p> Source code in <code>slisemap/utils.py</code> <pre><code>def get_dimensions(\n    self, fallback: bool = True, long: bool = False\n) -&gt; Optional[List[str]]:\n    \"\"\"Get a list of dimension names.\n\n    Args:\n        fallback: If metadata for dimensions is missing, return a new list instead of None. Defaults to True.\n        long: Use \"SLISEMAP 1\",... as fallback instead of \"Z_0\",...\n\n    Returns:\n        list of dimension names\n    \"\"\"\n    if \"dimensions\" in self:\n        return self[\"dimensions\"]\n    elif fallback:\n        if long:\n            cls = \"Slisemap\" if self.root is None else type(self.root).__name__\n            return [f\"{cls} {i+1}\" for i in range(self.root.d)]\n        else:\n            return [f\"Z_{i}\" for i in range(self.root.d)]\n    else:\n        return None\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.get_rows","title":"<code>get_rows(fallback=True)</code>","text":"<p>Get a list of row names.</p> <p>Parameters:</p> Name Type Description Default <code>fallback</code> <code>bool</code> <p>If metadata for rows is missing, return a range instead of None. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Sequence[Any]]</code> <p>list (or range) of row names</p> Source code in <code>slisemap/utils.py</code> <pre><code>def get_rows(self, fallback: bool = True) -&gt; Optional[Sequence[Any]]:\n    \"\"\"Get a list of row names.\n\n    Args:\n        fallback: If metadata for rows is missing, return a range instead of None. Defaults to True.\n\n    Returns:\n        list (or range) of row names\n    \"\"\"\n    if \"rows\" in self:\n        return self[\"rows\"]\n    elif fallback:\n        return range(self.root.n)\n    else:\n        return None\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_scale_X","title":"<code>set_scale_X(center=None, scale=None)</code>","text":"<p>Set scaling information with checks.</p> <p>Use if <code>X</code> has been scaled before being input to Slisemap. Assuming the scaling can be converted to the form <code>X = (X_unscaled - center) / scale</code>. This allows some plots to (temporarily) revert the scaling (for more intuitive units).</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Union[None, Tensor, ndarray, Sequence[float]]</code> <p>The constant offset of <code>X</code>. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Union[None, Tensor, ndarray, Sequence[float]]</code> <p>The scaling factor of <code>X</code>. Defaults to None.</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_scale_X(\n    self,\n    center: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n    scale: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n) -&gt; None:\n    \"\"\"Set scaling information with checks.\n\n    Use if `X` has been scaled before being input to Slisemap.\n    Assuming the scaling can be converted to the form `X = (X_unscaled - center) / scale`.\n    This allows some plots to (temporarily) revert the scaling (for more intuitive units).\n\n    Args:\n        center: The constant offset of `X`. Defaults to None.\n        scale: The scaling factor of `X`. Defaults to None.\n    \"\"\"\n    if center is not None:\n        center = tonp(center).ravel()\n        assert center.size == self.root.m - self.root.intercept\n        self[\"X_center\"] = center\n    if scale is not None:\n        scale = tonp(scale).ravel()\n        assert scale.size == self.root.m - self.root.intercept\n        self[\"X_scale\"] = scale\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.set_scale_Y","title":"<code>set_scale_Y(center=None, scale=None)</code>","text":"<p>Set scaling information with checks.</p> <p>Use if <code>Y</code> has been scaled before being input to Slisemap. Assuming the scaling can be converted to the form <code>Y = (Y_unscaled - center) / scale</code>. This allows some plots to (temporarily) revert the scaling (for more intuitive units).</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Union[None, Tensor, ndarray, Sequence[float]]</code> <p>The constant offset of <code>Y</code>. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Union[None, Tensor, ndarray, Sequence[float]]</code> <p>The scaling factor of <code>Y</code>. Defaults to None.</p> <code>None</code> Source code in <code>slisemap/utils.py</code> <pre><code>def set_scale_Y(\n    self,\n    center: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n    scale: Union[None, torch.Tensor, np.ndarray, Sequence[float]] = None,\n) -&gt; None:\n    \"\"\"Set scaling information with checks.\n\n    Use if `Y` has been scaled before being input to Slisemap.\n    Assuming the scaling can be converted to the form `Y = (Y_unscaled - center) / scale`.\n    This allows some plots to (temporarily) revert the scaling (for more intuitive units).\n\n    Args:\n        center: The constant offset of `Y`. Defaults to None.\n        scale: The scaling factor of `Y`. Defaults to None.\n    \"\"\"\n    if center is not None:\n        center = tonp(center).ravel()\n        assert center.size == self.root.o\n        self[\"Y_center\"] = center\n    if scale is not None:\n        scale = tonp(scale).ravel()\n        assert scale.size == self.root.o\n        self[\"Y_scale\"] = scale\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.unscale_X","title":"<code>unscale_X(X=None)</code>","text":"<p>Unscale X if the scaling information has been given (see <code>set_scale_X</code>).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[ndarray]</code> <p>The data matrix X (or <code>self.root.get_X(intercept=False)</code> if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Possibly scaled X.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def unscale_X(self, X: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"Unscale X if the scaling information has been given (see `set_scale_X`).\n\n    Args:\n        X: The data matrix X (or `self.root.get_X(intercept=False)` if None).\n\n    Returns:\n        Possibly scaled X.\n    \"\"\"\n    if X is None:\n        X = self.root.get_X(intercept=False)\n    if \"X_scale\" in self:\n        X = X * self[\"X_scale\"][None, :]\n    if \"X_center\" in self:\n        X = X + self[\"X_center\"][None, :]\n    return X\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.Metadata.unscale_Y","title":"<code>unscale_Y(Y=None)</code>","text":"<p>Unscale Y if the scaling information has been given (see <code>set_scale_Y</code>).</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>Optional[ndarray]</code> <p>The response matrix Y (or <code>self.root.get_Y()</code> if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Possibly scaled Y.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def unscale_Y(self, Y: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"Unscale Y if the scaling information has been given (see `set_scale_Y`).\n\n    Args:\n        Y: The response matrix Y (or `self.root.get_Y()` if None).\n\n    Returns:\n        Possibly scaled Y.\n    \"\"\"\n    if Y is None:\n        Y = self.root.get_Y()\n    if \"Y_scale\" in self:\n        Y = Y * self[\"Y_scale\"][None, :]\n    if \"Y_center\" in self:\n        Y = Y + self[\"Y_center\"][None, :]\n    return Y\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.make_grid","title":"<code>make_grid(num=50, d=2, hex=True)</code>","text":"<p>Create a circular grid of points with radius 1.0.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>The approximate number of points. Defaults to 50.</p> <code>50</code> <code>d</code> <code>int</code> <p>The number of dimensions. Defaults to 2.</p> <code>2</code> <code>hex</code> <code>bool</code> <p>If <code>d == 2</code> produce a hexagonal grid instead of a rectangular grid. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of coordinates <code>[num, d]</code>.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def make_grid(num: int = 50, d: int = 2, hex: bool = True) -&gt; np.ndarray:\n    \"\"\"Create a circular grid of points with radius 1.0.\n\n    Args:\n        num: The approximate number of points. Defaults to 50.\n        d: The number of dimensions. Defaults to 2.\n        hex: If ``d == 2`` produce a hexagonal grid instead of a rectangular grid. Defaults to True.\n\n    Returns:\n        A matrix of coordinates `[num, d]`.\n    \"\"\"\n    _assert(d &gt; 0, \"The number of dimensions must be positive\", make_grid)\n    if d == 1:\n        return np.linspace(-1, 1, num)[:, None]\n    elif d == 2 and hex:\n        return make_hex_grid(num)\n    else:\n        nball_frac = np.pi ** (d / 2) / np.math.gamma(d / 2 + 1) / 2**d\n        if 4**d * nball_frac &gt; num:\n            _warn(\n                \"Too few grid points per dimension. Try reducing the number of dimensions or increase the number of points in the grid.\",\n                make_grid,\n            )\n        proto_1d = int(np.ceil((num / nball_frac) ** (1 / d))) // 2 * 2 + 2\n        grid_1d = np.linspace(-0.9999, 0.9999, proto_1d)\n        grid = np.stack(np.meshgrid(*(grid_1d for _ in range(d))), -1).reshape((-1, d))\n        dist = np.sum(grid**2, 1)\n        q = np.quantile(dist, num / len(dist)) + np.finfo(dist.dtype).eps ** 0.5\n        grid = grid[dist &lt;= q]\n        return grid / np.quantile(grid, 0.99)\n</code></pre>"},{"location":"slisemap.utils/#slisemap.utils.make_hex_grid","title":"<code>make_hex_grid(num=52)</code>","text":"<p>Create a circular grid of 2D points with a hexagon pattern and radius 1.0.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>The approximate number of points. Defaults to 52.</p> <code>52</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of coordinates <code>[num, 2]</code>.</p> Source code in <code>slisemap/utils.py</code> <pre><code>def make_hex_grid(num: int = 52) -&gt; np.ndarray:\n    \"\"\"Create a circular grid of 2D points with a hexagon pattern and radius 1.0.\n\n    Args:\n        num: The approximate number of points. Defaults to 52.\n\n    Returns:\n        A matrix of coordinates `[num, 2]`.\n    \"\"\"\n    num_h = int(np.ceil(np.sqrt(num * 4 / np.pi))) // 2 * 2 + 3\n    grid_h, height = np.linspace(-0.9999, 0.9999, num_h, retstep=True)\n    width = height * 2 / 3 * np.sqrt(3)\n    num_w = int(np.ceil(1.0 / width))\n    grid_w = np.arange(-num_w, num_w + 1) * width\n    grid = np.stack(np.meshgrid(grid_w, grid_h), -1)\n    grid[(1 - num_h // 2 % 2) :: 2, :, 0] += width / 2\n    grid = grid.reshape((-1, 2))\n    best = None\n    for origo in (0.0, 0.5 * width):\n        if origo != 0.0:\n            grid[:, 0] += origo\n        dist = np.sum(grid**2, 1)\n        q = np.quantile(dist, num / len(dist))\n        for epsilon in (-1e-4, 1e-4):\n            grid2 = grid[dist &lt;= q + epsilon]\n            if best is None or abs(best.shape[0] - num) &gt; abs(grid2.shape[0] - num):\n                best = grid2.copy()\n    return best / np.quantile(best, 0.99)\n</code></pre>"}]}