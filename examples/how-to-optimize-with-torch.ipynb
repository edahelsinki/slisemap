{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4760bcc3",
   "metadata": {},
   "source": [
    "# How to optimise with Torch\n",
    "Kai Puolam√§ki, 28 June 2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This brief tutorial demonstrates how [PyTorch](https://pytorch.org) can be used to find minimum values of arbitrary functions, as is done in [SLISEMAP](https://github.com/edahelsinki/slisemap). The advantages of PyTorch include the use of autograd and optionally GPU acceleration. These may result in significant speedups when optimising high-dimensional loss functions, often in deep learning and elsewhere.\n",
    "\n",
    "The existing documentation of PyTorch is geared towards deep learning. It is currently difficult to find documentation of how to do \"simple\" optimisation without any deep learning context, which is why I wrote this tutorial in the hope that it will be helpful for someone.\n",
    "\n",
    "## Toy example\n",
    "\n",
    "Here we minimise a simple regularised least squares loss given by\n",
    "$$\n",
    "L = \\lVert {\\bf y}-{\\bf X}{\\bf b} \\rVert_2^2+\\lVert{\\bf{b}}\\rVert_2^2/10,\n",
    "$$\n",
    "where ${\\bf X}\\in{\\mathbb{R}}^{3\\times 2}$ and ${\\bf y}\\in{\\mathbb{R}}^3$ are constants and ${\\bf{b}}\\in{\\mathbb{R}}^2$ is a vector whose values are to be found by the optimiser. We could optimise any reasonably behaving function; here, we picked the least squares loss for simplicity.\n",
    "\n",
    "In this example, we use the following values for the constant matrix and vector:\n",
    "$$\n",
    "{\\bf{X}}=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 3.14159\n",
    "\\end{pmatrix},~~~~\n",
    "{\\bf{y}}=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ 2 \\\\ 3 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "In this example, the loss $L$ obtains a minimal value of $L=0.0887$ when ${\\bf{b}}=\\left(0.141,0.906\\right)^T$.\n",
    "\n",
    "## Numpy and Scipy\n",
    "\n",
    "We first solve the problem with the [standard `scipy` optimiser](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), by using an arbitrarily chosen initial starting point.\n",
    "\n",
    "We first define the matrices and vectors as Numpy arrays and then define a loss function `loss_fn0` that takes the value of ${\\bf{b}}$ as input and outputs the value of the loss $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08460753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nfrom scipy.optimize import minimize\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nfrom scipy.optimize import minimize\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8320242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027434485368359"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\\ny0 = np.array([1, 2, 3], dtype=float)\\nb0 = np.array([0.12, 0.34], dtype=float)\\n\\n\\ndef loss_fn0(b):\\n    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\nloss_fn0(b0)\";\n",
       "                var nbb_formatted_code = \"X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\\ny0 = np.array([1, 2, 3], dtype=float)\\nb0 = np.array([0.12, 0.34], dtype=float)\\n\\n\\ndef loss_fn0(b):\\n    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\nloss_fn0(b0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\n",
    "y0 = np.array([1, 2, 3], dtype=float)\n",
    "b0 = np.array([0.12, 0.34], dtype=float)\n",
    "\n",
    "\n",
    "def loss_fn0(b):\n",
    "    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\n",
    "\n",
    "\n",
    "loss_fn0(b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c5414",
   "metadata": {},
   "source": [
    "For this starting point, the loss value is $L=5.027$, which is larger than the optimal value.\n",
    "\n",
    "In this case, we can find the value of ${\\bf{b}}$ that minimizes the loss $L$ by using a library optimization algorithm, BFGS. We see the correct value of ${\\bf{b}}$ and the corresponding loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0c87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14119498, 0.90567679]), 0.08865057718228184)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"res0 = minimize(loss_fn0, b0, method=\\\"BFGS\\\")\\nres0.x, loss_fn0(res0.x)\";\n",
       "                var nbb_formatted_code = \"res0 = minimize(loss_fn0, b0, method=\\\"BFGS\\\")\\nres0.x, loss_fn0(res0.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res0 = minimize(loss_fn0, b0, method=\"BFGS\")\n",
    "res0.x, loss_fn0(res0.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a285d",
   "metadata": {},
   "source": [
    "It is always good to check if we have converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e14355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optimization terminated successfully.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"res0.message\";\n",
       "                var nbb_formatted_code = \"res0.message\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res0.message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdafca",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "We'll repeat the same with Pytorch. First, we define a helper function `LBFGS` that takes in the loss function and the variables to be optimised as input and that, as a side effect, updates the variables to their values at the minimum of the loss function.\n",
    "\n",
    "The helper function uses the [Torch LBFGS optimiser](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html). The `closure` is a function that essentially evaluates the loss function and updates the gradient values. \n",
    "\n",
    "You can use this helper function as a generic optimiser, much like you would use the `scipy. optimise. minimise` above by just cutting and pasting the LBGGS helper function into your code. The file [utils.py](https://github.com/edahelsinki/slisemap/blob/main/slisemap/utils.py) in the SLISEMAP source code contains a more advanced version of the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487eea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import torch\\n\\n\\ndef LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\\\"strong_wolfe\\\", **kwargs):\\n    \\\"\\\"\\\"Optimise a function using LBFGS.\\n    Args:\\n        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\\n        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\\n        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\\n        line_search_fn (Optional[str], optional): Line search method (None or \\\"strong_wolfe\\\"). Defaults to \\\"strong_wolfe\\\".\\n        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\\n    Returns:\\n        torch.optim.LBFGS: The LBFGS optimiser.\\n    \\\"\\\"\\\"\\n\\n    optimiser = torch.optim.LBFGS(\\n        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\\n    )\\n\\n    def closure():\\n        optimiser.zero_grad()\\n        loss = loss_fn()\\n        loss.backward()\\n        return loss\\n\\n    optimiser.step(closure)\\n\\n    return optimiser\";\n",
       "                var nbb_formatted_code = \"import torch\\n\\n\\ndef LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\\\"strong_wolfe\\\", **kwargs):\\n    \\\"\\\"\\\"Optimise a function using LBFGS.\\n    Args:\\n        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\\n        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\\n        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\\n        line_search_fn (Optional[str], optional): Line search method (None or \\\"strong_wolfe\\\"). Defaults to \\\"strong_wolfe\\\".\\n        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\\n    Returns:\\n        torch.optim.LBFGS: The LBFGS optimiser.\\n    \\\"\\\"\\\"\\n\\n    optimiser = torch.optim.LBFGS(\\n        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\\n    )\\n\\n    def closure():\\n        optimiser.zero_grad()\\n        loss = loss_fn()\\n        loss.backward()\\n        return loss\\n\\n    optimiser.step(closure)\\n\\n    return optimiser\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\"strong_wolfe\", **kwargs):\n",
    "    \"\"\"Optimise a function using LBFGS.\n",
    "    Args:\n",
    "        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\n",
    "        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\n",
    "        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\n",
    "        line_search_fn (Optional[str], optional): Line search method (None or \"strong_wolfe\"). Defaults to \"strong_wolfe\".\n",
    "        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\n",
    "    Returns:\n",
    "        torch.optim.LBFGS: The LBFGS optimiser.\n",
    "    \"\"\"\n",
    "\n",
    "    optimiser = torch.optim.LBFGS(\n",
    "        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\n",
    "    )\n",
    "\n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "        loss = loss_fn()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimiser.step(closure)\n",
    "\n",
    "    return optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f70091",
   "metadata": {},
   "source": [
    "Torch functions typically require that we define the variables as torch tensors. The torch tensors correspond to Numpy arrays, but they carry autograd information and can optionally be used within a GPU. Notice that we need to attach the slot for the gradients to ${\\bf{b}}$ tensor because we want to optimize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed52b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [1.0000, 2.0000],\n",
       "         [1.0000, 3.1416]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([0.1200, 0.3400], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"X = torch.tensor(X0, dtype=torch.float)\\ny = torch.tensor(y0, dtype=torch.float)\\nb = torch.tensor(b0, dtype=torch.float, requires_grad=True)\\nX, y, b\";\n",
       "                var nbb_formatted_code = \"X = torch.tensor(X0, dtype=torch.float)\\ny = torch.tensor(y0, dtype=torch.float)\\nb = torch.tensor(b0, dtype=torch.float, requires_grad=True)\\nX, y, b\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.tensor(X0, dtype=torch.float)\n",
    "y = torch.tensor(y0, dtype=torch.float)\n",
    "b = torch.tensor(b0, dtype=torch.float, requires_grad=True)\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4abb6d",
   "metadata": {},
   "source": [
    "The safe way to make Torch tensors Numpy arrays is first to move them to the CPU, then detach any autograd part and then make them NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1447c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 1.     ],\n",
       "       [1.     , 2.     ],\n",
       "       [1.     , 3.14159]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"X.cpu().detach().numpy()\";\n",
       "                var nbb_formatted_code = \"X.cpu().detach().numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc273b6d",
   "metadata": {},
   "source": [
    "Next, we define the loss function that takes no parameters as an input and outputs the loss (a tensor with only one real number as a value). If you want to evaluate the value of loss for different values of ${\\bf{b}}$ you must update the values in the corresponding tensor.\n",
    "\n",
    "It is essential to use only Torch arithmetic operations that support autograd. Luckily, there are enough operations to cover most needs. Instead of the `sum` method in the [Tensor object](https://pytorch.org/docs/stable/tensors.html) as in the first example below, we can alternatively use [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html) (both of which support torch tensors and autograd), but we cannot use, e.g., [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) (which does not support torch tensors and autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e766155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"## Using the sum method in the Tensor object:\\ndef loss_fn():\\n    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\n## Alternate but equivalent way of writing the same thing by using torch.sum:\\ndef loss_fn():\\n    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\\n\\n\\n## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\\ndef loss_fn_WRONG_DO_NOT_USE():\\n    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0\";\n",
       "                var nbb_formatted_code = \"## Using the sum method in the Tensor object:\\ndef loss_fn():\\n    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\n## Alternate but equivalent way of writing the same thing by using torch.sum:\\ndef loss_fn():\\n    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\\n\\n\\n## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\\ndef loss_fn_WRONG_DO_NOT_USE():\\n    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Using the sum method in the Tensor object:\n",
    "def loss_fn():\n",
    "    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\n",
    "\n",
    "\n",
    "## Alternate but equivalent way of writing the same thing by using torch.sum:\n",
    "def loss_fn():\n",
    "    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\n",
    "\n",
    "\n",
    "## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\n",
    "def loss_fn_WRONG_DO_NOT_USE():\n",
    "    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d50a1",
   "metadata": {},
   "source": [
    "Evaluating the loss function gives the value of the loss (as a tensor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a83e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0274, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"loss_fn()\";\n",
       "                var nbb_formatted_code = \"loss_fn()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e9da",
   "metadata": {},
   "source": [
    "If we want the loss value as a real number, the correct procedure is first to move the tensor to the CPU; this matters if we use GPU; otherwise, it is a null operation. Afterwards, we can detach the autograd component and take the only item as a real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf39e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027434349060059"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_formatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe981",
   "metadata": {},
   "source": [
    "We use the helper function `LBFGS` defined above to do the optimization. We need to give as parameters the loss function and a list of tensors to be optimized. As a result, the tensor ${\\bf{b}}$ is updated to the value that minimizes the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd29b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [1.0000, 2.0000],\n",
       "         [1.0000, 3.1416]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([0.1412, 0.9057], requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"res = LBFGS(loss_fn, [b])\\nX, y, b\";\n",
       "                var nbb_formatted_code = \"res = LBFGS(loss_fn, [b])\\nX, y, b\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = LBFGS(loss_fn, [b])\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47801939",
   "metadata": {},
   "source": [
    "The optimum value of the loss function is the same as in the first example with Numpy and the standard Scipy optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6cb1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08865057677030563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_formatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcb966",
   "metadata": {},
   "source": [
    "Again, it is good to check that the optimisation converged successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defbc653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"res.state_dict()[\\\"state\\\"][0][\\\"n_iter\\\"]\";\n",
       "                var nbb_formatted_code = \"res.state_dict()[\\\"state\\\"][0][\\\"n_iter\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.state_dict()[\"state\"][0][\"n_iter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf824a75",
   "metadata": {},
   "source": [
    "The optimisation took six iterations (cutoff being 500). Therefore, the optimisation was probably terminated due to convergence to a local minimum, and we should be fine. If there is no convergence, you may need to increase the cutoff or study the matter further (e.g., the loss to be optimised could be badly behaving)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91158601",
   "metadata": {},
   "source": [
    "## Addendum: differences between \"conventional\" optimisation and optimisation in deep learning\n",
    "\n",
    "Optimisation aims to find parameter values that minimise the value of a given target function (loss). When the parameters of the target function are real-valued numbers, then typically, gradient-based optimisers are used. \n",
    "\n",
    "In deep learning applications, the loss to be optimised is typically, e.g., classification error of the deep learning networks and the parameters are weights in the network. Below, I list some practical differences between optimisation in deep learning and more traditional optimisation problems.\n",
    "\n",
    "\n",
    "### Stochastic gradient algorithms are more prevalent in deep learning\n",
    "\n",
    "Deep learning problems are typically high dimensional (meaning there are many parameters). Stochastic gradient-based algorithms scale well for high-dimensional datasets, while more conventional optimisation methods may become too slow. However, the stochastic gradient-based algorithms may be slower to converge for lower-dimensional problems. LBFGS (which is not based on stochastic gradient) is an excellent conventional optimiser and might be a better default choice for traditional problems with a reasonable number of parameters to be optimised.\n",
    "\n",
    "\n",
    "### In deep learning, we do not want to find the minimum\n",
    "\n",
    "In deep learning, we do not usually want to find the parameter values that minimise the loss because this may result in overfitting the training data. Instead, gradient optimisation is typically run iteratively step by step. The optimisation is stopped when validation loss stops decreasing. A typical Torch workflow ` optimiser.step(closure)` would be run repeatedly until validation loss stops falling. \n",
    "\n",
    "A more traditional approach for optimisation (also used by the Scipy `minimise` above) is to run the optimiser until the pre-defined stopping criteria are met. It typically means that the solution is no longer improved, indicating that the optimiser has found a local minimum of the loss function; this is what our LBFGS helper function does. We need to run LBFGS optimiser only one step, which is, in most cases, enough to converge because the default maximum number of iterations within the step is in the LBFGS function set to 500, Torch default being 20. Often, the optimiser stops before 500 steps are used after convergence, but you should check this.\n",
    "\n",
    "### In deep learning, speed is considered more important than stability or robustness\n",
    "\n",
    "In deep learning, the stability or robustness of the algorithm is often considered less important than scalability. If the optimisation does not converge, it can be just restarted with different parameters, while in a more conventional setup, you would be happy if the optimiser behaves predictably. You do not have to fiddle with the parameters. Therefore line search is not by default used in Torch LBFGS optimiser. I have added a line search option, which guarantees that the value of the loss function does not increase at any iteration, resulting in better numerical stability with a slightly longer runtime penalty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm-env-1",
   "language": "python",
   "name": "atm-env-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
