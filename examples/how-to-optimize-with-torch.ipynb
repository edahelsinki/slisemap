{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4760bcc3",
   "metadata": {},
   "source": [
    "# How to optimize with Torch\n",
    "Kai Puolam√§ki, 28 June 2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This brief tutorial demonstrates how [PyTorch](https://pytorch.org) can be used to find minimum values of arbitrary functions, as is done in [SLISEMAP](https://github.com/edahelsinki/slisemap). The advantages of PyTorch include the use of autograd and optionally GPU acceleration. These may result in significant speedups when optimizing high-dimensional loss functions, which often happens in deep learning but also elsewhere.\n",
    "\n",
    "The existing documentation of PyTorch is geared towards deep learning. It is currently difficult to find documentation of how to do \"simple\" optimization without any deep learning context, which is why I wrote this tutorial in the hope that it will be useful for someone.\n",
    "\n",
    "## Toy example\n",
    "\n",
    "Here we minimise a simple least squares loss given by\n",
    "$$\n",
    "L = \\lVert {\\bf y}-{\\bf X}{\\bf b} \\rVert_2^2,\n",
    "$$\n",
    "where ${\\bf X}\\in{\\mathbb{R}}^{3\\times 2}$ and ${\\bf y}\\in{\\mathbb{R}}^3$ are constants and ${\\bf{b}}\\in{\\mathbb{R}}^2$ is a vector whose values are to be found by the optimiser. We could optimize any reasonably behaving function; here we picked the least squares loss for simplicity.\n",
    "\n",
    "In this example, we use the following values for the constant matrix and vector:\n",
    "$$\n",
    "{\\bf{X}}=\n",
    "\\begin{pmatrix}\n",
    "1 & 2.2 \\\\\n",
    "1 & 3 \\\\\n",
    "1 & 4.4\n",
    "\\end{pmatrix},~~~~\n",
    "{\\bf{y}}=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ 2 \\\\ 3 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "In this example, the loss $L$ obtains a minimal value of $L=0.0484$ when ${\\bf{b}}=\\left(-0.839,0.887\\right)^T$.\n",
    "\n",
    "## Numpy and Scipy\n",
    "\n",
    "We first solve the problem with the standard `scipy` optimizer, by using an arbitrarily chosen initial starting point.\n",
    "\n",
    "We first define the matrices and vectors as Numpy arrays and then define a loss function `loss_fn0` that that takes the value of ${\\bf{b}}$ as input and outputs the value of the loss $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08460753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8320242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.672479999999999"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = np.array([[1,2.2], [1,3], [1,4.4]], dtype=float)\n",
    "y0 = np.array([1,2,3], dtype=float)\n",
    "b0 = np.array([0.12,0.34], dtype=float)\n",
    "\n",
    "def loss_fn0(b):\n",
    "    return ((y0 - X0 @ b)**2).sum()\n",
    "\n",
    "loss_fn0(b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c5414",
   "metadata": {},
   "source": [
    "For this starting point the loss value is $L=2.672$, which is clearly larger than the optimal value.\n",
    "\n",
    "We can find the value of ${\\bf{b}}$ that minimizes the loss $L$ by using a library optimization algorithm, BFGS in this case. We find the correct value of ${\\bf{b}}$ and the corresponding loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0c87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.83870945,  0.8870967 ]), 0.04838709677420688)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(loss_fn0, b0, method=\"BFGS\")\n",
    "res.x, loss_fn0(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdafca",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "We'll repeat the same with Pytorch. First we define a helper function `LBFGS` that takes in the loss function and the variables to be optimized as input and that as a side effect updates the variables to their values at the minimum of the loss function.\n",
    "\n",
    "The helper function uses the Torch LBFGS optimizer. The `closure` is a function that essentially evaluates the loss function and updates the gradient values. The file [utils.py](https://github.com/edahelsinki/slisemap/blob/main/slisemap/utils.py) in the SLISEMAP source code contains a more advanced version of the helper function.\n",
    "\n",
    "You can use this helper function as a generic optimizer, much in the same way as you would use the `scipy.optimize.minimize` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487eea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\"strong_wolfe\", **kwargs):\n",
    "    \"\"\"Optimise a function using LBFGS.\n",
    "    Args:\n",
    "        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\n",
    "        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\n",
    "        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\n",
    "        line_search_fn (Optional[str], optional): Line search method (None or \"strong_wolfe\"). Defaults to \"strong_wolfe\".\n",
    "        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\n",
    "    Returns:\n",
    "        torch.optim.LBFGS: The LBFGS optimiser.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimiser = torch.optim.LBFGS(\n",
    "        variables,\n",
    "        max_iter=max_iter,\n",
    "        line_search_fn=line_search_fn,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "        loss = loss_fn()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimiser.step(closure)\n",
    "    \n",
    "    return optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f70091",
   "metadata": {},
   "source": [
    "Torch functions typically require that we define the variables torch tensors. The torch tensors correspond to Numpy arrays, but they carry autograd information and they can optionally be used within a GPU. Notice that we need to attach the slot for the gradients to ${\\bf{b}}$ tensor because we want to optimize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed52b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 2.2000],\n",
       "         [1.0000, 3.0000],\n",
       "         [1.0000, 4.4000]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([0.1200, 0.3400], requires_grad=True))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(X0, dtype=torch.float)\n",
    "y = torch.tensor(y0, dtype=torch.float)\n",
    "b = torch.tensor(b0, dtype=torch.float, requires_grad=True)\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4abb6d",
   "metadata": {},
   "source": [
    "The safe way to make Torch tensors Numpy arrays is to first move them to CPU, then detach any autograd part, and then make them numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1447c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2.2],\n",
       "       [1. , 3. ],\n",
       "       [1. , 4.4]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc273b6d",
   "metadata": {},
   "source": [
    "Next, we define the loss function that takes no parameters as an input and which outputs the loss (a tensor with only one real number as a value). If you want to evaluate the value of loss for different values of ${\\bf{b}}$ you must update the values in the corresponding tensor.\n",
    "\n",
    "It is important to use only Torch arithmetic operations that support autograd. Luckily, there are enough operations to cover most needs. Instead of `sum` method in the [Tensor object](https://pytorch.org/docs/stable/tensors.html) as in the first example below we can alternatively use [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html) (both of which supports torch tensors and autograd), but we cannot use, e.g., [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) (which does not support torch tensors and autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e766155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the sum method in the Tensor object:\n",
    "def loss_fn():\n",
    "    return ((y - X @ b)**2).sum()\n",
    "\n",
    "## Alternate but equivalent way of writing the same thing by using torch.sum:\n",
    "def loss_fn():\n",
    "    return torch.sum((y - X @ b)**2)\n",
    "\n",
    "## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\n",
    "def loss_fn_WRONG_DO_NOT_USE():\n",
    "    return np.sum((y - X @ b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d50a1",
   "metadata": {},
   "source": [
    "Evaluating the loss function gives the value of the loss (as a tensor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a83e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6725, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e9da",
   "metadata": {},
   "source": [
    "If we want to have the loss value as a real number then the correct procedure is to first move the tensor to CPU (this matters if we use GPU, otherwise it is a null operation), then detach the autograd component, and then take the only item out as a real number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf39e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6724798679351807"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe981",
   "metadata": {},
   "source": [
    "We use the helper function `LBFGS` defined above to do the optimization. We need to give as parameters the loss function and a list of tensors to be optimized. As a result, the value of the tensor ${\\bf{b}}$ is updated to the value that minimizes the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd29b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 2.2000],\n",
       "         [1.0000, 3.0000],\n",
       "         [1.0000, 4.4000]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([-0.8387,  0.8871], requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LBFGS(loss_fn, [b])\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47801939",
   "metadata": {},
   "source": [
    "The optimum value of the loss function is the same as in the first example with Numpy and standard Scipy optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6cb1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048387106508016586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm-env-1",
   "language": "python",
   "name": "atm-env-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
