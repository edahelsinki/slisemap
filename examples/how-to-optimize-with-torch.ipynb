{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4760bcc3",
   "metadata": {},
   "source": [
    "# How to optimize with Torch\n",
    "Kai Puolam√§ki, 28 June 2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This brief tutorial demonstrates how [PyTorch](https://pytorch.org) can be used to find minimum values of arbitrary functions, as is done in [SLISEMAP](https://github.com/edahelsinki/slisemap). The advantages of PyTorch include the use of autograd and optionally GPU acceleration. These may result in significant speedups when optimizing high-dimensional loss functions, which often happens in deep learning but also elsewhere.\n",
    "\n",
    "The existing documentation of PyTorch is geared towards deep learning. It is currently difficult to find documentation of how to do \"simple\" optimization without any deep learning context, which is why I wrote this tutorial in the hope that it will be useful for someone.\n",
    "\n",
    "## Toy example\n",
    "\n",
    "Here we minimise a simple regularized least squares loss given by\n",
    "$$\n",
    "L = \\lVert {\\bf y}-{\\bf X}{\\bf b} \\rVert_2^2+\\lVert{\\bf{b}}\\rVert_2^2/10,\n",
    "$$\n",
    "where ${\\bf X}\\in{\\mathbb{R}}^{3\\times 2}$ and ${\\bf y}\\in{\\mathbb{R}}^3$ are constants and ${\\bf{b}}\\in{\\mathbb{R}}^2$ is a vector whose values are to be found by the optimiser. We could optimize any reasonably behaving function; here we picked the least squares loss for simplicity.\n",
    "\n",
    "In this example, we use the following values for the constant matrix and vector:\n",
    "$$\n",
    "{\\bf{X}}=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 3.14159\n",
    "\\end{pmatrix},~~~~\n",
    "{\\bf{y}}=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ 2 \\\\ 3 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "In this example, the loss $L$ obtains a minimal value of $L=0.0887$ when ${\\bf{b}}=\\left(0.141,0.906\\right)^T$.\n",
    "\n",
    "## Numpy and Scipy\n",
    "\n",
    "We first solve the problem with the [standard `scipy` optimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), by using an arbitrarily chosen initial starting point.\n",
    "\n",
    "We first define the matrices and vectors as Numpy arrays and then define a loss function `loss_fn0` that that takes the value of ${\\bf{b}}$ as input and outputs the value of the loss $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08460753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nfrom scipy.optimize import minimize\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nfrom scipy.optimize import minimize\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8320242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027434485368359"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\\ny0 = np.array([1, 2, 3], dtype=float)\\nb0 = np.array([0.12, 0.34], dtype=float)\\n\\n\\ndef loss_fn0(b):\\n    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\nloss_fn0(b0)\";\n",
       "                var nbb_formatted_code = \"X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\\ny0 = np.array([1, 2, 3], dtype=float)\\nb0 = np.array([0.12, 0.34], dtype=float)\\n\\n\\ndef loss_fn0(b):\\n    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\nloss_fn0(b0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X0 = np.array([[1, 1], [1, 2], [1, 3.14159]], dtype=float)\n",
    "y0 = np.array([1, 2, 3], dtype=float)\n",
    "b0 = np.array([0.12, 0.34], dtype=float)\n",
    "\n",
    "\n",
    "def loss_fn0(b):\n",
    "    return ((y0 - X0 @ b) ** 2).sum() + (b**2).sum() / 10.0\n",
    "\n",
    "\n",
    "loss_fn0(b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c5414",
   "metadata": {},
   "source": [
    "For this starting point the loss value is $L=5.027$, which is clearly larger than the optimal value.\n",
    "\n",
    "We can find the value of ${\\bf{b}}$ that minimizes the loss $L$ by using a library optimization algorithm, BFGS in this case. We find the correct value of ${\\bf{b}}$ and the corresponding loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0c87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14119498, 0.90567679]), 0.08865057718228184)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"res0 = minimize(loss_fn0, b0, method=\\\"BFGS\\\")\\nres0.x, loss_fn0(res0.x)\";\n",
       "                var nbb_formatted_code = \"res0 = minimize(loss_fn0, b0, method=\\\"BFGS\\\")\\nres0.x, loss_fn0(res0.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res0 = minimize(loss_fn0, b0, method=\"BFGS\")\n",
    "res0.x, loss_fn0(res0.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a285d",
   "metadata": {},
   "source": [
    "It is always good to check if we have converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e14355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optimization terminated successfully.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"res0.message\";\n",
       "                var nbb_formatted_code = \"res0.message\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res0.message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdafca",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "We'll repeat the same with Pytorch. First we define a helper function `LBFGS` that takes in the loss function and the variables to be optimized as input and that as a side effect updates the variables to their values at the minimum of the loss function.\n",
    "\n",
    "The helper function uses the [Torch LBFGS optimizer](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html). The `closure` is a function that essentially evaluates the loss function and updates the gradient values. \n",
    "\n",
    "You can use this helper function as a generic optimizer, much in the same way as you would use the `scipy.optimize.minimize` above by just cutting-and-pasting the LBGGS helper function into your code. The file [utils.py](https://github.com/edahelsinki/slisemap/blob/main/slisemap/utils.py) in the SLISEMAP source code contains a more advanced version of the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487eea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import torch\\n\\n\\ndef LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\\\"strong_wolfe\\\", **kwargs):\\n    \\\"\\\"\\\"Optimise a function using LBFGS.\\n    Args:\\n        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\\n        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\\n        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\\n        line_search_fn (Optional[str], optional): Line search method (None or \\\"strong_wolfe\\\"). Defaults to \\\"strong_wolfe\\\".\\n        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\\n    Returns:\\n        torch.optim.LBFGS: The LBFGS optimiser.\\n    \\\"\\\"\\\"\\n\\n    optimiser = torch.optim.LBFGS(\\n        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\\n    )\\n\\n    def closure():\\n        optimiser.zero_grad()\\n        loss = loss_fn()\\n        loss.backward()\\n        return loss\\n\\n    optimiser.step(closure)\\n\\n    return optimiser\";\n",
       "                var nbb_formatted_code = \"import torch\\n\\n\\ndef LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\\\"strong_wolfe\\\", **kwargs):\\n    \\\"\\\"\\\"Optimise a function using LBFGS.\\n    Args:\\n        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\\n        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\\n        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\\n        line_search_fn (Optional[str], optional): Line search method (None or \\\"strong_wolfe\\\"). Defaults to \\\"strong_wolfe\\\".\\n        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\\n    Returns:\\n        torch.optim.LBFGS: The LBFGS optimiser.\\n    \\\"\\\"\\\"\\n\\n    optimiser = torch.optim.LBFGS(\\n        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\\n    )\\n\\n    def closure():\\n        optimiser.zero_grad()\\n        loss = loss_fn()\\n        loss.backward()\\n        return loss\\n\\n    optimiser.step(closure)\\n\\n    return optimiser\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def LBFGS(loss_fn, variables, max_iter=500, line_search_fn=\"strong_wolfe\", **kwargs):\n",
    "    \"\"\"Optimise a function using LBFGS.\n",
    "    Args:\n",
    "        loss_fn (Callable[[], torch.Tensor]): Function that returns a value to be minimised.\n",
    "        variables (List[torch.Tensor]): List of variables to optimise (must have `requires_grad=True`).\n",
    "        max_iter (int, optional): Maximum number of LBFGS iterations. Defaults to 500.\n",
    "        line_search_fn (Optional[str], optional): Line search method (None or \"strong_wolfe\"). Defaults to \"strong_wolfe\".\n",
    "        **kwargs (optional): Argumemts passed to `torch.optim.LBFGS`.\n",
    "    Returns:\n",
    "        torch.optim.LBFGS: The LBFGS optimiser.\n",
    "    \"\"\"\n",
    "\n",
    "    optimiser = torch.optim.LBFGS(\n",
    "        variables, max_iter=max_iter, line_search_fn=line_search_fn, **kwargs\n",
    "    )\n",
    "\n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "        loss = loss_fn()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimiser.step(closure)\n",
    "\n",
    "    return optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f70091",
   "metadata": {},
   "source": [
    "Torch functions typically require that we define the variables torch tensors. The torch tensors correspond to Numpy arrays, but they carry autograd information and they can optionally be used within a GPU. Notice that we need to attach the slot for the gradients to ${\\bf{b}}$ tensor because we want to optimize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed52b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [1.0000, 2.0000],\n",
       "         [1.0000, 3.1416]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([0.1200, 0.3400], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"X = torch.tensor(X0, dtype=torch.float)\\ny = torch.tensor(y0, dtype=torch.float)\\nb = torch.tensor(b0, dtype=torch.float, requires_grad=True)\\nX, y, b\";\n",
       "                var nbb_formatted_code = \"X = torch.tensor(X0, dtype=torch.float)\\ny = torch.tensor(y0, dtype=torch.float)\\nb = torch.tensor(b0, dtype=torch.float, requires_grad=True)\\nX, y, b\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.tensor(X0, dtype=torch.float)\n",
    "y = torch.tensor(y0, dtype=torch.float)\n",
    "b = torch.tensor(b0, dtype=torch.float, requires_grad=True)\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4abb6d",
   "metadata": {},
   "source": [
    "The safe way to make Torch tensors Numpy arrays is to first move them to CPU, then detach any autograd part, and then make them numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1447c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 1.     ],\n",
       "       [1.     , 2.     ],\n",
       "       [1.     , 3.14159]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"X.cpu().detach().numpy()\";\n",
       "                var nbb_formatted_code = \"X.cpu().detach().numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc273b6d",
   "metadata": {},
   "source": [
    "Next, we define the loss function that takes no parameters as an input and which outputs the loss (a tensor with only one real number as a value). If you want to evaluate the value of loss for different values of ${\\bf{b}}$ you must update the values in the corresponding tensor.\n",
    "\n",
    "It is important to use only Torch arithmetic operations that support autograd. Luckily, there are enough operations to cover most needs. Instead of `sum` method in the [Tensor object](https://pytorch.org/docs/stable/tensors.html) as in the first example below we can alternatively use [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html) (both of which supports torch tensors and autograd), but we cannot use, e.g., [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) (which does not support torch tensors and autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e766155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"## Using the sum method in the Tensor object:\\ndef loss_fn():\\n    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\n## Alternate but equivalent way of writing the same thing by using torch.sum:\\ndef loss_fn():\\n    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\\n\\n\\n## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\\ndef loss_fn_WRONG_DO_NOT_USE():\\n    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0\";\n",
       "                var nbb_formatted_code = \"## Using the sum method in the Tensor object:\\ndef loss_fn():\\n    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\\n\\n\\n## Alternate but equivalent way of writing the same thing by using torch.sum:\\ndef loss_fn():\\n    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\\n\\n\\n## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\\ndef loss_fn_WRONG_DO_NOT_USE():\\n    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Using the sum method in the Tensor object:\n",
    "def loss_fn():\n",
    "    return ((y - X @ b) ** 2).sum() + (b**2).sum() / 10.0\n",
    "\n",
    "\n",
    "## Alternate but equivalent way of writing the same thing by using torch.sum:\n",
    "def loss_fn():\n",
    "    return torch.sum((y - X @ b) ** 2) + torch.sum(b**2) / 10.0\n",
    "\n",
    "\n",
    "## You cannot use, e.g., Numpy operations which do not support tensors and autograd:\n",
    "def loss_fn_WRONG_DO_NOT_USE():\n",
    "    return np.sum((y - X @ b) ** 2) + np.sum(b**2) / 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d50a1",
   "metadata": {},
   "source": [
    "Evaluating the loss function gives the value of the loss (as a tensor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a83e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0274, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"loss_fn()\";\n",
       "                var nbb_formatted_code = \"loss_fn()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e9da",
   "metadata": {},
   "source": [
    "If we want to have the loss value as a real number then the correct procedure is to first move the tensor to CPU (this matters if we use GPU, otherwise it is a null operation), then detach the autograd component, and then take the only item out as a real number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf39e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027434349060059"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_formatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe981",
   "metadata": {},
   "source": [
    "We use the helper function `LBFGS` defined above to do the optimization. We need to give as parameters the loss function and a list of tensors to be optimized. As a result, the value of the tensor ${\\bf{b}}$ is updated to the value that minimizes the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd29b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [1.0000, 2.0000],\n",
       "         [1.0000, 3.1416]]),\n",
       " tensor([1., 2., 3.]),\n",
       " tensor([0.1412, 0.9057], requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"res = LBFGS(loss_fn, [b])\\nX, y, b\";\n",
       "                var nbb_formatted_code = \"res = LBFGS(loss_fn, [b])\\nX, y, b\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = LBFGS(loss_fn, [b])\n",
    "X, y, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47801939",
   "metadata": {},
   "source": [
    "The optimum value of the loss function is the same as in the first example with Numpy and standard Scipy optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6cb1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08865057677030563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_formatted_code = \"loss_fn().cpu().detach().item()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn().cpu().detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcb966",
   "metadata": {},
   "source": [
    "Again, it is good to check that the optimization converged successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defbc653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"res.state_dict()[\\\"state\\\"][0][\\\"n_iter\\\"]\";\n",
       "                var nbb_formatted_code = \"res.state_dict()[\\\"state\\\"][0][\\\"n_iter\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.state_dict()[\"state\"][0][\"n_iter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf824a75",
   "metadata": {},
   "source": [
    "The optimization took 6 iterations (cutoff being 500). Therefore, the optimization was probably terminated due to convergence to a local minumum and we should be fine. If there is no convergence you may need to increase the cutoff or study the matter further (e.g., the loss to be optimized could be badly behaving)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91158601",
   "metadata": {},
   "source": [
    "## Addendum: differences between \"conventional\" optimization and optimization in deep learning\n",
    "\n",
    "The idea in optimization is to find parameter values that minimize the value of a given target function (loss). When the parameters of the target function are real-valued numbers then typically gradient-based optimizers are used. \n",
    "\n",
    "In deep learning applications the loss to be optimized is typically, e.g., classification error of the deep learning networks and the parameters are weights in the network. Below, I list some practical differences between optimization in deep learning and more traditional optimization problems.\n",
    "\n",
    "\n",
    "### Stochastic gradient algorithms are more popular in deep learning\n",
    "\n",
    "Deep learning problems are typically high dimensional (meaning there are lots of parameters). Stochastic gradient -based algorithms scale well for very high-dimensional datasets, while more conventional optimization methods may become too slow. However, for lower-dimensional problems the stochastic gradient -based algorithms may be slower to converge. LBFGS (which is not based on stochastic gradient) used above is very good conventional optimizer and me be a better default choice for conventional problems with a reasonable number of parameters to be optimised.\n",
    "\n",
    "\n",
    "### In deep learning we do not want to find the minimum\n",
    "\n",
    "In deep learning we do not usually want to find the parameter values that minimize the loss, because this may result into overfitting to the training data. Instead, gradient optimization is typically run iteratively step by step. The optimization is stopped when validation loss stops decreasing. In a normal Torch workflow `optimiser.step(closure)` would be run repeatedly until validation loss stops decreasing. \n",
    "\n",
    "A more traditional approach for optimization (used also by the Scipy `minimize` above) is to run the optimizer until the pre-defined stopping criteria are met, which typically means that the solution is no longer improved indicating that the optimizer has found a local minimum of the loss function; this is what our LBFGS helper function does. We need to run LBFGS optimizer only one step, which is in most cases enough to converge, because the default maximum number of iterations within step is in LBFGS function set to 500, Torch default being 20. Often, the optimizer stops before 500 steps are used after convergence, but you should check this.\n",
    "\n",
    "### In deep learning speed is considered more important than stability or robustness\n",
    "\n",
    "In deep learning stability or robustness of the algorithm is often considered less important than scalability. If the optimization does not converge it can be just restarted with different parameters, while in a more conventional setup you would be happy if the optimizer behaves predictably and you do not have to fiddle with parameters. Therefore line search is not by default used in Torch LBFGS optimizer. I have added line search option, which guarantees that the value of the loss function does not increase at any iteration. This results to better numerical stability with the penalty of slightly longer runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm-env-1",
   "language": "python",
   "name": "atm-env-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
