{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Slisemap on the Air Quality dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use Slisemap on the Air Quality dataset with a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from slisemap import Slisemap\n",
    "from slisemap.local_models import logistic_regression, logistic_regression_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "These are the objectives of this notebook:\n",
    "\n",
    "- Demonstrate how to use a different type of white box model with Slisemap.\n",
    "- Discuss the use of subsampling.\n",
    "- Demonstrate how to tweak parameters.\n",
    "- Investigate the solution and the data.\n",
    "- Demonstrate how to add data not in the initial subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slisemap Model Caching\n",
    "\n",
    "To reduce execution times, we provide dumps of pretrained Slisemap models. If you want to train them yourself instead, please comment out the following lines and remove the `cache` directory in this `examples` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cached in [\"sm-02.v1.sm\", \"sm-02.v2.sm\", \"BZ-02.v3.npz\"]:\n",
    "    path = Path(\"cache\") / cached\n",
    "    path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    if not path.exists():\n",
    "        urlretrieve(\n",
    "            f\"https://raw.githubusercontent.com/edahelsinki/slisemap/data/examples/cache/{cached}\",\n",
    "            path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The Air Quality dataset contains 7355 hourly instances of 12 different air quality measurements, one of which is used as a dependent variable (CO) and the others as covariates. Since this notebook demonstrates classification, we split the $y$-values into two clusters based on the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in previous noteboook, load the dataset (download it if necessary).\n",
    "path = Path(\"data\") / \"AQ_cleaned_version.csv\"\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "if not path.exists():\n",
    "    urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/edahelsinki/drifter/master/TiittanenOHP2019-code/data/AQ_cleaned_version.csv\",\n",
    "        path,\n",
    "    )\n",
    "AQ = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Air Quality dataset is cleaned and preprocessed as in:\n",
    "\n",
    "    Oikarinen E, Tiittanen H, Henelius A, PuolamÃ¤ki K (2021)\n",
    "    Detecting virtual concept drift of regressors without ground truth values.\n",
    "    Data Mining and Knowledge Discovery 35(3):726-747, DOI 10.1007/s10618-021-00739-7]\n",
    "\"\"\"\n",
    "columns = [\n",
    "    \"PT08.S1(CO)\", \"C6H6(GT)\", \"PT08.S2(NMHC)\", \"NOx(GT)\", \"PT08.S3(NOx)\", \"NO2(GT)\",\n",
    "    \"PT08.S4(NO2)\", \"PT08.S5(O3)\", \"T\", \"RH\", \"AH\",\n",
    "]\n",
    "names = [\n",
    "    \"CO(sensor)\", \"C6H6(GT)\", \"NMHC(sensor)\", \"NOx(GT)\", \"NOx(sensor)\", \"NO2(GT)\",\n",
    "    \"NO2(sensor)\", \"O3(sensor)\", \"Temperature\", \"Relative hum.\", \"Absolute hum.\",\n",
    "]\n",
    "\n",
    "# X contains the covariates, y is the target variable and names are column names.\n",
    "X0 = AQ[columns].to_numpy()\n",
    "y0 = AQ[\"CO(GT)\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([y0.reshape((-1,1)), X0], axis=1), columns=[\"CO(GT)\"]+names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now split the target variables to two binary classes\n",
    "# Straight through the middle based on target median\n",
    "y = (y0 > np.median(y0)).astype(float)\n",
    "y = np.stack((y, 1 - y), 1)\n",
    "X = StandardScaler().fit_transform(X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black box model\n",
    "\n",
    "Now, as in the previous notebook, we will use a black box model for the target value. For that we build a random forest classifier using `sklearn.ensemble.RandomForestClassifier`. Note that we use the predicted probabilities instead of the predicted labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=42).fit(X, y)\n",
    "y2 = random_forest.predict_proba(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slisemap\n",
    "\n",
    "Since we are now dealing with a classification task we replace the default white box model (linear regression) in Slisemap with logistic regression. Furthermore, 7355 data items is more than necessary for visualisation purposes, so we subsample 1500 data items. If results for all data items is needed we can add them later (using the structure from the 1500 samples for faster training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data\n",
    "size = 1500\n",
    "X, X_test, y2, y_test, X0, _ = train_test_split(\n",
    "    X, y2, X0, train_size=size, test_size=size, stratify=y[:, 0], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a Slisemap object (without an intercept) and optimise the solution. Since this example takes a while to optimise, we instead load a precalculated Slisemap object from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"cache\") / \"sm-02.v1.sm\"\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if path.exists():\n",
    "    sm = Slisemap.load(path)\n",
    "else:\n",
    "    # Create a Slisemap object with lasso regularisation\n",
    "    sm = Slisemap(\n",
    "        X,\n",
    "        y2,\n",
    "        lasso=0.0001,\n",
    "        radius=3.5,\n",
    "        intercept=False,\n",
    "        local_model=logistic_regression,\n",
    "        local_loss=logistic_regression_loss,\n",
    "        coefficients=X.shape[1],\n",
    "        random_state=42,\n",
    "    )\n",
    "    # Optimise the solution\n",
    "    %time sm.optimise()\n",
    "\n",
    "    sm.cpu()\n",
    "    sm.save(path)\n",
    "\n",
    "print(f\"Loss: {sm.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking\n",
    "\n",
    "If we plot the solution we see that the linear models are not very sparse even though we used lasso/L1 regularisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.plot(title=\"Slisemap with Air Quality data\", variables=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is beacuse we used quite weak regularisation. We can increase the regularisation strength without creating a new `Slisemap` object (remember to re-optimise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"cache\") / \"sm-02.v2.sm\"\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if Path(path).exists():\n",
    "    sm = Slisemap.load(path)\n",
    "else:\n",
    "    sm.lasso = 0.005\n",
    "    %time sm.optimise()\n",
    "\n",
    "    sm.cpu()\n",
    "    sm.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.plot(title=\"Slisemap with Air Quality data\", variables=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the local models are more sparse (more coefficients are zero). This makes them easier to interpret. Additionally, the clusters in the embedding are more distinct, due to the larger differences in the local models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "\n",
    "Next we will add some jitter and plot the local models in a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "bars = 7\n",
    "sm.plot(\n",
    "    title=\"Slisemap with local model clusters\",\n",
    "    clusters=clusters,\n",
    "    bars=bars,\n",
    "    jitter=0.1,\n",
    "    variables=names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easier to see which clusters and local models are connected, and to compare the magnitudes of the coefficients of the local models.\n",
    "\n",
    "Next we plot the distribution of attribute values for the clusters (note the use of unscaled data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.plot_dist(\n",
    "    X=X0,\n",
    "    title=\"Distributions of the variable values\",\n",
    "    clusters=clusters,\n",
    "    variables=names,\n",
    "    targets=\"CO(GT)\",\n",
    "    height=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the clusters do not only have different local models, there are also differences in the distributions of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen test data\n",
    "\n",
    "Initially we subsampled the data into the training set consisting of 1500 data items and the testing set of 1500. We will now examine how data items not used for training the Slisemap solution could be added to the embedding. First we take 7 random data items and show what their fidelities would be if we were to copy the embedding and local model from one training data item (here we plot every possible choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sel = np.random.randint(0, X_test.shape[0], 7)\n",
    "sm.plot_position(\n",
    "    X_test[sel],\n",
    "    y_test[sel],\n",
    "    title=\"Projection of unseen datapoints\",\n",
    "    height=3,\n",
    "    jitter=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the local models in the different clusters are not suitable for every unseen data item.\n",
    "\n",
    "Next we will optimise new embeddings and new local models for all the unseen data, using the structure from the training data as a guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"cache\") / \"BZ-02.v3.npz\"\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if Path(path).exists():\n",
    "    BZ = np.load(path)\n",
    "    B_test = BZ[\"B_test\"]\n",
    "    Z_test = BZ[\"Z_test\"]\n",
    "else:\n",
    "    %time B_test, Z_test = sm.fit_new(X_test, y_test)\n",
    "\n",
    "    np.savez_compressed(path, B_test=B_test, Z_test=Z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is much faster than training the solution from scratch.\n",
    "\n",
    "Finally, lets plot the embeddings of the new and old data in the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zcon = np.concatenate((sm.get_Z(), Z_test), 0)\n",
    "Zcon = Zcon @ np.linalg.svd(Zcon)[2].T # Rotate to match previous plots\n",
    "sm.plot(\n",
    "    B=np.concatenate((sm.get_B(), B_test), 0),\n",
    "    Z=Zcon,\n",
    "    title=\"test\",\n",
    "    clusters=[\"old\"]*sm.n + [\"new\"]*B_test.shape[0],\n",
    "    jitter=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the structure of the unseen data matches that of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook we have looked at a simple classifier use case for Slisemap, and demonstrated the various plots that can be used to to visualise, explore, and investigate the solution and the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
